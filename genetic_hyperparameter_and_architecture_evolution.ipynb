{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 train samples\n",
      "1000 test samples\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train[:2000].reshape(2000, 784)\n",
    "x_test = x_test[2000:3000].reshape(1000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# we just pick 2000 instances in order to train the models faster\n",
    "y_train = keras.utils.to_categorical(y_train[:2000], num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test[2000:3000], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(x_train, y_train, x_test, y_test, d_1, d_2, dr, if_dense3, d_3, act):\n",
    "    model = '_'   # i do this to erase the data of previous models (just in case)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(d_1, activation=act, input_shape=(784,)))\n",
    "    model.add(Dropout(dr))\n",
    "    model.add(Dense(d_2, activation=act))\n",
    "    if if_dense3:                                 # should we add a third layer?\n",
    "        model.add(Dense(d_3, activation=act))\n",
    "    model.add(Dense(num_classes, activation='softmax'))    # we'll stay vanilla with that\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test))\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    return float(score[0])    #this float is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones:  8\n"
     ]
    }
   ],
   "source": [
    "from nevergrad import instrumentation as instru\n",
    "\n",
    "# argument transformation\n",
    "first_dense_neurons = instru.variables.OrderedDiscrete([10, 20])  # neurons of the first dense layer\n",
    "second_dense_neurons = instru.variables.OrderedDiscrete([10, 30])  # neurons of the second dense layer\n",
    "dropout_rate = instru.variables.OrderedDiscrete([0.2, 0.6])  # dropout rate\n",
    "\n",
    "third_dense = instru.variables.SoftmaxCategorical([False, True]) #Wether to add a third dense layer\n",
    "third_dense_neurons = instru.variables.OrderedDiscrete([10, 50, 80]) #And its number of neurons if True\n",
    "\n",
    "activation = instru.variables.SoftmaxCategorical(['relu', 'elu']) # Let's also play with the activation functions\n",
    "\n",
    "# create the instrumented function\n",
    "ifunc = instru.InstrumentedFunction(network, x_train, y_train, x_test, y_test,\n",
    "                                    first_dense_neurons, second_dense_neurons, dropout_rate,\n",
    "                                    third_dense, third_dense_neurons,\n",
    "                                    activation)\n",
    "\n",
    "print('Dimensiones: ', ifunc.dimension)  # 5 dimensional space\n",
    "\n",
    "# The dimension is 5 because:\n",
    "# - the 1st discrete variable has 1 possible values, represented by a hard thresholding in a 1-dimensional space, i.e. we add 1 coordinate to the continuous problem\n",
    "# - the 2nd discrete variable has 3 possible values, represented by softmax, i.e. we add 3 coordinates to the continuous problem\n",
    "# - the 3rd variable has no uncertainty, so it does not introduce any coordinate in the continuous problem\n",
    "# - the 4th variable is a real number, represented by single coordinate.\n",
    "\n",
    "#ifunc([1, -80])  # will print \"b e blublu\" and return 49 = (mean + std * arg)**2 = (1 + 2 * 3)**2\n",
    "# b is selected because 1 > 0 (the threshold is 0 here since there are 2 values.\n",
    "# e is selected because proba(e) = exp(80) / (exp(80) + exp(-80) + exp(-80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? instru.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5_w,10)-aCMA-ES (mu_w=3.2,w_1=45%) in dimension 8 (seed=595728, Sat Dec 22 22:44:56 2018)\n"
     ]
    }
   ],
   "source": [
    "from nevergrad.optimization import optimizerlib\n",
    "\n",
    "optimizer = optimizerlib.CMA(dimension=ifunc.dimension, budget=200, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching 1 jobs with new suggestions\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 80)                880       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                810       \n",
      "=================================================================\n",
      "Total params: 9,650\n",
      "Trainable params: 9,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Tensor dense_4_target:0, specified in either feed_devices or fetch_devices was not found in the Graph",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-216-1ebedffba497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mrecommendation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mifunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/genetik/nevergrad/nevergrad/optimization/base.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, objective_function, executor, batch_mode, verbosity)\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0msleeper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Updating fitness with value {job.result()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/genetik/nevergrad/nevergrad/instrumentation/instanciate.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_call_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_call_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_call_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_call_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# probably inpractical for large arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-208-ca8f5a4cb0ff>\u001b[0m in \u001b[0;36mnetwork\u001b[0;34m(x_train, y_train, x_test, y_test, d_1, d_2, dr, if_dense3, d_3, act)\u001b[0m\n\u001b[1;32m     19\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                         validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \"\"\"\n\u001b[1;32m   1470\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1425\u001b[0;31m               session._session, options_ptr, status)\n\u001b[0m\u001b[1;32m   1426\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Tensor dense_4_target:0, specified in either feed_devices or fetch_devices was not found in the Graph"
     ]
    }
   ],
   "source": [
    "from concurrent import futures\n",
    "\n",
    "with futures.ThreadPoolExecutor(max_workers=optimizer.num_workers) as executor:\n",
    "    recommendation = optimizer.optimize(ifunc, executor=executor, batch_mode=True, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generation:  0\n",
      "X:  [array([-0.09269268,  0.55326621, -0.42939344,  1.05164666,  0.51763137,\n",
      "        0.64493807, -0.9906441 ,  1.58133418])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 5ms/step - loss: 2.0470 - acc: 0.3365 - val_loss: 1.8599 - val_acc: 0.4850\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 1.6657 - acc: 0.5445 - val_loss: 1.5840 - val_acc: 0.5510\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 76us/step - loss: 1.4185 - acc: 0.6110 - val_loss: 1.3759 - val_acc: 0.6030\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 84us/step - loss: 1.2489 - acc: 0.6505 - val_loss: 1.2242 - val_acc: 0.6310\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 73us/step - loss: 1.0706 - acc: 0.7195 - val_loss: 1.0789 - val_acc: 0.6910\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 79us/step - loss: 0.9556 - acc: 0.7435 - val_loss: 0.9730 - val_acc: 0.7280\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 73us/step - loss: 0.8692 - acc: 0.7710 - val_loss: 0.8953 - val_acc: 0.7550\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 73us/step - loss: 0.7978 - acc: 0.7810 - val_loss: 0.8248 - val_acc: 0.7770\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 72us/step - loss: 0.7395 - acc: 0.8005 - val_loss: 0.7700 - val_acc: 0.7880\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 73us/step - loss: 0.6812 - acc: 0.8080 - val_loss: 0.7328 - val_acc: 0.7900\n",
      "Test loss: 0.7328361854553223\n",
      "Test accuracy: 0.79\n",
      "value:  0.7328361854553223\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  1\n",
      "X:  [array([ 0.21298084, -0.64473395, -0.41387274, -2.02706103, -0.05518718,\n",
      "        0.12452323,  0.92247129,  1.04696677])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 1.9291 - acc: 0.3770 - val_loss: 1.6952 - val_acc: 0.5010\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 79us/step - loss: 1.4532 - acc: 0.5780 - val_loss: 1.3974 - val_acc: 0.5980\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.1665 - acc: 0.6770 - val_loss: 1.1558 - val_acc: 0.6630\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 87us/step - loss: 0.9378 - acc: 0.7485 - val_loss: 0.9898 - val_acc: 0.7120\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 0.8002 - acc: 0.7910 - val_loss: 0.8782 - val_acc: 0.7370\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 0.6893 - acc: 0.8085 - val_loss: 0.8054 - val_acc: 0.7490\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 0.6069 - acc: 0.8315 - val_loss: 0.7305 - val_acc: 0.7650\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 0.5626 - acc: 0.8340 - val_loss: 0.6792 - val_acc: 0.7920\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 0.4964 - acc: 0.8680 - val_loss: 0.6688 - val_acc: 0.7990\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 84us/step - loss: 0.4617 - acc: 0.8680 - val_loss: 0.6341 - val_acc: 0.8010\n",
      "Test loss: 0.6340950512886048\n",
      "Test accuracy: 0.801\n",
      "value:  0.6340950512886048\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  2\n",
      "X:  [array([ 0.28965749,  0.15380109,  0.02414381, -0.21871814,  0.53369279,\n",
      "        0.06990533, -0.28987532,  0.15773437])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 2.0791 - acc: 0.2775 - val_loss: 1.7693 - val_acc: 0.5340\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.6180 - acc: 0.4925 - val_loss: 1.3900 - val_acc: 0.6200\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 87us/step - loss: 1.3279 - acc: 0.5760 - val_loss: 1.1442 - val_acc: 0.6830\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 1.1602 - acc: 0.6245 - val_loss: 0.9806 - val_acc: 0.7180\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.0195 - acc: 0.6760 - val_loss: 0.8638 - val_acc: 0.7390\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 0.9385 - acc: 0.6830 - val_loss: 0.8082 - val_acc: 0.7620\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 0.8920 - acc: 0.7090 - val_loss: 0.7536 - val_acc: 0.7840\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 87us/step - loss: 0.8448 - acc: 0.7085 - val_loss: 0.7101 - val_acc: 0.7930\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 0.8072 - acc: 0.7355 - val_loss: 0.6815 - val_acc: 0.8020\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 0.7693 - acc: 0.7475 - val_loss: 0.6552 - val_acc: 0.8160\n",
      "Test loss: 0.6551961889266967\n",
      "Test accuracy: 0.816\n",
      "value:  0.6551961889266967\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  3\n",
      "X:  [array([ 0.62385514, -1.09495526, -0.35331223,  0.2441219 ,  1.17612997,\n",
      "       -0.37738797, -0.57284834, -0.87968619])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 2.1523 - acc: 0.1970 - val_loss: 2.0286 - val_acc: 0.2960\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 76us/step - loss: 1.8681 - acc: 0.3315 - val_loss: 1.8327 - val_acc: 0.3830\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 1.6826 - acc: 0.3950 - val_loss: 1.6641 - val_acc: 0.4240\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.5435 - acc: 0.4335 - val_loss: 1.5399 - val_acc: 0.4740\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 1.4140 - acc: 0.4985 - val_loss: 1.4208 - val_acc: 0.5760\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 84us/step - loss: 1.3110 - acc: 0.5715 - val_loss: 1.3216 - val_acc: 0.5970\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 1.1915 - acc: 0.6275 - val_loss: 1.2143 - val_acc: 0.6270\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 1.1134 - acc: 0.6515 - val_loss: 1.1247 - val_acc: 0.6800\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.0270 - acc: 0.7000 - val_loss: 1.0564 - val_acc: 0.7210\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 0.9756 - acc: 0.7040 - val_loss: 0.9860 - val_acc: 0.7380\n",
      "Test loss: 0.9860171785354614\n",
      "Test accuracy: 0.738\n",
      "value:  0.9860171785354614\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  4\n",
      "X:  [array([ 0.5244142 ,  0.91928177,  1.12844669,  0.21487503, -0.36481139,\n",
      "        0.55842785, -1.17821118, -0.11893813])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 4ms/step - loss: 2.0597 - acc: 0.2705 - val_loss: 1.7633 - val_acc: 0.5360\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.6403 - acc: 0.4875 - val_loss: 1.4733 - val_acc: 0.6650\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.4102 - acc: 0.5720 - val_loss: 1.2681 - val_acc: 0.7040\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 1.2716 - acc: 0.6090 - val_loss: 1.1218 - val_acc: 0.7310\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 1.1502 - acc: 0.6605 - val_loss: 1.0048 - val_acc: 0.7440\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 1.0430 - acc: 0.6770 - val_loss: 0.9158 - val_acc: 0.7580\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 0.9907 - acc: 0.6865 - val_loss: 0.8543 - val_acc: 0.7660\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 0.9385 - acc: 0.7110 - val_loss: 0.7829 - val_acc: 0.7920\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 0.8923 - acc: 0.7210 - val_loss: 0.7574 - val_acc: 0.7880\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 89us/step - loss: 0.8210 - acc: 0.7495 - val_loss: 0.7312 - val_acc: 0.7880\n",
      "Test loss: 0.731190789937973\n",
      "Test accuracy: 0.788\n",
      "value:  0.731190789937973\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  5\n",
      "X:  [array([ 1.81053031,  1.13430547,  0.47886339,  0.73525086,  0.47211186,\n",
      "       -0.40755953,  0.57646331, -0.09720461])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 4ms/step - loss: 2.2035 - acc: 0.1880 - val_loss: 2.0488 - val_acc: 0.4320\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 84us/step - loss: 1.9482 - acc: 0.3265 - val_loss: 1.7605 - val_acc: 0.5120\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 1.7472 - acc: 0.3905 - val_loss: 1.5382 - val_acc: 0.5680\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 1.5844 - acc: 0.4460 - val_loss: 1.3804 - val_acc: 0.5750\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 87us/step - loss: 1.4771 - acc: 0.4855 - val_loss: 1.2612 - val_acc: 0.6520\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 1.3989 - acc: 0.4935 - val_loss: 1.1548 - val_acc: 0.6700\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 84us/step - loss: 1.3039 - acc: 0.5425 - val_loss: 1.0860 - val_acc: 0.6810\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 87us/step - loss: 1.2630 - acc: 0.5435 - val_loss: 1.0275 - val_acc: 0.7240\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 79us/step - loss: 1.2124 - acc: 0.5670 - val_loss: 0.9782 - val_acc: 0.7260\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 1.1577 - acc: 0.5810 - val_loss: 0.9454 - val_acc: 0.7100\n",
      "Test loss: 0.9453985605239869\n",
      "Test accuracy: 0.71\n",
      "value:  0.9453985605239869\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  6\n",
      "X:  [array([ 0.27490747,  0.01745295, -0.24211933, -0.8499886 ,  1.07691014,\n",
      "       -1.11389829,  0.95905237, -0.88730265])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 1.9687 - acc: 0.4025 - val_loss: 1.7464 - val_acc: 0.5120\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 1.5435 - acc: 0.6000 - val_loss: 1.4679 - val_acc: 0.6010\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 77us/step - loss: 1.2734 - acc: 0.6565 - val_loss: 1.2486 - val_acc: 0.6680\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 1.0630 - acc: 0.7025 - val_loss: 1.0847 - val_acc: 0.6940\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.9285 - acc: 0.7455 - val_loss: 0.9607 - val_acc: 0.7340\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 0.8195 - acc: 0.7715 - val_loss: 0.8741 - val_acc: 0.7490\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 79us/step - loss: 0.7195 - acc: 0.8035 - val_loss: 0.8153 - val_acc: 0.7540\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 0.6503 - acc: 0.8295 - val_loss: 0.7581 - val_acc: 0.7760\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 0.5867 - acc: 0.8340 - val_loss: 0.7158 - val_acc: 0.7800\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 0.5356 - acc: 0.8490 - val_loss: 0.6899 - val_acc: 0.7870\n",
      "Test loss: 0.6899283213615417\n",
      "Test accuracy: 0.787\n",
      "value:  0.6899283213615417\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  7\n",
      "X:  [array([ 0.63727544,  1.0175647 , -1.60243881, -0.40318969,  0.98844436,\n",
      "       -1.16659987, -0.50661263, -0.66528572])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 4ms/step - loss: 2.2125 - acc: 0.1660 - val_loss: 2.1449 - val_acc: 0.1880\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 2.0281 - acc: 0.2530 - val_loss: 1.9978 - val_acc: 0.2570\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 1.8755 - acc: 0.2965 - val_loss: 1.8631 - val_acc: 0.3260\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 1.7378 - acc: 0.3440 - val_loss: 1.7465 - val_acc: 0.3890\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 1.5912 - acc: 0.4385 - val_loss: 1.6403 - val_acc: 0.4470\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 1.4632 - acc: 0.5005 - val_loss: 1.5473 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 1.3575 - acc: 0.5330 - val_loss: 1.4665 - val_acc: 0.5250\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 1.2475 - acc: 0.5810 - val_loss: 1.3659 - val_acc: 0.5630\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 1.1929 - acc: 0.6145 - val_loss: 1.3148 - val_acc: 0.6020\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.1127 - acc: 0.6680 - val_loss: 1.2685 - val_acc: 0.6230\n",
      "Test loss: 1.2684972891807555\n",
      "Test accuracy: 0.623\n",
      "value:  1.2684972891807555\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  8\n",
      "X:  [array([-1.08041021,  0.21922919,  0.30301077, -0.25227692, -0.6832679 ,\n",
      "        0.08022235,  2.80902131,  0.35737184])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 2.2490 - acc: 0.1395 - val_loss: 2.1748 - val_acc: 0.2840\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 72us/step - loss: 2.1317 - acc: 0.2090 - val_loss: 2.0614 - val_acc: 0.4310\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 72us/step - loss: 2.0066 - acc: 0.2865 - val_loss: 1.9479 - val_acc: 0.4990\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 75us/step - loss: 1.9459 - acc: 0.3165 - val_loss: 1.8602 - val_acc: 0.5510\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 74us/step - loss: 1.8718 - acc: 0.3400 - val_loss: 1.7829 - val_acc: 0.5630\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 74us/step - loss: 1.8187 - acc: 0.3610 - val_loss: 1.6973 - val_acc: 0.6030\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 77us/step - loss: 1.7579 - acc: 0.3690 - val_loss: 1.6072 - val_acc: 0.6120\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 1.7141 - acc: 0.3660 - val_loss: 1.5657 - val_acc: 0.6000\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 74us/step - loss: 1.6883 - acc: 0.3835 - val_loss: 1.5032 - val_acc: 0.6310\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 75us/step - loss: 1.6439 - acc: 0.3965 - val_loss: 1.4394 - val_acc: 0.6690\n",
      "Test loss: 1.439442907333374\n",
      "Test accuracy: 0.669\n",
      "value:  1.439442907333374\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  9\n",
      "X:  [array([ 0.81639642, -1.20567269,  1.86047874,  0.27378403, -0.05203723,\n",
      "       -0.3207135 , -0.87307797, -1.31281093])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x1b9d52400>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tfg/musicai/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1455, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"/Users/tfg/musicai/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: No such callable handle: 6346346896\n",
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x1ba27bc50>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tfg/musicai/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1455, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"/Users/tfg/musicai/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: No such callable handle: 6692289792\n",
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x1ba57cfd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tfg/musicai/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1455, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"/Users/tfg/musicai/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: No such callable handle: 7093303808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 2.2002 - acc: 0.1615 - val_loss: 2.0424 - val_acc: 0.2970\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 74us/step - loss: 2.0273 - acc: 0.2300 - val_loss: 1.9019 - val_acc: 0.3680\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 1.9271 - acc: 0.2705 - val_loss: 1.7959 - val_acc: 0.4220\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 1.8281 - acc: 0.3290 - val_loss: 1.7052 - val_acc: 0.4480\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.7275 - acc: 0.3535 - val_loss: 1.5983 - val_acc: 0.5050\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 1.6744 - acc: 0.3795 - val_loss: 1.5221 - val_acc: 0.5880\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.5919 - acc: 0.4115 - val_loss: 1.4456 - val_acc: 0.5980\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 1.5458 - acc: 0.4430 - val_loss: 1.3806 - val_acc: 0.6700\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 1.4768 - acc: 0.4530 - val_loss: 1.3103 - val_acc: 0.6750\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 1.4151 - acc: 0.4825 - val_loss: 1.2567 - val_acc: 0.6990\n",
      "Test loss: 1.2567339963912965\n",
      "Test accuracy: 0.699\n",
      "value:  1.2567339963912965\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  10\n",
      "X:  [array([ 1.14202787, -1.08729605,  0.68345415, -0.13565247, -0.29477201,\n",
      "        1.9386896 ,  0.88609916, -0.05462069])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 5ms/step - loss: 2.2407 - acc: 0.1530 - val_loss: 2.1497 - val_acc: 0.1900\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 84us/step - loss: 2.0372 - acc: 0.2420 - val_loss: 1.9251 - val_acc: 0.3260\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.8642 - acc: 0.3085 - val_loss: 1.7502 - val_acc: 0.4600\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 1.7019 - acc: 0.3790 - val_loss: 1.5831 - val_acc: 0.4820\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.6230 - acc: 0.3945 - val_loss: 1.4679 - val_acc: 0.5250\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 116us/step - loss: 1.5296 - acc: 0.4260 - val_loss: 1.3712 - val_acc: 0.5710\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 1.4700 - acc: 0.4455 - val_loss: 1.3154 - val_acc: 0.6010\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 84us/step - loss: 1.3914 - acc: 0.4830 - val_loss: 1.2611 - val_acc: 0.6200\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 1.3372 - acc: 0.4985 - val_loss: 1.1985 - val_acc: 0.6520\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 1.2992 - acc: 0.5275 - val_loss: 1.1594 - val_acc: 0.6790\n",
      "Test loss: 1.159443039894104\n",
      "Test accuracy: 0.679\n",
      "value:  1.159443039894104\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  11\n",
      "X:  [array([ 1.25539543, -0.40452311, -1.18030323, -0.39193933, -1.67952292,\n",
      "       -0.98873451,  1.97730251,  1.08134663])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 2.1716 - acc: 0.1655 - val_loss: 2.0516 - val_acc: 0.2730\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.9352 - acc: 0.2915 - val_loss: 1.8629 - val_acc: 0.3770\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.7441 - acc: 0.4530 - val_loss: 1.6792 - val_acc: 0.5240\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 1.5651 - acc: 0.5375 - val_loss: 1.5351 - val_acc: 0.5900\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 75us/step - loss: 1.4330 - acc: 0.5800 - val_loss: 1.4280 - val_acc: 0.6080\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 1.3123 - acc: 0.6270 - val_loss: 1.3353 - val_acc: 0.6260\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 1.2198 - acc: 0.6495 - val_loss: 1.2500 - val_acc: 0.6590\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.1198 - acc: 0.6810 - val_loss: 1.1675 - val_acc: 0.6820\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 1.0427 - acc: 0.7050 - val_loss: 1.1082 - val_acc: 0.7000\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 0.9557 - acc: 0.7475 - val_loss: 1.0324 - val_acc: 0.7200\n",
      "Test loss: 1.032375214099884\n",
      "Test accuracy: 0.72\n",
      "value:  1.032375214099884\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  12\n",
      "X:  [array([ 1.12572465,  0.01975709, -0.32362221, -1.75046457, -0.67786131,\n",
      "       -0.83155089, -1.01941366,  1.4317467 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 2.0528 - acc: 0.3450 - val_loss: 1.8211 - val_acc: 0.4920\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 1.5925 - acc: 0.5640 - val_loss: 1.5470 - val_acc: 0.5500\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 1.3425 - acc: 0.6385 - val_loss: 1.3637 - val_acc: 0.5900\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 1.1572 - acc: 0.6870 - val_loss: 1.2191 - val_acc: 0.6410\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 113us/step - loss: 1.0252 - acc: 0.7195 - val_loss: 1.1261 - val_acc: 0.6670\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 0.9271 - acc: 0.7530 - val_loss: 1.0503 - val_acc: 0.6870\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.8329 - acc: 0.7905 - val_loss: 0.9613 - val_acc: 0.7250\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 111us/step - loss: 0.7442 - acc: 0.8050 - val_loss: 0.9040 - val_acc: 0.7340\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 0.6978 - acc: 0.8090 - val_loss: 0.8455 - val_acc: 0.7510\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.6275 - acc: 0.8240 - val_loss: 0.7891 - val_acc: 0.7750\n",
      "Test loss: 0.7890836458206176\n",
      "Test accuracy: 0.775\n",
      "value:  0.7890836458206176\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  13\n",
      "X:  [array([ 1.45004141, -0.85726089, -2.86574869, -0.15317516,  0.5710983 ,\n",
      "       -0.04987153,  0.60695716,  1.1230574 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 5ms/step - loss: 2.1306 - acc: 0.1985 - val_loss: 2.0085 - val_acc: 0.2420\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.8574 - acc: 0.3515 - val_loss: 1.8169 - val_acc: 0.4150\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 1.6930 - acc: 0.4490 - val_loss: 1.6669 - val_acc: 0.5110\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 1.5071 - acc: 0.5440 - val_loss: 1.4884 - val_acc: 0.5820\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 1.3350 - acc: 0.5895 - val_loss: 1.3197 - val_acc: 0.6330\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 1.1970 - acc: 0.6215 - val_loss: 1.1884 - val_acc: 0.6630\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 87us/step - loss: 1.1070 - acc: 0.6735 - val_loss: 1.0889 - val_acc: 0.6870\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 0.9845 - acc: 0.7035 - val_loss: 1.0003 - val_acc: 0.7240\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.9310 - acc: 0.7165 - val_loss: 0.9352 - val_acc: 0.7360\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 0.8695 - acc: 0.7395 - val_loss: 0.8958 - val_acc: 0.7440\n",
      "Test loss: 0.895812539100647\n",
      "Test accuracy: 0.744\n",
      "value:  0.895812539100647\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  14\n",
      "X:  [array([ 0.41635032, -0.85623786,  0.67103953, -2.66892622, -0.16111222,\n",
      "        0.32923237,  0.56279881,  0.0442318 ])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 2.0812 - acc: 0.2805 - val_loss: 1.7847 - val_acc: 0.5630\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.6407 - acc: 0.5185 - val_loss: 1.4388 - val_acc: 0.6030\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 1.3872 - acc: 0.5610 - val_loss: 1.2057 - val_acc: 0.6660\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.1965 - acc: 0.6195 - val_loss: 1.0555 - val_acc: 0.6960\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.1237 - acc: 0.6335 - val_loss: 0.9477 - val_acc: 0.7330\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.0298 - acc: 0.6470 - val_loss: 0.8945 - val_acc: 0.7380\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 84us/step - loss: 0.9752 - acc: 0.6650 - val_loss: 0.8318 - val_acc: 0.7670\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 0.8967 - acc: 0.7045 - val_loss: 0.7644 - val_acc: 0.7860\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 81us/step - loss: 0.8491 - acc: 0.7215 - val_loss: 0.7240 - val_acc: 0.7940\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 0.8358 - acc: 0.7275 - val_loss: 0.6928 - val_acc: 0.8000\n",
      "Test loss: 0.6928043441772461\n",
      "Test accuracy: 0.8\n",
      "value:  0.6928043441772461\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  15\n",
      "X:  [array([ 1.16605777, -0.5706412 , -0.64767999,  0.09336188,  0.27771034,\n",
      "       -0.41687396,  0.83799074,  0.75341313])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 4ms/step - loss: 2.0740 - acc: 0.2020 - val_loss: 1.9093 - val_acc: 0.3360\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 84us/step - loss: 1.7279 - acc: 0.4055 - val_loss: 1.6453 - val_acc: 0.5110\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 1.4870 - acc: 0.5115 - val_loss: 1.4541 - val_acc: 0.5450\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.3092 - acc: 0.5890 - val_loss: 1.3044 - val_acc: 0.6140\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.1763 - acc: 0.6425 - val_loss: 1.1880 - val_acc: 0.6460\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 1.0445 - acc: 0.7000 - val_loss: 1.0898 - val_acc: 0.6760\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 0.9532 - acc: 0.7145 - val_loss: 1.0114 - val_acc: 0.7050\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 111us/step - loss: 0.8797 - acc: 0.7430 - val_loss: 0.9475 - val_acc: 0.7150\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.8142 - acc: 0.7590 - val_loss: 0.9043 - val_acc: 0.7380\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.7753 - acc: 0.7640 - val_loss: 0.8643 - val_acc: 0.7570\n",
      "Test loss: 0.864314013004303\n",
      "Test accuracy: 0.757\n",
      "value:  0.864314013004303\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  16\n",
      "X:  [array([ 0.77865058, -0.42174239, -1.05509185, -3.09356207, -0.36655368,\n",
      "       -0.94919289,  1.22215305, -0.0940158 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 2.1266 - acc: 0.2355 - val_loss: 1.9744 - val_acc: 0.3710\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 1.8448 - acc: 0.3780 - val_loss: 1.7531 - val_acc: 0.4390\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 1.6456 - acc: 0.4525 - val_loss: 1.5829 - val_acc: 0.5010\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 1.4672 - acc: 0.5110 - val_loss: 1.4279 - val_acc: 0.5560\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.3184 - acc: 0.5685 - val_loss: 1.2954 - val_acc: 0.6030\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 1.1947 - acc: 0.6090 - val_loss: 1.1938 - val_acc: 0.6290\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 113us/step - loss: 1.0756 - acc: 0.6635 - val_loss: 1.0886 - val_acc: 0.6850\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 0.9945 - acc: 0.6945 - val_loss: 1.0224 - val_acc: 0.7070\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.9190 - acc: 0.7325 - val_loss: 0.9558 - val_acc: 0.7280\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.8722 - acc: 0.7275 - val_loss: 0.8978 - val_acc: 0.7520\n",
      "Test loss: 0.8978030796051025\n",
      "Test accuracy: 0.752\n",
      "value:  0.8978030796051025\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  17\n",
      "X:  [array([-0.00959574, -1.12399197, -1.16998495, -2.31330944,  0.32455166,\n",
      "        0.48692677, -0.21449297,  0.23390757])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 2.2308 - acc: 0.1675 - val_loss: 2.1155 - val_acc: 0.2990\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.9472 - acc: 0.3580 - val_loss: 1.8061 - val_acc: 0.4760\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 87us/step - loss: 1.6506 - acc: 0.4430 - val_loss: 1.5413 - val_acc: 0.5170\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 1.4463 - acc: 0.4915 - val_loss: 1.3630 - val_acc: 0.5900\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 87us/step - loss: 1.3077 - acc: 0.5560 - val_loss: 1.2449 - val_acc: 0.6190\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 1.2082 - acc: 0.5630 - val_loss: 1.1421 - val_acc: 0.6370\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.1256 - acc: 0.6000 - val_loss: 1.0750 - val_acc: 0.6600\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 74us/step - loss: 1.0711 - acc: 0.6190 - val_loss: 1.0246 - val_acc: 0.6710\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 1.0198 - acc: 0.6250 - val_loss: 0.9903 - val_acc: 0.6850\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 80us/step - loss: 0.9833 - acc: 0.6310 - val_loss: 0.9491 - val_acc: 0.7010\n",
      "Test loss: 0.9490737228393554\n",
      "Test accuracy: 0.701\n",
      "value:  0.9490737228393554\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  18\n",
      "X:  [array([-0.99422522,  0.92681088, -1.5414679 , -3.10337179,  2.00190271,\n",
      "        1.49393425, -0.45669605, -0.96541593])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 2.1614 - acc: 0.2395 - val_loss: 1.9896 - val_acc: 0.3290\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 1.8218 - acc: 0.3600 - val_loss: 1.6950 - val_acc: 0.4320\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 87us/step - loss: 1.5970 - acc: 0.4490 - val_loss: 1.4762 - val_acc: 0.5930\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 1.3887 - acc: 0.5715 - val_loss: 1.2849 - val_acc: 0.6560\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 1.2278 - acc: 0.6135 - val_loss: 1.1471 - val_acc: 0.6750\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.1234 - acc: 0.6280 - val_loss: 1.0405 - val_acc: 0.6930\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.0050 - acc: 0.6790 - val_loss: 0.9488 - val_acc: 0.7270\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 0.9474 - acc: 0.6875 - val_loss: 0.8989 - val_acc: 0.7360\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 0.8871 - acc: 0.7090 - val_loss: 0.8457 - val_acc: 0.7650\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 83us/step - loss: 0.8740 - acc: 0.7255 - val_loss: 0.8031 - val_acc: 0.7680\n",
      "Test loss: 0.8030706219673157\n",
      "Test accuracy: 0.768\n",
      "value:  0.8030706219673157\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  19\n",
      "X:  [array([ 0.35333893,  1.21238503,  0.37160002, -3.26538764,  0.42713573,\n",
      "       -0.83997413,  0.22006203,  0.45719713])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 2.2373 - acc: 0.1485 - val_loss: 2.1203 - val_acc: 0.2260\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 2.1058 - acc: 0.2300 - val_loss: 2.0025 - val_acc: 0.2910\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 84us/step - loss: 2.0087 - acc: 0.2620 - val_loss: 1.9146 - val_acc: 0.3480\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 1.9063 - acc: 0.2985 - val_loss: 1.8239 - val_acc: 0.3980\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.8242 - acc: 0.3435 - val_loss: 1.7228 - val_acc: 0.4220\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.7244 - acc: 0.3650 - val_loss: 1.6532 - val_acc: 0.4500\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 1.6593 - acc: 0.3830 - val_loss: 1.5614 - val_acc: 0.4850\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 1.5804 - acc: 0.4190 - val_loss: 1.4919 - val_acc: 0.5190\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 1.5520 - acc: 0.4310 - val_loss: 1.4232 - val_acc: 0.5320\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 84us/step - loss: 1.4974 - acc: 0.4815 - val_loss: 1.3908 - val_acc: 0.5470\n",
      "Test loss: 1.3908320531845093\n",
      "Test accuracy: 0.547\n",
      "value:  1.3908320531845093\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  20\n",
      "X:  [array([ 0.51748746,  0.42089782,  0.1711911 , -3.206645  , -1.01483138,\n",
      "        1.11799856, -1.26703942,  0.6594474 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 5ms/step - loss: 1.9514 - acc: 0.3510 - val_loss: 1.6189 - val_acc: 0.5240\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 1.4532 - acc: 0.5510 - val_loss: 1.2575 - val_acc: 0.6460\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 1.2265 - acc: 0.6060 - val_loss: 1.0515 - val_acc: 0.7190\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.0536 - acc: 0.6510 - val_loss: 0.9368 - val_acc: 0.7260\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.9802 - acc: 0.6840 - val_loss: 0.8553 - val_acc: 0.7590\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 0.9022 - acc: 0.7100 - val_loss: 0.7731 - val_acc: 0.7890\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.8723 - acc: 0.7065 - val_loss: 0.7329 - val_acc: 0.7890\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 0.8054 - acc: 0.7345 - val_loss: 0.6972 - val_acc: 0.7980\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.7933 - acc: 0.7385 - val_loss: 0.6643 - val_acc: 0.8080\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.7228 - acc: 0.7650 - val_loss: 0.6569 - val_acc: 0.7970\n",
      "Test loss: 0.6569387140274048\n",
      "Test accuracy: 0.797\n",
      "value:  0.6569387140274048\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  21\n",
      "X:  [array([ 1.60558901,  0.25329283,  1.66673182, -2.85521397, -0.80886087,\n",
      "        0.68948264, -0.48153757,  0.48747006])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 1.9967 - acc: 0.3255 - val_loss: 1.6027 - val_acc: 0.6030\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 1.4975 - acc: 0.5290 - val_loss: 1.2402 - val_acc: 0.6690\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 1.2350 - acc: 0.6070 - val_loss: 1.0297 - val_acc: 0.7060\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 1.0744 - acc: 0.6545 - val_loss: 0.8926 - val_acc: 0.7520\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.9794 - acc: 0.6720 - val_loss: 0.8002 - val_acc: 0.7740\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.9065 - acc: 0.7035 - val_loss: 0.7644 - val_acc: 0.7580\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.8582 - acc: 0.7175 - val_loss: 0.6908 - val_acc: 0.7950\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.7679 - acc: 0.7475 - val_loss: 0.6675 - val_acc: 0.7940\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.7754 - acc: 0.7355 - val_loss: 0.6302 - val_acc: 0.8100\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.7416 - acc: 0.7555 - val_loss: 0.5955 - val_acc: 0.8240\n",
      "Test loss: 0.5955339164733887\n",
      "Test accuracy: 0.824\n",
      "value:  0.5955339164733887\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  22\n",
      "X:  [array([ 0.00808321, -0.72662903,  0.46994847, -1.65239896,  2.88787351,\n",
      "       -0.42328406, -1.34759329,  0.54277124])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 2.0482 - acc: 0.2845 - val_loss: 1.7818 - val_acc: 0.4660\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 1.6506 - acc: 0.4650 - val_loss: 1.4809 - val_acc: 0.5820\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 1.4193 - acc: 0.5555 - val_loss: 1.2825 - val_acc: 0.6390\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.2639 - acc: 0.6125 - val_loss: 1.1284 - val_acc: 0.6700\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.1493 - acc: 0.6335 - val_loss: 1.0330 - val_acc: 0.6910\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 1.0443 - acc: 0.6565 - val_loss: 0.9504 - val_acc: 0.7060\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 0.9941 - acc: 0.6600 - val_loss: 0.8898 - val_acc: 0.7060\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 0.9235 - acc: 0.6960 - val_loss: 0.8503 - val_acc: 0.7310\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 0.8627 - acc: 0.7235 - val_loss: 0.7941 - val_acc: 0.7480\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.8452 - acc: 0.7200 - val_loss: 0.7654 - val_acc: 0.7620\n",
      "Test loss: 0.7653580327033996\n",
      "Test accuracy: 0.762\n",
      "value:  0.7653580327033996\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  23\n",
      "X:  [array([ 0.756588  ,  0.54543808, -0.74427757, -2.84331963,  0.3561457 ,\n",
      "       -0.49942684,  0.26959955,  0.50215652])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 1.9689 - acc: 0.3550 - val_loss: 1.7371 - val_acc: 0.4980\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 1.5397 - acc: 0.5845 - val_loss: 1.4918 - val_acc: 0.5550\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.2960 - acc: 0.6690 - val_loss: 1.2857 - val_acc: 0.6620\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.1167 - acc: 0.7265 - val_loss: 1.1594 - val_acc: 0.6940\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.9936 - acc: 0.7405 - val_loss: 1.0539 - val_acc: 0.7020\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.8721 - acc: 0.7690 - val_loss: 0.9553 - val_acc: 0.7410\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 0.7810 - acc: 0.7935 - val_loss: 0.8781 - val_acc: 0.7550\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 0.7166 - acc: 0.8090 - val_loss: 0.8360 - val_acc: 0.7620\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 0.6492 - acc: 0.8225 - val_loss: 0.7849 - val_acc: 0.7790\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 0.5934 - acc: 0.8450 - val_loss: 0.7431 - val_acc: 0.7840\n",
      "Test loss: 0.7431003746986389\n",
      "Test accuracy: 0.784\n",
      "value:  0.7431003746986389\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  24\n",
      "X:  [array([-0.52474251, -0.78972806, -0.57240373, -2.63063215, -0.57505536,\n",
      "        0.42784324, -0.89156176, -0.55259753])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 4ms/step - loss: 2.1923 - acc: 0.1850 - val_loss: 2.0759 - val_acc: 0.3250\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 82us/step - loss: 1.9707 - acc: 0.3220 - val_loss: 1.8474 - val_acc: 0.4130\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 73us/step - loss: 1.7656 - acc: 0.3925 - val_loss: 1.6460 - val_acc: 0.5400\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.5830 - acc: 0.4820 - val_loss: 1.4806 - val_acc: 0.5760\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 87us/step - loss: 1.4680 - acc: 0.5435 - val_loss: 1.3649 - val_acc: 0.6230\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 1.3374 - acc: 0.5790 - val_loss: 1.2485 - val_acc: 0.6680\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 1.2387 - acc: 0.6025 - val_loss: 1.1644 - val_acc: 0.6770\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 85us/step - loss: 1.1853 - acc: 0.6195 - val_loss: 1.1147 - val_acc: 0.6820\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.0836 - acc: 0.6510 - val_loss: 1.0468 - val_acc: 0.6890\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 1.0151 - acc: 0.6640 - val_loss: 0.9968 - val_acc: 0.6970\n",
      "Test loss: 0.996847978591919\n",
      "Test accuracy: 0.697\n",
      "value:  0.996847978591919\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  25\n",
      "X:  [array([-0.10328472,  0.16601436, -2.09499985, -2.75254745, -1.63242971,\n",
      "        0.39049102,  0.35930636, -0.68659582])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 5ms/step - loss: 2.1848 - acc: 0.1770 - val_loss: 2.0365 - val_acc: 0.2670\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.8453 - acc: 0.3640 - val_loss: 1.6871 - val_acc: 0.5350\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.5245 - acc: 0.5160 - val_loss: 1.3895 - val_acc: 0.6340\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 1.2918 - acc: 0.5835 - val_loss: 1.1856 - val_acc: 0.6680\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.1650 - acc: 0.6080 - val_loss: 1.0574 - val_acc: 0.7120\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 1.0526 - acc: 0.6460 - val_loss: 0.9741 - val_acc: 0.7220\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 0.9745 - acc: 0.6810 - val_loss: 0.8918 - val_acc: 0.7490\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 0.9078 - acc: 0.6840 - val_loss: 0.8358 - val_acc: 0.7580\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 0.8551 - acc: 0.6995 - val_loss: 0.7820 - val_acc: 0.7630\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.7959 - acc: 0.7280 - val_loss: 0.7540 - val_acc: 0.7800\n",
      "Test loss: 0.7539857225418091\n",
      "Test accuracy: 0.78\n",
      "value:  0.7539857225418091\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  26\n",
      "X:  [array([-0.29076793, -0.17088875,  0.19143454, -1.82545956,  0.60440566,\n",
      "       -0.3640707 ,  0.66900027, -0.09965014])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 2.2542 - acc: 0.1510 - val_loss: 2.2011 - val_acc: 0.1930\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 2.1456 - acc: 0.2075 - val_loss: 2.0954 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 2.0558 - acc: 0.2535 - val_loss: 1.9879 - val_acc: 0.3090\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.9597 - acc: 0.2955 - val_loss: 1.8716 - val_acc: 0.4110\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 1.8653 - acc: 0.3295 - val_loss: 1.7687 - val_acc: 0.4170\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 87us/step - loss: 1.7843 - acc: 0.3520 - val_loss: 1.6872 - val_acc: 0.4270\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 87us/step - loss: 1.7402 - acc: 0.3395 - val_loss: 1.6191 - val_acc: 0.4670\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.6845 - acc: 0.3770 - val_loss: 1.5630 - val_acc: 0.4800\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 1.6403 - acc: 0.3815 - val_loss: 1.5032 - val_acc: 0.5380\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 1.6354 - acc: 0.3700 - val_loss: 1.4591 - val_acc: 0.5370\n",
      "Test loss: 1.4590978574752809\n",
      "Test accuracy: 0.537\n",
      "value:  1.4590978574752809\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  27\n",
      "X:  [array([ 2.3150699 , -0.34831851, -2.44333297, -3.08155362, -1.62955979,\n",
      "        0.33947829,  0.20968822, -0.83182945])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 4ms/step - loss: 1.9454 - acc: 0.3600 - val_loss: 1.6892 - val_acc: 0.5200\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 1.4611 - acc: 0.6045 - val_loss: 1.3764 - val_acc: 0.6190\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.1633 - acc: 0.6960 - val_loss: 1.1543 - val_acc: 0.6780\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.9836 - acc: 0.7255 - val_loss: 1.0089 - val_acc: 0.7100\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 0.8345 - acc: 0.7635 - val_loss: 0.9066 - val_acc: 0.7370\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.7402 - acc: 0.7875 - val_loss: 0.8283 - val_acc: 0.7510\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 0.6622 - acc: 0.8150 - val_loss: 0.7630 - val_acc: 0.7710\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 0.6014 - acc: 0.8325 - val_loss: 0.7232 - val_acc: 0.7800\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 0.5690 - acc: 0.8345 - val_loss: 0.6806 - val_acc: 0.7960\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 0.5145 - acc: 0.8470 - val_loss: 0.6577 - val_acc: 0.7970\n",
      "Test loss: 0.6577464814186096\n",
      "Test accuracy: 0.797\n",
      "value:  0.6577464814186096\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  28\n",
      "X:  [array([ 0.31574849,  0.43835419, -0.32729899, -2.56652078, -0.122415  ,\n",
      "        0.80372881,  0.7477288 , -0.29746647])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 5ms/step - loss: 2.0636 - acc: 0.3420 - val_loss: 1.7246 - val_acc: 0.5230\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.4503 - acc: 0.5745 - val_loss: 1.2305 - val_acc: 0.6480\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 1.1118 - acc: 0.6535 - val_loss: 0.9876 - val_acc: 0.7010\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 0.9156 - acc: 0.7190 - val_loss: 0.8732 - val_acc: 0.7590\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 0.8183 - acc: 0.7365 - val_loss: 0.7954 - val_acc: 0.7530\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 0.7247 - acc: 0.7610 - val_loss: 0.7444 - val_acc: 0.7700\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.6798 - acc: 0.7865 - val_loss: 0.7027 - val_acc: 0.7700\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 0.5927 - acc: 0.8155 - val_loss: 0.6766 - val_acc: 0.7840\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.5705 - acc: 0.8215 - val_loss: 0.6609 - val_acc: 0.8030\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 0.5375 - acc: 0.8395 - val_loss: 0.6147 - val_acc: 0.8060\n",
      "Test loss: 0.6147148942947388\n",
      "Test accuracy: 0.806\n",
      "value:  0.6147148942947388\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  29\n",
      "X:  [array([-0.42436571, -1.31617394, -0.57382122, -0.81335654, -0.75053355,\n",
      "        1.92242929, -0.99312502,  0.66946387])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 5ms/step - loss: 2.0062 - acc: 0.3580 - val_loss: 1.7415 - val_acc: 0.4820\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 1.5559 - acc: 0.5395 - val_loss: 1.4212 - val_acc: 0.5690\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 1.2603 - acc: 0.6280 - val_loss: 1.1738 - val_acc: 0.6730\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.0485 - acc: 0.6965 - val_loss: 1.0149 - val_acc: 0.7020\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.9090 - acc: 0.7295 - val_loss: 0.9096 - val_acc: 0.7220\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 0.8019 - acc: 0.7575 - val_loss: 0.8346 - val_acc: 0.7430\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 0.7278 - acc: 0.7885 - val_loss: 0.7818 - val_acc: 0.7630\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 0.6770 - acc: 0.7965 - val_loss: 0.7456 - val_acc: 0.7680\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.6383 - acc: 0.7945 - val_loss: 0.7165 - val_acc: 0.7810\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 0.5940 - acc: 0.8125 - val_loss: 0.7014 - val_acc: 0.7870\n",
      "Test loss: 0.701399920463562\n",
      "Test accuracy: 0.787\n",
      "value:  0.701399920463562\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  30\n",
      "X:  [array([ 0.8246256 ,  0.0208691 ,  0.19890615, -1.32777999,  1.30586423,\n",
      "       -0.07789132, -0.01581269, -2.05204435])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 5ms/step - loss: 2.2170 - acc: 0.1745 - val_loss: 2.0808 - val_acc: 0.4950\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.9455 - acc: 0.3570 - val_loss: 1.8039 - val_acc: 0.5650\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 1.7314 - acc: 0.4085 - val_loss: 1.5635 - val_acc: 0.5800\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.5686 - acc: 0.4575 - val_loss: 1.3871 - val_acc: 0.5950\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 1.4532 - acc: 0.4695 - val_loss: 1.2529 - val_acc: 0.6510\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 1.3565 - acc: 0.4975 - val_loss: 1.1559 - val_acc: 0.6800\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 1.2727 - acc: 0.5235 - val_loss: 1.0874 - val_acc: 0.7090\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.2359 - acc: 0.5540 - val_loss: 1.0528 - val_acc: 0.7040\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 1.1833 - acc: 0.5785 - val_loss: 0.9918 - val_acc: 0.7310\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.1510 - acc: 0.5800 - val_loss: 0.9378 - val_acc: 0.7470\n",
      "Test loss: 0.9378352174758912\n",
      "Test accuracy: 0.747\n",
      "value:  0.9378352174758912\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  31\n",
      "X:  [array([ 0.45708003, -1.07550399,  0.66204696, -3.66022183,  1.11110307,\n",
      "        0.75889726,  0.37287868,  1.08937934])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 5ms/step - loss: 2.1869 - acc: 0.1980 - val_loss: 2.0272 - val_acc: 0.4140\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.9403 - acc: 0.3320 - val_loss: 1.7903 - val_acc: 0.4450\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 1.7296 - acc: 0.4065 - val_loss: 1.5859 - val_acc: 0.5470\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 1.6076 - acc: 0.4195 - val_loss: 1.4462 - val_acc: 0.5560\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 1.5179 - acc: 0.4490 - val_loss: 1.3367 - val_acc: 0.6030\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.4383 - acc: 0.4720 - val_loss: 1.2499 - val_acc: 0.6120\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.3750 - acc: 0.4915 - val_loss: 1.1875 - val_acc: 0.6480\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 1.2833 - acc: 0.5400 - val_loss: 1.1408 - val_acc: 0.6590\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 1.2738 - acc: 0.5160 - val_loss: 1.1021 - val_acc: 0.6680\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 1.2012 - acc: 0.5650 - val_loss: 1.0375 - val_acc: 0.6930\n",
      "Test loss: 1.0375330862998962\n",
      "Test accuracy: 0.693\n",
      "value:  1.0375330862998962\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  32\n",
      "X:  [array([ 0.39030454, -0.14143268,  2.0993587 , -3.27550866, -1.83507015,\n",
      "        0.47238355, -1.28049658,  0.18961969])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 5ms/step - loss: 2.0078 - acc: 0.3220 - val_loss: 1.6831 - val_acc: 0.5470\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.5500 - acc: 0.5250 - val_loss: 1.3657 - val_acc: 0.6270\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.3105 - acc: 0.5755 - val_loss: 1.1455 - val_acc: 0.6860\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 1.1744 - acc: 0.6225 - val_loss: 1.0198 - val_acc: 0.7010\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.0568 - acc: 0.6495 - val_loss: 0.9147 - val_acc: 0.7290\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 0.9547 - acc: 0.6935 - val_loss: 0.8438 - val_acc: 0.7460\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.9157 - acc: 0.7010 - val_loss: 0.7971 - val_acc: 0.7530\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.8608 - acc: 0.7075 - val_loss: 0.7589 - val_acc: 0.7620\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 0.8375 - acc: 0.7200 - val_loss: 0.7318 - val_acc: 0.7660\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 0.8011 - acc: 0.7410 - val_loss: 0.7196 - val_acc: 0.7650\n",
      "Test loss: 0.7195991201400757\n",
      "Test accuracy: 0.765\n",
      "value:  0.7195991201400757\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  33\n",
      "X:  [array([ 1.56541509,  0.53284535, -0.21536317, -3.54208423,  0.53811154,\n",
      "       -0.04787193, -0.9235255 ,  1.33154316])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 1.8739 - acc: 0.4300 - val_loss: 1.5224 - val_acc: 0.6200\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 1.2452 - acc: 0.6815 - val_loss: 1.1040 - val_acc: 0.7200\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 0.9105 - acc: 0.7660 - val_loss: 0.8753 - val_acc: 0.7620\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.7174 - acc: 0.8080 - val_loss: 0.7577 - val_acc: 0.7780\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.5855 - acc: 0.8335 - val_loss: 0.6635 - val_acc: 0.7970\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 0.5234 - acc: 0.8555 - val_loss: 0.6102 - val_acc: 0.8120\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 0.4794 - acc: 0.8630 - val_loss: 0.5619 - val_acc: 0.8350\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.4214 - acc: 0.8800 - val_loss: 0.5590 - val_acc: 0.8290\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 0.3976 - acc: 0.8810 - val_loss: 0.5231 - val_acc: 0.8350\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 0.3648 - acc: 0.8925 - val_loss: 0.5201 - val_acc: 0.8370\n",
      "Test loss: 0.520056704044342\n",
      "Test accuracy: 0.837\n",
      "value:  0.520056704044342\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  34\n",
      "X:  [array([ 1.5323534 , -0.62815683,  1.86198494, -4.21405034, -0.42809275,\n",
      "        0.99980283, -0.1081776 , -0.62588234])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 5ms/step - loss: 2.2543 - acc: 0.1830 - val_loss: 2.1652 - val_acc: 0.4140\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 2.1018 - acc: 0.2810 - val_loss: 1.9530 - val_acc: 0.4600\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 1.8943 - acc: 0.3615 - val_loss: 1.7195 - val_acc: 0.5610\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.7357 - acc: 0.3975 - val_loss: 1.5332 - val_acc: 0.6140\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.5857 - acc: 0.4545 - val_loss: 1.3607 - val_acc: 0.6410\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 1.5026 - acc: 0.4840 - val_loss: 1.2551 - val_acc: 0.6960\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.3843 - acc: 0.5145 - val_loss: 1.1669 - val_acc: 0.7010\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 1.3229 - acc: 0.5495 - val_loss: 1.0790 - val_acc: 0.7110\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 1.2721 - acc: 0.5500 - val_loss: 1.0466 - val_acc: 0.7190\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.2027 - acc: 0.5900 - val_loss: 0.9726 - val_acc: 0.7340\n",
      "Test loss: 0.9726359119415283\n",
      "Test accuracy: 0.734\n",
      "value:  0.9726359119415283\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  35\n",
      "X:  [array([-0.04174474,  0.8743338 ,  1.18934489, -2.15703034,  0.61511425,\n",
      "        0.37980877, -0.84757744,  0.97987274])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 5ms/step - loss: 2.2444 - acc: 0.1530 - val_loss: 2.1681 - val_acc: 0.2210\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 2.0993 - acc: 0.2315 - val_loss: 2.0355 - val_acc: 0.3680\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 1.9890 - acc: 0.3005 - val_loss: 1.9182 - val_acc: 0.4410\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 1.8869 - acc: 0.3430 - val_loss: 1.8036 - val_acc: 0.4570\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 1.8174 - acc: 0.3670 - val_loss: 1.7350 - val_acc: 0.4880\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.7608 - acc: 0.3720 - val_loss: 1.6611 - val_acc: 0.5100\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.7188 - acc: 0.3890 - val_loss: 1.5958 - val_acc: 0.5610\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 1.6786 - acc: 0.4115 - val_loss: 1.5364 - val_acc: 0.5610\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 1.6304 - acc: 0.4210 - val_loss: 1.4901 - val_acc: 0.5820\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 1.5692 - acc: 0.4380 - val_loss: 1.4096 - val_acc: 0.6050\n",
      "Test loss: 1.4096308345794677\n",
      "Test accuracy: 0.605\n",
      "value:  1.4096308345794677\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  36\n",
      "X:  [array([ 2.46034743,  1.147719  ,  1.42441264, -3.86424425,  0.27336671,\n",
      "        1.05299863,  0.9853307 ,  0.56954611])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 2.1936 - acc: 0.2000 - val_loss: 2.0719 - val_acc: 0.3820\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 1.9597 - acc: 0.3050 - val_loss: 1.8059 - val_acc: 0.5480\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.7409 - acc: 0.3900 - val_loss: 1.5539 - val_acc: 0.6250\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.6047 - acc: 0.4225 - val_loss: 1.4024 - val_acc: 0.6740\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.4671 - acc: 0.5035 - val_loss: 1.2541 - val_acc: 0.6860\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 1.3975 - acc: 0.4980 - val_loss: 1.1572 - val_acc: 0.7080\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 1.3279 - acc: 0.5580 - val_loss: 1.0830 - val_acc: 0.7030\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 1.2513 - acc: 0.5745 - val_loss: 1.0070 - val_acc: 0.7310\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 1.1695 - acc: 0.5945 - val_loss: 0.9663 - val_acc: 0.7320\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 1.1589 - acc: 0.5885 - val_loss: 0.9279 - val_acc: 0.7360\n",
      "Test loss: 0.9279166145324707\n",
      "Test accuracy: 0.736\n",
      "value:  0.9279166145324707\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  37\n",
      "X:  [array([ 1.51030451,  0.37744224, -0.48149611, -2.15278907,  0.19527302,\n",
      "       -0.41102538, -0.79418625,  0.23770934])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 1.8530 - acc: 0.4480 - val_loss: 1.5092 - val_acc: 0.6240\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 1.2153 - acc: 0.6980 - val_loss: 1.1119 - val_acc: 0.6970\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 0.9132 - acc: 0.7675 - val_loss: 0.8819 - val_acc: 0.7550\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 0.7174 - acc: 0.8105 - val_loss: 0.7385 - val_acc: 0.7830\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.5882 - acc: 0.8420 - val_loss: 0.6554 - val_acc: 0.7970\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.5165 - acc: 0.8515 - val_loss: 0.6005 - val_acc: 0.8220\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.4494 - acc: 0.8680 - val_loss: 0.5704 - val_acc: 0.8340\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 0.4198 - acc: 0.8745 - val_loss: 0.5347 - val_acc: 0.8360\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 0.3843 - acc: 0.8925 - val_loss: 0.5183 - val_acc: 0.8350\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.3529 - acc: 0.8945 - val_loss: 0.5059 - val_acc: 0.8420\n",
      "Test loss: 0.5058998556137085\n",
      "Test accuracy: 0.842\n",
      "value:  0.5058998556137085\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  38\n",
      "X:  [array([ 1.02426506,  1.52335644,  1.78333951, -2.23170818,  1.54234207,\n",
      "        0.80664506,  0.40266854,  0.77568057])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 2.1875 - acc: 0.2040 - val_loss: 2.0126 - val_acc: 0.4440\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 1.9486 - acc: 0.3220 - val_loss: 1.7468 - val_acc: 0.5800\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 1.7033 - acc: 0.4235 - val_loss: 1.4849 - val_acc: 0.6230\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 1.5400 - acc: 0.4625 - val_loss: 1.3225 - val_acc: 0.6580\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 1.3960 - acc: 0.5130 - val_loss: 1.1720 - val_acc: 0.6830\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.3143 - acc: 0.5475 - val_loss: 1.0950 - val_acc: 0.7090\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 1.2668 - acc: 0.5500 - val_loss: 1.0092 - val_acc: 0.7070\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 1.2240 - acc: 0.5615 - val_loss: 0.9431 - val_acc: 0.7370\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.1704 - acc: 0.5820 - val_loss: 0.9152 - val_acc: 0.7420\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.1478 - acc: 0.5860 - val_loss: 0.8770 - val_acc: 0.7510\n",
      "Test loss: 0.8770010938644409\n",
      "Test accuracy: 0.751\n",
      "value:  0.8770010938644409\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  39\n",
      "X:  [array([ 1.11806076, -0.36006368,  1.37073751, -3.77023134, -0.87156758,\n",
      "        1.19356662,  0.74644433,  1.72850546])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 2.2525 - acc: 0.1530 - val_loss: 2.1836 - val_acc: 0.3220\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 2.1018 - acc: 0.2730 - val_loss: 2.0102 - val_acc: 0.3940\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 1.9409 - acc: 0.3265 - val_loss: 1.8286 - val_acc: 0.4700\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.7992 - acc: 0.3690 - val_loss: 1.6754 - val_acc: 0.5510\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 1.7008 - acc: 0.4000 - val_loss: 1.5387 - val_acc: 0.6130\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 1.5653 - acc: 0.4475 - val_loss: 1.4257 - val_acc: 0.6400\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 1.4916 - acc: 0.4780 - val_loss: 1.3002 - val_acc: 0.6830\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.4139 - acc: 0.5135 - val_loss: 1.2418 - val_acc: 0.6940\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 1.3594 - acc: 0.5465 - val_loss: 1.1499 - val_acc: 0.6880\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 1.3002 - acc: 0.5450 - val_loss: 1.1237 - val_acc: 0.7210\n",
      "Test loss: 1.1236909770965575\n",
      "Test accuracy: 0.721\n",
      "value:  1.1236909770965575\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  40\n",
      "X:  [array([-0.10759984,  1.0162685 ,  0.42591725, -2.05693944, -0.19694955,\n",
      "        0.15723131, -0.47904393,  0.77624032])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 2.1570 - acc: 0.2365 - val_loss: 1.9155 - val_acc: 0.4890\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 1.7915 - acc: 0.3985 - val_loss: 1.5495 - val_acc: 0.6040\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 1.5733 - acc: 0.4590 - val_loss: 1.3227 - val_acc: 0.6360\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.4294 - acc: 0.4920 - val_loss: 1.1839 - val_acc: 0.6870\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 1.3191 - acc: 0.5290 - val_loss: 1.0672 - val_acc: 0.7020\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 116us/step - loss: 1.2558 - acc: 0.5365 - val_loss: 0.9771 - val_acc: 0.7170\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 1.1974 - acc: 0.5575 - val_loss: 0.9309 - val_acc: 0.7260\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 1.1721 - acc: 0.5575 - val_loss: 0.8779 - val_acc: 0.7270\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.1339 - acc: 0.5875 - val_loss: 0.8623 - val_acc: 0.7270\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 1.1263 - acc: 0.5985 - val_loss: 0.8281 - val_acc: 0.7340\n",
      "Test loss: 0.8281450591087341\n",
      "Test accuracy: 0.734\n",
      "value:  0.8281450591087341\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  41\n",
      "X:  [array([ 0.49029965,  0.24325236,  0.12053956, -2.82324743,  0.62365374,\n",
      "        1.08038185, -1.62681783,  1.91724548])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 2.0059 - acc: 0.3015 - val_loss: 1.6443 - val_acc: 0.5450\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 1.4716 - acc: 0.5260 - val_loss: 1.2237 - val_acc: 0.6660\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 1.2054 - acc: 0.5965 - val_loss: 1.0139 - val_acc: 0.7190\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 1.0569 - acc: 0.6415 - val_loss: 0.8824 - val_acc: 0.7590\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 0.9521 - acc: 0.6815 - val_loss: 0.8159 - val_acc: 0.7670\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 0.8688 - acc: 0.7145 - val_loss: 0.7513 - val_acc: 0.7780\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 0.8160 - acc: 0.7360 - val_loss: 0.7176 - val_acc: 0.7810\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 0.7836 - acc: 0.7405 - val_loss: 0.6811 - val_acc: 0.7900\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.7501 - acc: 0.7505 - val_loss: 0.6708 - val_acc: 0.7850\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.7314 - acc: 0.7660 - val_loss: 0.6488 - val_acc: 0.8020\n",
      "Test loss: 0.6487774105072022\n",
      "Test accuracy: 0.802\n",
      "value:  0.6487774105072022\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  42\n",
      "X:  [array([ 2.04996995,  0.11465616, -0.02806642, -1.76789418, -1.0941688 ,\n",
      "        2.22773554, -0.94971019, -0.12705572])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 1.7363 - acc: 0.4710 - val_loss: 1.3817 - val_acc: 0.6350\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 1.1111 - acc: 0.6830 - val_loss: 1.0250 - val_acc: 0.7180\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.8466 - acc: 0.7570 - val_loss: 0.8520 - val_acc: 0.7590\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 0.7040 - acc: 0.8015 - val_loss: 0.7602 - val_acc: 0.7790\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.6130 - acc: 0.8150 - val_loss: 0.6968 - val_acc: 0.7970\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 0.5525 - acc: 0.8360 - val_loss: 0.6479 - val_acc: 0.8040\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.4900 - acc: 0.8555 - val_loss: 0.6267 - val_acc: 0.8190\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.4643 - acc: 0.8615 - val_loss: 0.5959 - val_acc: 0.8250\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 0.4188 - acc: 0.8775 - val_loss: 0.5879 - val_acc: 0.8330\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.3975 - acc: 0.8845 - val_loss: 0.5875 - val_acc: 0.8240\n",
      "Test loss: 0.5874707436561585\n",
      "Test accuracy: 0.824\n",
      "value:  0.5874707436561585\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  43\n",
      "X:  [array([ 1.97944557,  0.28786759,  0.10647442, -2.73666721,  0.7605585 ,\n",
      "       -1.04311944, -1.62590862,  0.9210404 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 2.0266 - acc: 0.2985 - val_loss: 1.7382 - val_acc: 0.5380\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.6529 - acc: 0.4730 - val_loss: 1.5096 - val_acc: 0.6390\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.4668 - acc: 0.5340 - val_loss: 1.3424 - val_acc: 0.6700\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.3236 - acc: 0.5735 - val_loss: 1.2223 - val_acc: 0.6940\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.2282 - acc: 0.6280 - val_loss: 1.1337 - val_acc: 0.6930\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 1.1740 - acc: 0.6260 - val_loss: 1.0657 - val_acc: 0.7010\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 1.0849 - acc: 0.6630 - val_loss: 1.0003 - val_acc: 0.7130\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 1.0210 - acc: 0.6855 - val_loss: 0.9369 - val_acc: 0.7320\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 0.9999 - acc: 0.6815 - val_loss: 0.9074 - val_acc: 0.7270\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.9753 - acc: 0.7025 - val_loss: 0.8657 - val_acc: 0.7400\n",
      "Test loss: 0.865682035446167\n",
      "Test accuracy: 0.74\n",
      "value:  0.865682035446167\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  44\n",
      "X:  [array([-0.02682157,  0.28515549,  0.61644359, -3.84333565, -1.75024259,\n",
      "        0.43761167, -1.06826222, -0.45962082])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 2.1465 - acc: 0.2380 - val_loss: 1.9429 - val_acc: 0.5030\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 1.8877 - acc: 0.3695 - val_loss: 1.7071 - val_acc: 0.5830\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.6924 - acc: 0.4525 - val_loss: 1.5265 - val_acc: 0.6020\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 1.5438 - acc: 0.4870 - val_loss: 1.3693 - val_acc: 0.6400\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.4590 - acc: 0.5085 - val_loss: 1.2572 - val_acc: 0.6680\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 1.3730 - acc: 0.5310 - val_loss: 1.1485 - val_acc: 0.7110\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 1.3165 - acc: 0.5375 - val_loss: 1.0829 - val_acc: 0.7120\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 1.2692 - acc: 0.5545 - val_loss: 1.0205 - val_acc: 0.7400\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 1.2315 - acc: 0.5625 - val_loss: 0.9834 - val_acc: 0.7340\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 1.1806 - acc: 0.5820 - val_loss: 0.9264 - val_acc: 0.7590\n",
      "Test loss: 0.926406536102295\n",
      "Test accuracy: 0.759\n",
      "value:  0.926406536102295\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  45\n",
      "X:  [array([ 0.13609652,  2.11406837,  0.6692622 , -3.74950149, -0.05961125,\n",
      "        1.37066687, -1.84928833,  0.37325652])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 1.9765 - acc: 0.3230 - val_loss: 1.5822 - val_acc: 0.5920\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 1.4834 - acc: 0.5300 - val_loss: 1.2283 - val_acc: 0.6660\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.2329 - acc: 0.5980 - val_loss: 1.0173 - val_acc: 0.7290\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.0941 - acc: 0.6345 - val_loss: 0.9326 - val_acc: 0.7480\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.9925 - acc: 0.6790 - val_loss: 0.8470 - val_acc: 0.7410\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 0.9411 - acc: 0.6805 - val_loss: 0.7793 - val_acc: 0.7800\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.8594 - acc: 0.7210 - val_loss: 0.7325 - val_acc: 0.7980\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 0.8391 - acc: 0.7140 - val_loss: 0.7004 - val_acc: 0.7930\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 0.8068 - acc: 0.7360 - val_loss: 0.6952 - val_acc: 0.8030\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 0.7962 - acc: 0.7310 - val_loss: 0.6710 - val_acc: 0.7960\n",
      "Test loss: 0.6709898180961609\n",
      "Test accuracy: 0.796\n",
      "value:  0.6709898180961609\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  46\n",
      "X:  [array([ 0.99485025,  1.16593043,  1.406864  , -4.65660156, -1.53004755,\n",
      "        1.68813964, -1.48384599,  0.19046278])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 1.9306 - acc: 0.3545 - val_loss: 1.5641 - val_acc: 0.5670\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 1.4501 - acc: 0.5360 - val_loss: 1.2270 - val_acc: 0.6770\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 1.2133 - acc: 0.6055 - val_loss: 1.0270 - val_acc: 0.7370\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 1.0606 - acc: 0.6540 - val_loss: 0.9225 - val_acc: 0.7350\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 0.9476 - acc: 0.6875 - val_loss: 0.8070 - val_acc: 0.7730\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.8769 - acc: 0.7070 - val_loss: 0.7655 - val_acc: 0.7860\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.8191 - acc: 0.7230 - val_loss: 0.7188 - val_acc: 0.7880\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 0.7858 - acc: 0.7460 - val_loss: 0.6803 - val_acc: 0.7950\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.7628 - acc: 0.7425 - val_loss: 0.6545 - val_acc: 0.7990\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.7034 - acc: 0.7725 - val_loss: 0.6303 - val_acc: 0.8110\n",
      "Test loss: 0.630279260635376\n",
      "Test accuracy: 0.811\n",
      "value:  0.630279260635376\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  47\n",
      "X:  [array([ 1.76220209,  0.56034504,  0.69770904, -5.42013826, -0.85784633,\n",
      "       -0.22186464, -0.92890303,  1.19300172])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 2.0400 - acc: 0.2905 - val_loss: 1.7135 - val_acc: 0.5590\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 1.6058 - acc: 0.5060 - val_loss: 1.3829 - val_acc: 0.6470\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.3356 - acc: 0.5815 - val_loss: 1.1417 - val_acc: 0.6960\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 1.1417 - acc: 0.6335 - val_loss: 0.9722 - val_acc: 0.7520\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 1.0116 - acc: 0.6770 - val_loss: 0.8633 - val_acc: 0.7740\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 0.9222 - acc: 0.7015 - val_loss: 0.7919 - val_acc: 0.7910\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 0.8693 - acc: 0.7155 - val_loss: 0.7572 - val_acc: 0.7940\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 0.8412 - acc: 0.7255 - val_loss: 0.7131 - val_acc: 0.8040\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.7995 - acc: 0.7380 - val_loss: 0.6758 - val_acc: 0.8080\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.7658 - acc: 0.7520 - val_loss: 0.6700 - val_acc: 0.7980\n",
      "Test loss: 0.6700393772125244\n",
      "Test accuracy: 0.798\n",
      "value:  0.6700393772125244\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  48\n",
      "X:  [array([ 2.37037465,  1.13007165,  1.07897682, -2.8735586 ,  1.76127065,\n",
      "        0.40250537,  1.11377829,  0.52821612])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 2.2369 - acc: 0.1575 - val_loss: 2.1242 - val_acc: 0.2940\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 2.0691 - acc: 0.2420 - val_loss: 1.9476 - val_acc: 0.4500\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.9092 - acc: 0.3320 - val_loss: 1.7700 - val_acc: 0.5500\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.7649 - acc: 0.4240 - val_loss: 1.6076 - val_acc: 0.6050\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.6163 - acc: 0.4840 - val_loss: 1.4461 - val_acc: 0.6160\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 1.5112 - acc: 0.5120 - val_loss: 1.3217 - val_acc: 0.6360\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.3898 - acc: 0.5310 - val_loss: 1.1985 - val_acc: 0.6420\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 1.3138 - acc: 0.5485 - val_loss: 1.1171 - val_acc: 0.6990\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 1.2698 - acc: 0.5495 - val_loss: 1.0797 - val_acc: 0.6730\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.1834 - acc: 0.5890 - val_loss: 1.0015 - val_acc: 0.7070\n",
      "Test loss: 1.0015019416809081\n",
      "Test accuracy: 0.707\n",
      "value:  1.0015019416809081\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  49\n",
      "X:  [array([ 1.20946495, -1.20954874, -0.56879835, -3.55326339, -0.10211046,\n",
      "       -0.40871376, -1.21598608,  0.98720265])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 1.9268 - acc: 0.3945 - val_loss: 1.6268 - val_acc: 0.5720\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.3735 - acc: 0.6285 - val_loss: 1.2442 - val_acc: 0.6590\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 1.0510 - acc: 0.7110 - val_loss: 1.0177 - val_acc: 0.7100\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.8583 - acc: 0.7555 - val_loss: 0.8902 - val_acc: 0.7470\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 0.7378 - acc: 0.7895 - val_loss: 0.8063 - val_acc: 0.7660\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 0.6766 - acc: 0.8025 - val_loss: 0.7513 - val_acc: 0.7890\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 0.5951 - acc: 0.8230 - val_loss: 0.7260 - val_acc: 0.7860\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 0.5543 - acc: 0.8345 - val_loss: 0.6775 - val_acc: 0.8060\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 0.5127 - acc: 0.8475 - val_loss: 0.6469 - val_acc: 0.8180\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 0.4725 - acc: 0.8605 - val_loss: 0.6340 - val_acc: 0.8160\n",
      "Test loss: 0.6339644265174865\n",
      "Test accuracy: 0.816\n",
      "value:  0.6339644265174865\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  50\n",
      "X:  [array([ 3.18730386, -1.09242573,  1.29195263, -2.72395612, -0.31100121,\n",
      "        0.86282139, -2.54392624,  0.70917063])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 2.0312 - acc: 0.3025 - val_loss: 1.7643 - val_acc: 0.4990\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 1.6131 - acc: 0.4740 - val_loss: 1.4621 - val_acc: 0.5770\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 1.4023 - acc: 0.5330 - val_loss: 1.2748 - val_acc: 0.6360\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 1.2666 - acc: 0.5700 - val_loss: 1.1451 - val_acc: 0.6810\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.1419 - acc: 0.6075 - val_loss: 1.0399 - val_acc: 0.6920\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 1.0721 - acc: 0.6360 - val_loss: 0.9548 - val_acc: 0.7130\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 0.9756 - acc: 0.6740 - val_loss: 0.8977 - val_acc: 0.7290\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.9518 - acc: 0.6800 - val_loss: 0.8537 - val_acc: 0.7410\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 0.9123 - acc: 0.6895 - val_loss: 0.8155 - val_acc: 0.7580\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 0.8770 - acc: 0.6935 - val_loss: 0.7735 - val_acc: 0.7650\n",
      "Test loss: 0.7735457377433776\n",
      "Test accuracy: 0.765\n",
      "value:  0.7735457377433776\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  51\n",
      "X:  [array([ 2.07969508,  1.73601885,  0.95513976, -3.88744139, -1.63444612,\n",
      "        0.2501613 , -1.50342331, -0.08258337])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 2.0498 - acc: 0.2870 - val_loss: 1.7354 - val_acc: 0.5380\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 1.5593 - acc: 0.5415 - val_loss: 1.3801 - val_acc: 0.6340\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 1.3112 - acc: 0.5985 - val_loss: 1.1562 - val_acc: 0.6840\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 1.1359 - acc: 0.6380 - val_loss: 0.9958 - val_acc: 0.7240\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 1.0241 - acc: 0.6825 - val_loss: 0.9046 - val_acc: 0.7470\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.9525 - acc: 0.6815 - val_loss: 0.8292 - val_acc: 0.7780\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.9222 - acc: 0.6930 - val_loss: 0.7884 - val_acc: 0.7780\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.8660 - acc: 0.7140 - val_loss: 0.7292 - val_acc: 0.7900\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.8139 - acc: 0.7405 - val_loss: 0.6876 - val_acc: 0.7970\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.7994 - acc: 0.725 - 0s 103us/step - loss: 0.7924 - acc: 0.7295 - val_loss: 0.6856 - val_acc: 0.8030\n",
      "Test loss: 0.6856090092658996\n",
      "Test accuracy: 0.803\n",
      "value:  0.6856090092658996\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  52\n",
      "X:  [array([ 0.84265345, -0.49089025,  0.05838559, -2.69060134,  0.49075995,\n",
      "        3.07263144, -1.18514316, -0.46563475])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 2.0723 - acc: 0.2965 - val_loss: 1.8119 - val_acc: 0.4740\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.6713 - acc: 0.4620 - val_loss: 1.5107 - val_acc: 0.5440\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 1.4345 - acc: 0.5275 - val_loss: 1.2946 - val_acc: 0.6020\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 111us/step - loss: 1.2691 - acc: 0.5740 - val_loss: 1.1248 - val_acc: 0.6600\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 1.1536 - acc: 0.6125 - val_loss: 1.0043 - val_acc: 0.7210\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 1.0539 - acc: 0.6465 - val_loss: 0.9119 - val_acc: 0.7360\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.9626 - acc: 0.6845 - val_loss: 0.8718 - val_acc: 0.7380\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.9238 - acc: 0.6850 - val_loss: 0.8079 - val_acc: 0.7620\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 0.8604 - acc: 0.7120 - val_loss: 0.7594 - val_acc: 0.7680\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.8430 - acc: 0.7085 - val_loss: 0.7306 - val_acc: 0.7760\n",
      "Test loss: 0.7306123986244202\n",
      "Test accuracy: 0.776\n",
      "value:  0.7306123986244202\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  53\n",
      "X:  [array([ 2.78421254,  0.86423707,  0.10673327, -1.59435981, -0.15959445,\n",
      "        2.58189527, -1.32392111,  0.55201052])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 1.9687 - acc: 0.3410 - val_loss: 1.6206 - val_acc: 0.5700\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.4893 - acc: 0.5295 - val_loss: 1.2714 - val_acc: 0.6530\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 1.2323 - acc: 0.6020 - val_loss: 1.0821 - val_acc: 0.6880\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.0928 - acc: 0.6415 - val_loss: 0.9233 - val_acc: 0.7310\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.9687 - acc: 0.6745 - val_loss: 0.8268 - val_acc: 0.7480\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 111us/step - loss: 0.9128 - acc: 0.6965 - val_loss: 0.7615 - val_acc: 0.7620\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.8501 - acc: 0.7160 - val_loss: 0.7205 - val_acc: 0.7820\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.8158 - acc: 0.7285 - val_loss: 0.6938 - val_acc: 0.7830\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.7882 - acc: 0.7350 - val_loss: 0.6504 - val_acc: 0.7890\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.7391 - acc: 0.7455 - val_loss: 0.6291 - val_acc: 0.7990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6291264350414276\n",
      "Test accuracy: 0.799\n",
      "value:  0.6291264350414276\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  54\n",
      "X:  [array([ 0.30782288, -1.20783141,  1.83223623, -2.22825622, -0.89864137,\n",
      "        0.31439505, -1.10497313, -0.59833232])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 2.2114 - acc: 0.1535 - val_loss: 2.0913 - val_acc: 0.2730\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 2.0360 - acc: 0.2365 - val_loss: 1.9418 - val_acc: 0.3880\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 1.9268 - acc: 0.2965 - val_loss: 1.8119 - val_acc: 0.4460\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 1.8223 - acc: 0.3615 - val_loss: 1.6878 - val_acc: 0.5150\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.7101 - acc: 0.4270 - val_loss: 1.5530 - val_acc: 0.5470\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.6142 - acc: 0.4540 - val_loss: 1.4591 - val_acc: 0.5710\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 1.5415 - acc: 0.4970 - val_loss: 1.3799 - val_acc: 0.6210\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 1.4562 - acc: 0.5200 - val_loss: 1.2960 - val_acc: 0.6290\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 1.4324 - acc: 0.5215 - val_loss: 1.2513 - val_acc: 0.6400\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.3628 - acc: 0.5650 - val_loss: 1.1882 - val_acc: 0.6770\n",
      "Test loss: 1.1881724472045898\n",
      "Test accuracy: 0.677\n",
      "value:  1.1881724472045898\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  55\n",
      "X:  [array([ 4.41563562,  1.47332145,  0.03765885, -3.40323472, -1.97246302,\n",
      "        2.09629375, -2.33023474, -0.88621816])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 1.9824 - acc: 0.3320 - val_loss: 1.6113 - val_acc: 0.5720\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 1.4906 - acc: 0.5180 - val_loss: 1.2435 - val_acc: 0.6500\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.2103 - acc: 0.6055 - val_loss: 1.0324 - val_acc: 0.7010\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.0381 - acc: 0.6670 - val_loss: 0.9122 - val_acc: 0.7360\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.9614 - acc: 0.6875 - val_loss: 0.8453 - val_acc: 0.7390\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 0.8839 - acc: 0.7160 - val_loss: 0.7677 - val_acc: 0.7710\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 0.8119 - acc: 0.7410 - val_loss: 0.7068 - val_acc: 0.7940\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.7706 - acc: 0.7525 - val_loss: 0.6717 - val_acc: 0.8070\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.7586 - acc: 0.7465 - val_loss: 0.6674 - val_acc: 0.7980\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.7250 - acc: 0.7490 - val_loss: 0.6378 - val_acc: 0.8040\n",
      "Test loss: 0.6377812728881836\n",
      "Test accuracy: 0.804\n",
      "value:  0.6377812728881836\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  56\n",
      "X:  [array([ 0.92695242, -0.57951523,  0.10396251, -3.75416254, -1.64886698,\n",
      "        0.37234412, -0.56162984, -0.14372717])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 2.0711 - acc: 0.2705 - val_loss: 1.7507 - val_acc: 0.5600\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.6333 - acc: 0.4845 - val_loss: 1.4238 - val_acc: 0.5950\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 1.3871 - acc: 0.5455 - val_loss: 1.2247 - val_acc: 0.6250\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 1.2320 - acc: 0.5865 - val_loss: 1.1179 - val_acc: 0.6450\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 1.1356 - acc: 0.6325 - val_loss: 0.9919 - val_acc: 0.7200\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 1.0411 - acc: 0.6515 - val_loss: 0.9335 - val_acc: 0.7370\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.9940 - acc: 0.6635 - val_loss: 0.8701 - val_acc: 0.7480\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.9373 - acc: 0.6910 - val_loss: 0.8344 - val_acc: 0.7620\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.8746 - acc: 0.7150 - val_loss: 0.7915 - val_acc: 0.7760\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.8248 - acc: 0.7255 - val_loss: 0.7555 - val_acc: 0.7860\n",
      "Test loss: 0.7554785504341125\n",
      "Test accuracy: 0.786\n",
      "value:  0.7554785504341125\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  57\n",
      "X:  [array([ 1.5555076 , -0.37576122,  1.36924282, -3.27339905, -1.10777024,\n",
      "        2.93518492, -1.5767656 , -0.16022214])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 2.2362 - acc: 0.1815 - val_loss: 2.1416 - val_acc: 0.3420\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 2.0662 - acc: 0.2930 - val_loss: 1.9419 - val_acc: 0.4280\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 1.9041 - acc: 0.3440 - val_loss: 1.7426 - val_acc: 0.5240\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 1.7395 - acc: 0.4070 - val_loss: 1.5591 - val_acc: 0.5430\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 1.5979 - acc: 0.4395 - val_loss: 1.3939 - val_acc: 0.6020\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 1.5085 - acc: 0.4880 - val_loss: 1.2736 - val_acc: 0.6550\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 1.4370 - acc: 0.5030 - val_loss: 1.2092 - val_acc: 0.6740\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 1.3727 - acc: 0.5390 - val_loss: 1.1131 - val_acc: 0.7210\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 1.3036 - acc: 0.5620 - val_loss: 1.0603 - val_acc: 0.7400\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 1.2578 - acc: 0.5910 - val_loss: 0.9997 - val_acc: 0.7440\n",
      "Test loss: 0.9997304515838623\n",
      "Test accuracy: 0.744\n",
      "value:  0.9997304515838623\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  58\n",
      "X:  [array([ 0.17564267, -1.11112344,  0.82735525, -2.89672522, -0.74171469,\n",
      "        0.75174632, -0.15699057,  1.39726025])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 2.2606 - acc: 0.1340 - val_loss: 2.2104 - val_acc: 0.2640\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 2.1215 - acc: 0.2310 - val_loss: 2.0567 - val_acc: 0.3760\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.9543 - acc: 0.3340 - val_loss: 1.8601 - val_acc: 0.4440\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 1.7942 - acc: 0.3650 - val_loss: 1.6599 - val_acc: 0.4850\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 1.6224 - acc: 0.4255 - val_loss: 1.4854 - val_acc: 0.5380\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 1.4932 - acc: 0.4775 - val_loss: 1.3406 - val_acc: 0.5860\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 1.4085 - acc: 0.4995 - val_loss: 1.2364 - val_acc: 0.6430\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 1.3674 - acc: 0.5075 - val_loss: 1.1903 - val_acc: 0.6390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 1.2897 - acc: 0.5380 - val_loss: 1.1143 - val_acc: 0.6590\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 1.2466 - acc: 0.5420 - val_loss: 1.0703 - val_acc: 0.6910\n",
      "Test loss: 1.070279586791992\n",
      "Test accuracy: 0.691\n",
      "value:  1.070279586791992\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  59\n",
      "X:  [array([ 2.92927544,  1.39946343,  1.26831072, -1.1650114 , -1.03427136,\n",
      "        0.46308559, -1.61301813,  1.28855425])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 1.9753 - acc: 0.3310 - val_loss: 1.6135 - val_acc: 0.5790\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 1.5079 - acc: 0.5350 - val_loss: 1.2632 - val_acc: 0.6660\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 1.2252 - acc: 0.6030 - val_loss: 1.0329 - val_acc: 0.7070\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 1.0773 - acc: 0.6455 - val_loss: 0.9007 - val_acc: 0.7300\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.9420 - acc: 0.6930 - val_loss: 0.7866 - val_acc: 0.7750\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.8702 - acc: 0.7275 - val_loss: 0.7196 - val_acc: 0.7910\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.8068 - acc: 0.7350 - val_loss: 0.6790 - val_acc: 0.7940\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.7515 - acc: 0.7545 - val_loss: 0.6400 - val_acc: 0.8040\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.7389 - acc: 0.7640 - val_loss: 0.6275 - val_acc: 0.8090\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 0.7384 - acc: 0.7630 - val_loss: 0.6060 - val_acc: 0.8100\n",
      "Test loss: 0.6060016765594483\n",
      "Test accuracy: 0.81\n",
      "value:  0.6060016765594483\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  60\n",
      "X:  [array([ 3.5440188 ,  0.84543734,  0.8508538 , -1.29181815, -0.83195633,\n",
      "       -0.87493716, -0.85175267,  0.47183771])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 2.0014 - acc: 0.3230 - val_loss: 1.6783 - val_acc: 0.5730\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 1.5571 - acc: 0.5215 - val_loss: 1.3835 - val_acc: 0.6430\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 1.3330 - acc: 0.5975 - val_loss: 1.1859 - val_acc: 0.6990\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 1.1914 - acc: 0.6385 - val_loss: 1.0447 - val_acc: 0.7290\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 1.0783 - acc: 0.6675 - val_loss: 0.9356 - val_acc: 0.7590\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 154us/step - loss: 0.9867 - acc: 0.6965 - val_loss: 0.8609 - val_acc: 0.7720\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.9287 - acc: 0.7070 - val_loss: 0.8090 - val_acc: 0.7780\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.8804 - acc: 0.7270 - val_loss: 0.7587 - val_acc: 0.7870\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.8442 - acc: 0.7345 - val_loss: 0.7178 - val_acc: 0.8060\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.8002 - acc: 0.7460 - val_loss: 0.7052 - val_acc: 0.7950\n",
      "Test loss: 0.7051867136955261\n",
      "Test accuracy: 0.795\n",
      "value:  0.7051867136955261\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  61\n",
      "X:  [array([ 3.89161143,  0.25966629,  0.82266552, -0.49297637, -1.80595912,\n",
      "        1.81210062, -1.28640915,  0.08603521])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 2.2400 - acc: 0.1655 - val_loss: 2.1217 - val_acc: 0.3630\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 2.0308 - acc: 0.2895 - val_loss: 1.9246 - val_acc: 0.4680\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 1.8967 - acc: 0.3310 - val_loss: 1.7419 - val_acc: 0.5510\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 1.7046 - acc: 0.4170 - val_loss: 1.5620 - val_acc: 0.6270\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 1.6326 - acc: 0.4445 - val_loss: 1.4537 - val_acc: 0.6620\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 1.5336 - acc: 0.4785 - val_loss: 1.3387 - val_acc: 0.6970\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 1.4430 - acc: 0.5035 - val_loss: 1.2436 - val_acc: 0.7170\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 1.3688 - acc: 0.5285 - val_loss: 1.1858 - val_acc: 0.7320\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 121us/step - loss: 1.3230 - acc: 0.5325 - val_loss: 1.1019 - val_acc: 0.7300\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 113us/step - loss: 1.2920 - acc: 0.5460 - val_loss: 1.0577 - val_acc: 0.7550\n",
      "Test loss: 1.0577046642303467\n",
      "Test accuracy: 0.755\n",
      "value:  1.0577046642303467\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  62\n",
      "X:  [array([ 2.8436322 ,  2.24607199,  1.93062712, -3.38528758, -1.99478681,\n",
      "       -0.50849138, -2.99023815,  1.20307164])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 2.1504 - acc: 0.2420 - val_loss: 1.9365 - val_acc: 0.4140\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.8435 - acc: 0.3885 - val_loss: 1.7261 - val_acc: 0.5410\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 1.6407 - acc: 0.4825 - val_loss: 1.5536 - val_acc: 0.6000\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.5028 - acc: 0.5525 - val_loss: 1.3937 - val_acc: 0.6450\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 88us/step - loss: 1.3734 - acc: 0.5930 - val_loss: 1.2742 - val_acc: 0.6720\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 86us/step - loss: 1.2697 - acc: 0.6200 - val_loss: 1.1674 - val_acc: 0.6840\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 1.1448 - acc: 0.6620 - val_loss: 1.0662 - val_acc: 0.7060\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.0960 - acc: 0.6770 - val_loss: 1.0070 - val_acc: 0.7150\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 1.0555 - acc: 0.6730 - val_loss: 0.9443 - val_acc: 0.7310\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 0.9937 - acc: 0.6990 - val_loss: 0.9071 - val_acc: 0.7210\n",
      "Test loss: 0.9070820927619934\n",
      "Test accuracy: 0.721\n",
      "value:  0.9070820927619934\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  63\n",
      "X:  [array([ 4.80414307,  0.99780937,  1.4642576 , -1.8696746 , -2.73943249,\n",
      "        0.40276411, -1.56966739, -0.0881968 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.0582 - acc: 0.2550 - val_loss: 1.7606 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 1.6321 - acc: 0.4880 - val_loss: 1.4550 - val_acc: 0.6450\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 1.4114 - acc: 0.5645 - val_loss: 1.2536 - val_acc: 0.6850\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 1.2386 - acc: 0.6185 - val_loss: 1.0990 - val_acc: 0.7040\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 1.1340 - acc: 0.6525 - val_loss: 0.9921 - val_acc: 0.7240\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 1.0561 - acc: 0.6720 - val_loss: 0.9117 - val_acc: 0.7590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 175us/step - loss: 0.9757 - acc: 0.6965 - val_loss: 0.8482 - val_acc: 0.7750\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 167us/step - loss: 0.9126 - acc: 0.7175 - val_loss: 0.7919 - val_acc: 0.7770\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.9012 - acc: 0.7120 - val_loss: 0.7635 - val_acc: 0.7820\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 154us/step - loss: 0.8266 - acc: 0.7430 - val_loss: 0.7251 - val_acc: 0.7840\n",
      "Test loss: 0.7251115140914917\n",
      "Test accuracy: 0.784\n",
      "value:  0.7251115140914917\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  64\n",
      "X:  [array([ 1.8120189 ,  1.00121784,  2.43912207, -3.38264472, -2.43364005,\n",
      "        0.60231191, -1.49501729, -0.26454269])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 2.0255 - acc: 0.3080 - val_loss: 1.6884 - val_acc: 0.5800\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 1.5369 - acc: 0.5245 - val_loss: 1.2866 - val_acc: 0.6600\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.2628 - acc: 0.6035 - val_loss: 1.0611 - val_acc: 0.6950\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 1.1274 - acc: 0.6275 - val_loss: 0.9237 - val_acc: 0.7300\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.9927 - acc: 0.6695 - val_loss: 0.8227 - val_acc: 0.7690\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 0.9064 - acc: 0.7085 - val_loss: 0.7544 - val_acc: 0.7840\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.8682 - acc: 0.7115 - val_loss: 0.7097 - val_acc: 0.7940\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.8358 - acc: 0.7305 - val_loss: 0.6766 - val_acc: 0.8070\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 0.7840 - acc: 0.7400 - val_loss: 0.6514 - val_acc: 0.8020\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.7408 - acc: 0.7665 - val_loss: 0.6325 - val_acc: 0.8120\n",
      "Test loss: 0.6325192601680756\n",
      "Test accuracy: 0.812\n",
      "value:  0.6325192601680756\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  65\n",
      "X:  [array([ 1.72680652,  0.76390913,  0.94004469, -1.61124743, -1.24852652,\n",
      "        2.64684286, -2.21647472,  1.69261295])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 1.9951 - acc: 0.3130 - val_loss: 1.6715 - val_acc: 0.6220\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 1.6145 - acc: 0.4835 - val_loss: 1.4020 - val_acc: 0.7100\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 1.3716 - acc: 0.5710 - val_loss: 1.2335 - val_acc: 0.7000\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.2549 - acc: 0.6160 - val_loss: 1.1014 - val_acc: 0.7250\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.1526 - acc: 0.6375 - val_loss: 0.9954 - val_acc: 0.7480\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 1.0549 - acc: 0.6745 - val_loss: 0.9277 - val_acc: 0.7600\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.9828 - acc: 0.6965 - val_loss: 0.8791 - val_acc: 0.7730\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.9725 - acc: 0.7020 - val_loss: 0.8315 - val_acc: 0.7700\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.9178 - acc: 0.7230 - val_loss: 0.8006 - val_acc: 0.7800\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.8723 - acc: 0.7335 - val_loss: 0.7636 - val_acc: 0.7880\n",
      "Test loss: 0.7636477851867676\n",
      "Test accuracy: 0.788\n",
      "value:  0.7636477851867676\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  66\n",
      "X:  [array([ 3.67074083,  2.25217962,  0.38598627, -2.25917855, -2.60612272,\n",
      "        1.01456277, -1.25291552, -0.72017844])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 13s 7ms/step - loss: 2.0454 - acc: 0.2805 - val_loss: 1.7928 - val_acc: 0.5520\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 167us/step - loss: 1.7054 - acc: 0.4605 - val_loss: 1.5049 - val_acc: 0.6600\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 167us/step - loss: 1.4716 - acc: 0.5530 - val_loss: 1.3098 - val_acc: 0.6950\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 197us/step - loss: 1.3277 - acc: 0.5945 - val_loss: 1.1604 - val_acc: 0.7010\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 1.1981 - acc: 0.6260 - val_loss: 1.0395 - val_acc: 0.7470\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 1.1122 - acc: 0.6535 - val_loss: 0.9498 - val_acc: 0.7600\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 1.0227 - acc: 0.6870 - val_loss: 0.8747 - val_acc: 0.7710\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.9510 - acc: 0.6990 - val_loss: 0.8105 - val_acc: 0.7880\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.9155 - acc: 0.7110 - val_loss: 0.7762 - val_acc: 0.7860\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 0.8739 - acc: 0.7235 - val_loss: 0.7230 - val_acc: 0.7950\n",
      "Test loss: 0.7230440139770508\n",
      "Test accuracy: 0.795\n",
      "value:  0.7230440139770508\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  67\n",
      "X:  [array([ 3.32691619,  1.53733982, -0.30809807, -2.77464146, -1.47954478,\n",
      "        0.83621251, -0.03912857,  1.29349572])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.7703 - acc: 0.4970 - val_loss: 1.4241 - val_acc: 0.6470\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 90us/step - loss: 1.1233 - acc: 0.7250 - val_loss: 1.0278 - val_acc: 0.7320\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 0.8312 - acc: 0.7835 - val_loss: 0.8479 - val_acc: 0.7740\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 89us/step - loss: 0.6629 - acc: 0.8170 - val_loss: 0.7280 - val_acc: 0.7940\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 0.5642 - acc: 0.8385 - val_loss: 0.6694 - val_acc: 0.7930\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.5026 - acc: 0.8515 - val_loss: 0.6274 - val_acc: 0.8110\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 0.4509 - acc: 0.8755 - val_loss: 0.6045 - val_acc: 0.8240\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 0.4141 - acc: 0.8815 - val_loss: 0.5766 - val_acc: 0.8280\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 0.3856 - acc: 0.8900 - val_loss: 0.5618 - val_acc: 0.8320\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 0.3558 - acc: 0.8960 - val_loss: 0.5794 - val_acc: 0.8290\n",
      "Test loss: 0.5793622305393219\n",
      "Test accuracy: 0.829\n",
      "value:  0.5793622305393219\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  68\n",
      "X:  [array([ 3.82222073,  0.7211193 ,  0.87529062, -1.15332802, -1.05487545,\n",
      "        0.21822655, -3.0170832 , -0.05388049])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 13s 6ms/step - loss: 2.0337 - acc: 0.3025 - val_loss: 1.7475 - val_acc: 0.5960\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 1.6489 - acc: 0.4945 - val_loss: 1.4552 - val_acc: 0.6590\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 1.4341 - acc: 0.5660 - val_loss: 1.2658 - val_acc: 0.6880\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 1.2546 - acc: 0.6345 - val_loss: 1.1113 - val_acc: 0.7290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.1368 - acc: 0.6460 - val_loss: 1.0049 - val_acc: 0.7400\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 1.0473 - acc: 0.6865 - val_loss: 0.9017 - val_acc: 0.7630\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 170us/step - loss: 0.9797 - acc: 0.6915 - val_loss: 0.8403 - val_acc: 0.7640\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 162us/step - loss: 0.9123 - acc: 0.7225 - val_loss: 0.8049 - val_acc: 0.7760\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 156us/step - loss: 0.8703 - acc: 0.7270 - val_loss: 0.7553 - val_acc: 0.7870\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.8632 - acc: 0.7255 - val_loss: 0.7260 - val_acc: 0.7870\n",
      "Test loss: 0.7259521913528443\n",
      "Test accuracy: 0.787\n",
      "value:  0.7259521913528443\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  69\n",
      "X:  [array([ 2.26511492,  2.10113585,  0.47598134, -2.68143839, -2.9896546 ,\n",
      "        2.34597847, -3.3357611 ,  1.3430619 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.9709 - acc: 0.3470 - val_loss: 1.6033 - val_acc: 0.5900\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 1.5033 - acc: 0.5265 - val_loss: 1.2442 - val_acc: 0.6700\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 1.2271 - acc: 0.6010 - val_loss: 1.0163 - val_acc: 0.7310\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.0920 - acc: 0.6410 - val_loss: 0.8777 - val_acc: 0.7540\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.9752 - acc: 0.6785 - val_loss: 0.7835 - val_acc: 0.7820\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.9183 - acc: 0.6970 - val_loss: 0.7219 - val_acc: 0.8050\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.8476 - acc: 0.7200 - val_loss: 0.6836 - val_acc: 0.8080\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.7935 - acc: 0.7380 - val_loss: 0.6521 - val_acc: 0.8100\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 0.7611 - acc: 0.7480 - val_loss: 0.6377 - val_acc: 0.8190\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.7383 - acc: 0.7500 - val_loss: 0.6239 - val_acc: 0.8300\n",
      "Test loss: 0.6239077498912812\n",
      "Test accuracy: 0.83\n",
      "value:  0.6239077498912812\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  70\n",
      "X:  [array([ 1.48678849,  2.92105858,  0.95910065, -2.11742369, -3.59260894,\n",
      "        1.40444322, -1.33503355,  2.12375205])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 2.0442 - acc: 0.2995 - val_loss: 1.7542 - val_acc: 0.5620\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.6527 - acc: 0.4985 - val_loss: 1.4738 - val_acc: 0.6710\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 92us/step - loss: 1.4448 - acc: 0.5635 - val_loss: 1.2759 - val_acc: 0.7120\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 1.2676 - acc: 0.6120 - val_loss: 1.1270 - val_acc: 0.7290\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 1.1472 - acc: 0.6530 - val_loss: 0.9958 - val_acc: 0.7750\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.0653 - acc: 0.6710 - val_loss: 0.9046 - val_acc: 0.7900\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 0.9725 - acc: 0.6895 - val_loss: 0.8356 - val_acc: 0.7980\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 0.9376 - acc: 0.6985 - val_loss: 0.7823 - val_acc: 0.8160\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.8910 - acc: 0.7135 - val_loss: 0.7530 - val_acc: 0.8110\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 0.8368 - acc: 0.7400 - val_loss: 0.7229 - val_acc: 0.8160\n",
      "Test loss: 0.7228664886951447\n",
      "Test accuracy: 0.816\n",
      "value:  0.7228664886951447\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  71\n",
      "X:  [array([ 2.39037579,  2.28156574,  0.75275274, -2.85389963, -2.96203652,\n",
      "        0.86868942, -3.25003334,  1.15056741])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.0334 - acc: 0.2945 - val_loss: 1.7088 - val_acc: 0.5650\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 1.6029 - acc: 0.5075 - val_loss: 1.4341 - val_acc: 0.6430\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 1.3630 - acc: 0.5820 - val_loss: 1.2341 - val_acc: 0.6760\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 1.2229 - acc: 0.6100 - val_loss: 1.1010 - val_acc: 0.6940\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 1.1296 - acc: 0.6255 - val_loss: 0.9869 - val_acc: 0.7210\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 1.0279 - acc: 0.6790 - val_loss: 0.9202 - val_acc: 0.7400\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 111us/step - loss: 0.9771 - acc: 0.6910 - val_loss: 0.8545 - val_acc: 0.7550\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.9133 - acc: 0.7190 - val_loss: 0.8097 - val_acc: 0.7640\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.8657 - acc: 0.7240 - val_loss: 0.7598 - val_acc: 0.7750\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 0.8219 - acc: 0.7345 - val_loss: 0.7412 - val_acc: 0.7780\n",
      "Test loss: 0.741227352142334\n",
      "Test accuracy: 0.778\n",
      "value:  0.741227352142334\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  72\n",
      "X:  [array([ 4.27778308,  2.80707624,  0.94725923, -2.84854766, -1.58274704,\n",
      "        2.55568315, -1.30008223, -0.96092734])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.8660 - acc: 0.3760 - val_loss: 1.5095 - val_acc: 0.6340\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.4169 - acc: 0.5465 - val_loss: 1.1835 - val_acc: 0.6930\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 1.2162 - acc: 0.6150 - val_loss: 0.9710 - val_acc: 0.7280\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 1.0459 - acc: 0.6630 - val_loss: 0.8563 - val_acc: 0.7600\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.9431 - acc: 0.6965 - val_loss: 0.7773 - val_acc: 0.7770\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.8694 - acc: 0.7110 - val_loss: 0.7374 - val_acc: 0.7830\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 0.8366 - acc: 0.7205 - val_loss: 0.6794 - val_acc: 0.8050\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 0.7884 - acc: 0.7355 - val_loss: 0.6523 - val_acc: 0.7980\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 0.7373 - acc: 0.7665 - val_loss: 0.6249 - val_acc: 0.8160\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 0.7167 - acc: 0.7630 - val_loss: 0.6054 - val_acc: 0.8260\n",
      "Test loss: 0.6054144387245178\n",
      "Test accuracy: 0.826\n",
      "value:  0.6054144387245178\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  73\n",
      "X:  [array([ 3.77876711,  0.87786927,  0.95471207, -2.72149927, -1.74571499,\n",
      "        2.5610389 , -1.778614  ,  2.30723436])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.9908 - acc: 0.3285 - val_loss: 1.6151 - val_acc: 0.5700\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 1.5047 - acc: 0.5405 - val_loss: 1.2174 - val_acc: 0.6740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 1.2183 - acc: 0.6160 - val_loss: 0.9914 - val_acc: 0.7370\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 1.0802 - acc: 0.6525 - val_loss: 0.8435 - val_acc: 0.7810\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 0.9416 - acc: 0.7005 - val_loss: 0.7656 - val_acc: 0.7650\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.8963 - acc: 0.6910 - val_loss: 0.7016 - val_acc: 0.8100\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 0.8049 - acc: 0.7345 - val_loss: 0.6727 - val_acc: 0.8130\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.7776 - acc: 0.7430 - val_loss: 0.6337 - val_acc: 0.8150\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.7587 - acc: 0.7580 - val_loss: 0.6208 - val_acc: 0.8230\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.6837 - acc: 0.7715 - val_loss: 0.5940 - val_acc: 0.8220\n",
      "Test loss: 0.5940132074356079\n",
      "Test accuracy: 0.822\n",
      "value:  0.5940132074356079\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  74\n",
      "X:  [array([ 2.98507431,  2.94865191,  0.10535769, -2.2704296 , -3.52787326,\n",
      "       -0.07351972, -2.82467053,  1.29225905])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 13s 7ms/step - loss: 2.0715 - acc: 0.2890 - val_loss: 1.7817 - val_acc: 0.5230\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 1.6902 - acc: 0.4765 - val_loss: 1.5297 - val_acc: 0.6220\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 1.4832 - acc: 0.5475 - val_loss: 1.3497 - val_acc: 0.6710\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 1.3216 - acc: 0.5965 - val_loss: 1.1811 - val_acc: 0.6840\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 1.1802 - acc: 0.6405 - val_loss: 1.0458 - val_acc: 0.7210\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 1.0856 - acc: 0.6800 - val_loss: 0.9502 - val_acc: 0.7510\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 1.0155 - acc: 0.6825 - val_loss: 0.8680 - val_acc: 0.7740\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 0.9511 - acc: 0.7045 - val_loss: 0.8080 - val_acc: 0.7770\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 0.9128 - acc: 0.7105 - val_loss: 0.7658 - val_acc: 0.7900\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.8466 - acc: 0.7505 - val_loss: 0.7372 - val_acc: 0.7950\n",
      "Test loss: 0.7372424449920655\n",
      "Test accuracy: 0.795\n",
      "value:  0.7372424449920655\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  75\n",
      "X:  [array([ 0.24648744,  2.06031851,  1.51865545, -1.31060644, -1.1673427 ,\n",
      "        0.49787962, -2.15840186,  0.17951783])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 1.9588 - acc: 0.3525 - val_loss: 1.6374 - val_acc: 0.5240\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 1.4695 - acc: 0.5230 - val_loss: 1.2676 - val_acc: 0.6510\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 1.2003 - acc: 0.6310 - val_loss: 1.0333 - val_acc: 0.7130\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 1.0542 - acc: 0.6645 - val_loss: 0.9074 - val_acc: 0.7240\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 186us/step - loss: 0.9716 - acc: 0.6885 - val_loss: 0.8276 - val_acc: 0.7570\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.9138 - acc: 0.6970 - val_loss: 0.7611 - val_acc: 0.7700\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 177us/step - loss: 0.8324 - acc: 0.7245 - val_loss: 0.7054 - val_acc: 0.7850\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.8027 - acc: 0.7360 - val_loss: 0.7038 - val_acc: 0.7750\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.7441 - acc: 0.7515 - val_loss: 0.6669 - val_acc: 0.8000\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.7106 - acc: 0.7615 - val_loss: 0.6464 - val_acc: 0.8020\n",
      "Test loss: 0.6464420447349548\n",
      "Test accuracy: 0.802\n",
      "value:  0.6464420447349548\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  76\n",
      "X:  [array([ 2.97446202,  0.85606095,  1.48376502, -3.12939815, -2.35326215,\n",
      "        0.98634201, -1.45606966, -0.42765787])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 23s 11ms/step - loss: 2.0106 - acc: 0.3040 - val_loss: 1.6686 - val_acc: 0.6100\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 1.6020 - acc: 0.4885 - val_loss: 1.4060 - val_acc: 0.6550\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 1.3891 - acc: 0.5595 - val_loss: 1.2238 - val_acc: 0.6780\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 1.2611 - acc: 0.6040 - val_loss: 1.0966 - val_acc: 0.7040\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 1.1540 - acc: 0.6440 - val_loss: 1.0131 - val_acc: 0.7110\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 93us/step - loss: 1.0742 - acc: 0.6685 - val_loss: 0.9356 - val_acc: 0.7480\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 1.0068 - acc: 0.6840 - val_loss: 0.8716 - val_acc: 0.7650\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 0.9433 - acc: 0.7165 - val_loss: 0.8206 - val_acc: 0.7630\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 91us/step - loss: 0.9169 - acc: 0.7185 - val_loss: 0.7904 - val_acc: 0.7810\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 0.8470 - acc: 0.7355 - val_loss: 0.7590 - val_acc: 0.7770\n",
      "Test loss: 0.7589750204086304\n",
      "Test accuracy: 0.777\n",
      "value:  0.7589750204086304\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  77\n",
      "X:  [array([ 2.30456417,  2.72426615,  0.06057529, -1.21906792, -0.88433904,\n",
      "        0.6055158 , -2.25461594,  2.90285346])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.9277 - acc: 0.3675 - val_loss: 1.5362 - val_acc: 0.5980\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 1.4280 - acc: 0.5495 - val_loss: 1.1784 - val_acc: 0.6650\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 1.1546 - acc: 0.6200 - val_loss: 0.9771 - val_acc: 0.7190\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 113us/step - loss: 1.0399 - acc: 0.6525 - val_loss: 0.8608 - val_acc: 0.7490\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.9588 - acc: 0.6875 - val_loss: 0.7993 - val_acc: 0.7680\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.8883 - acc: 0.7010 - val_loss: 0.7417 - val_acc: 0.7880\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.8383 - acc: 0.7170 - val_loss: 0.6897 - val_acc: 0.7940\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.8142 - acc: 0.7230 - val_loss: 0.6533 - val_acc: 0.8050\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.7481 - acc: 0.7475 - val_loss: 0.6429 - val_acc: 0.8160\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.7424 - acc: 0.7525 - val_loss: 0.6225 - val_acc: 0.8200\n",
      "Test loss: 0.6225328178405761\n",
      "Test accuracy: 0.82\n",
      "value:  0.6225328178405761\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  78\n",
      "X:  [array([ 3.78631667,  0.95420378, -2.01276182, -3.90068669, -0.58380242,\n",
      "        2.63546594, -2.08674166,  1.87206354])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 13s 6ms/step - loss: 1.8602 - acc: 0.4325 - val_loss: 1.5281 - val_acc: 0.5930\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 1.2002 - acc: 0.6765 - val_loss: 1.1307 - val_acc: 0.6920\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 116us/step - loss: 0.8824 - acc: 0.7625 - val_loss: 0.9362 - val_acc: 0.7380\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 168us/step - loss: 0.7083 - acc: 0.8030 - val_loss: 0.7643 - val_acc: 0.7940\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.5825 - acc: 0.8355 - val_loss: 0.6721 - val_acc: 0.8050\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.5177 - acc: 0.8500 - val_loss: 0.6303 - val_acc: 0.8050\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.4531 - acc: 0.8645 - val_loss: 0.5852 - val_acc: 0.8200\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.4019 - acc: 0.8840 - val_loss: 0.5724 - val_acc: 0.8260\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.3725 - acc: 0.8985 - val_loss: 0.5369 - val_acc: 0.8420\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.3568 - acc: 0.8975 - val_loss: 0.5548 - val_acc: 0.8310\n",
      "Test loss: 0.5548022160530091\n",
      "Test accuracy: 0.831\n",
      "value:  0.5548022160530091\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  79\n",
      "X:  [array([ 2.94021161,  1.16416126,  0.58270158, -4.32163294, -2.60324027,\n",
      "       -0.06181035, -2.08364002,  0.94444456])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.0221 - acc: 0.3155 - val_loss: 1.6774 - val_acc: 0.6040\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 1.5271 - acc: 0.5640 - val_loss: 1.2952 - val_acc: 0.6830\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 1.2428 - acc: 0.6395 - val_loss: 1.0554 - val_acc: 0.7140\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 1.0909 - acc: 0.6715 - val_loss: 0.9440 - val_acc: 0.7220\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 0.9730 - acc: 0.7015 - val_loss: 0.8079 - val_acc: 0.7690\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.9094 - acc: 0.7075 - val_loss: 0.7515 - val_acc: 0.7800\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.8387 - acc: 0.7170 - val_loss: 0.7014 - val_acc: 0.7970\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 0.7882 - acc: 0.7475 - val_loss: 0.6700 - val_acc: 0.8040\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 0.7573 - acc: 0.7650 - val_loss: 0.6402 - val_acc: 0.8110\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 0.7534 - acc: 0.7465 - val_loss: 0.6262 - val_acc: 0.7950\n",
      "Test loss: 0.6262414984703064\n",
      "Test accuracy: 0.795\n",
      "value:  0.6262414984703064\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  80\n",
      "X:  [array([ 6.09971568,  1.4738992 ,  1.3074295 , -4.18720328, -3.55533842,\n",
      "        1.95615265, -2.54564421,  2.244217  ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.9762 - acc: 0.3470 - val_loss: 1.6214 - val_acc: 0.5920\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 1.4629 - acc: 0.5610 - val_loss: 1.2480 - val_acc: 0.6360\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 1.2236 - acc: 0.6090 - val_loss: 1.0249 - val_acc: 0.7240\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 1.0686 - acc: 0.6520 - val_loss: 0.8831 - val_acc: 0.7590\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 0.9528 - acc: 0.6920 - val_loss: 0.7981 - val_acc: 0.7730\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.8970 - acc: 0.7145 - val_loss: 0.7334 - val_acc: 0.7980\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.8018 - acc: 0.7395 - val_loss: 0.6840 - val_acc: 0.8040\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.7640 - acc: 0.7465 - val_loss: 0.6487 - val_acc: 0.8050\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 0.7431 - acc: 0.7585 - val_loss: 0.6274 - val_acc: 0.8150\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.7282 - acc: 0.7510 - val_loss: 0.6192 - val_acc: 0.8070\n",
      "Test loss: 0.6191890931129456\n",
      "Test accuracy: 0.807\n",
      "value:  0.6191890931129456\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  81\n",
      "X:  [array([ 2.06206633,  0.19636901, -3.08984037, -2.96472731, -0.67830791,\n",
      "        3.63925214, -1.97223421,  1.71882582])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.8082 - acc: 0.4960 - val_loss: 1.4715 - val_acc: 0.6120\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 1.1399 - acc: 0.7160 - val_loss: 1.0285 - val_acc: 0.7140\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.8230 - acc: 0.7845 - val_loss: 0.8228 - val_acc: 0.7590\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.6460 - acc: 0.8200 - val_loss: 0.7141 - val_acc: 0.7820\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 94us/step - loss: 0.5492 - acc: 0.8385 - val_loss: 0.6681 - val_acc: 0.7890\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 96us/step - loss: 0.4893 - acc: 0.8595 - val_loss: 0.5945 - val_acc: 0.8110\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 0.4453 - acc: 0.8670 - val_loss: 0.5768 - val_acc: 0.8180\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.4044 - acc: 0.8830 - val_loss: 0.5572 - val_acc: 0.8270\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.3781 - acc: 0.8890 - val_loss: 0.5437 - val_acc: 0.8310\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.3540 - acc: 0.8900 - val_loss: 0.5212 - val_acc: 0.8470\n",
      "Test loss: 0.5212077980041504\n",
      "Test accuracy: 0.847\n",
      "value:  0.5212077980041504\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  82\n",
      "X:  [array([ 4.5858139 ,  0.72556867, -0.25643228, -3.35233424, -0.71196514,\n",
      "        3.6691439 , -3.3424144 ,  1.42257852])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.8122 - acc: 0.4360 - val_loss: 1.5628 - val_acc: 0.5580\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 1.2619 - acc: 0.6515 - val_loss: 1.2287 - val_acc: 0.6470\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.9855 - acc: 0.7560 - val_loss: 1.0126 - val_acc: 0.7270\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 0.8065 - acc: 0.7910 - val_loss: 0.8789 - val_acc: 0.7660\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 0.7020 - acc: 0.8290 - val_loss: 0.7883 - val_acc: 0.7690\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 95us/step - loss: 0.6094 - acc: 0.8425 - val_loss: 0.7225 - val_acc: 0.8010\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 0.5434 - acc: 0.8535 - val_loss: 0.6751 - val_acc: 0.8090\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 0.4880 - acc: 0.8715 - val_loss: 0.6256 - val_acc: 0.8250\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 0.4612 - acc: 0.8750 - val_loss: 0.5936 - val_acc: 0.8370\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 0.4053 - acc: 0.8895 - val_loss: 0.5782 - val_acc: 0.8290\n",
      "Test loss: 0.5782352848052978\n",
      "Test accuracy: 0.829\n",
      "value:  0.5782352848052978\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  83\n",
      "X:  [array([ 5.06636131,  1.77036758, -0.82444393, -3.3815377 , -0.08502776,\n",
      "        0.53432524, -3.40618954,  3.13523386])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 1.8357 - acc: 0.4395 - val_loss: 1.5261 - val_acc: 0.5650\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 1.2076 - acc: 0.6870 - val_loss: 1.0923 - val_acc: 0.7010\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.8565 - acc: 0.7730 - val_loss: 0.8419 - val_acc: 0.7670\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 116us/step - loss: 0.6759 - acc: 0.8025 - val_loss: 0.7194 - val_acc: 0.7860\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.5641 - acc: 0.8415 - val_loss: 0.6375 - val_acc: 0.8100\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.4866 - acc: 0.8600 - val_loss: 0.6075 - val_acc: 0.8030\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 0.4407 - acc: 0.8715 - val_loss: 0.5563 - val_acc: 0.8170\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.4109 - acc: 0.8785 - val_loss: 0.5701 - val_acc: 0.8050\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 0.3618 - acc: 0.8825 - val_loss: 0.5229 - val_acc: 0.8350\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.3439 - acc: 0.8970 - val_loss: 0.5461 - val_acc: 0.8310\n",
      "Test loss: 0.5461482362747192\n",
      "Test accuracy: 0.831\n",
      "value:  0.5461482362747192\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  84\n",
      "X:  [array([ 4.10683676,  0.60031239, -2.06894155, -5.01599925, -1.13586864,\n",
      "        3.93556989, -1.53334787,  2.05751573])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 1.7740 - acc: 0.4705 - val_loss: 1.4053 - val_acc: 0.6350\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 1.1321 - acc: 0.7065 - val_loss: 1.0177 - val_acc: 0.7240\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.8253 - acc: 0.7780 - val_loss: 0.8234 - val_acc: 0.7660\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.6516 - acc: 0.8220 - val_loss: 0.7215 - val_acc: 0.7860\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.5590 - acc: 0.8440 - val_loss: 0.6459 - val_acc: 0.8060\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 0.4933 - acc: 0.8620 - val_loss: 0.6049 - val_acc: 0.8170\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 235us/step - loss: 0.4296 - acc: 0.8760 - val_loss: 0.5703 - val_acc: 0.8280\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.3972 - acc: 0.8845 - val_loss: 0.5611 - val_acc: 0.8220\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.3656 - acc: 0.8915 - val_loss: 0.5415 - val_acc: 0.8280\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.3430 - acc: 0.9005 - val_loss: 0.5366 - val_acc: 0.8390\n",
      "Test loss: 0.5365931224822998\n",
      "Test accuracy: 0.839\n",
      "value:  0.5365931224822998\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  85\n",
      "X:  [array([ 2.3571611 ,  0.71114767,  0.12503535, -3.20534223, -2.44650659,\n",
      "        1.38418519, -1.94477644,  2.50001993])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 2.0117 - acc: 0.3155 - val_loss: 1.6795 - val_acc: 0.5450\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.5091 - acc: 0.5230 - val_loss: 1.2958 - val_acc: 0.6060\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.2294 - acc: 0.6095 - val_loss: 1.0637 - val_acc: 0.6690\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 1.0444 - acc: 0.6585 - val_loss: 0.9403 - val_acc: 0.7080\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 0.9548 - acc: 0.6870 - val_loss: 0.8510 - val_acc: 0.7350\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 97us/step - loss: 0.8871 - acc: 0.7095 - val_loss: 0.7690 - val_acc: 0.7540\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.8249 - acc: 0.7245 - val_loss: 0.7376 - val_acc: 0.7770\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.7681 - acc: 0.7515 - val_loss: 0.7039 - val_acc: 0.7660\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.7501 - acc: 0.7445 - val_loss: 0.7001 - val_acc: 0.7850\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.7219 - acc: 0.7550 - val_loss: 0.6567 - val_acc: 0.7970\n",
      "Test loss: 0.6567098784446717\n",
      "Test accuracy: 0.797\n",
      "value:  0.6567098784446717\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  86\n",
      "X:  [array([ 3.2014277 ,  2.06226878, -0.28258901, -4.49373372, -3.15329928,\n",
      "        3.81878875, -4.04313702, -0.70294745])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 1.7564 - acc: 0.4785 - val_loss: 1.3724 - val_acc: 0.6170\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 1.1095 - acc: 0.7045 - val_loss: 0.9930 - val_acc: 0.7210\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 113us/step - loss: 0.8420 - acc: 0.7610 - val_loss: 0.8360 - val_acc: 0.7390\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.6719 - acc: 0.8025 - val_loss: 0.7335 - val_acc: 0.7640\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.5807 - acc: 0.8245 - val_loss: 0.6698 - val_acc: 0.7990\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.5221 - acc: 0.8415 - val_loss: 0.6209 - val_acc: 0.8080\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.4672 - acc: 0.8630 - val_loss: 0.6110 - val_acc: 0.8140\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 98us/step - loss: 0.4258 - acc: 0.8680 - val_loss: 0.5625 - val_acc: 0.8250\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.3989 - acc: 0.8780 - val_loss: 0.5497 - val_acc: 0.8350\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.3736 - acc: 0.8920 - val_loss: 0.5524 - val_acc: 0.8260\n",
      "Test loss: 0.5524451816082001\n",
      "Test accuracy: 0.826\n",
      "value:  0.5524451816082001\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  87\n",
      "X:  [array([ 3.72578728,  0.90892139, -2.382588  , -5.58644163, -0.77577454,\n",
      "        4.53234102, -0.06234381,  1.84231153])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.7022 - acc: 0.5100 - val_loss: 1.3608 - val_acc: 0.6090\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 1.0951 - acc: 0.7020 - val_loss: 1.0115 - val_acc: 0.7030\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.8155 - acc: 0.7680 - val_loss: 0.8361 - val_acc: 0.7470\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 0.6521 - acc: 0.8105 - val_loss: 0.7391 - val_acc: 0.7770\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.5701 - acc: 0.8340 - val_loss: 0.6699 - val_acc: 0.8030\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.5026 - acc: 0.8595 - val_loss: 0.6059 - val_acc: 0.8070\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.4522 - acc: 0.8605 - val_loss: 0.5820 - val_acc: 0.8180\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.4135 - acc: 0.8785 - val_loss: 0.5646 - val_acc: 0.8300\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 0.3905 - acc: 0.8865 - val_loss: 0.5601 - val_acc: 0.8310\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.3530 - acc: 0.8960 - val_loss: 0.5373 - val_acc: 0.8320\n",
      "Test loss: 0.5372935509681702\n",
      "Test accuracy: 0.832\n",
      "value:  0.5372935509681702\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  88\n",
      "X:  [array([ 3.19200553, -1.51495277,  0.53956406, -3.87113681, -0.08933373,\n",
      "        1.90474264, -1.86077526,  1.20190836])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 13s 7ms/step - loss: 2.0514 - acc: 0.2985 - val_loss: 1.7774 - val_acc: 0.5140\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 1.6047 - acc: 0.4950 - val_loss: 1.4048 - val_acc: 0.6010\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 1.3435 - acc: 0.5625 - val_loss: 1.1935 - val_acc: 0.6770\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 1.1780 - acc: 0.6235 - val_loss: 1.0532 - val_acc: 0.6990\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 1.0516 - acc: 0.6555 - val_loss: 0.9415 - val_acc: 0.7370\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.9608 - acc: 0.6800 - val_loss: 0.8814 - val_acc: 0.7400\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 113us/step - loss: 0.9146 - acc: 0.6965 - val_loss: 0.8366 - val_acc: 0.7480\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.8672 - acc: 0.7115 - val_loss: 0.7902 - val_acc: 0.7690\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.8211 - acc: 0.7305 - val_loss: 0.7686 - val_acc: 0.7670\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.8005 - acc: 0.7160 - val_loss: 0.7363 - val_acc: 0.7840\n",
      "Test loss: 0.7362502367496491\n",
      "Test accuracy: 0.784\n",
      "value:  0.7362502367496491\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  89\n",
      "X:  [array([ 4.1733622 ,  3.09852372, -1.32615114, -3.86637672, -1.84528737,\n",
      "        4.78084057, -1.32098953, -0.63089398])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.0733 - acc: 0.3270 - val_loss: 1.7847 - val_acc: 0.5420\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 1.5604 - acc: 0.5485 - val_loss: 1.3418 - val_acc: 0.6180\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 1.2188 - acc: 0.6345 - val_loss: 1.0689 - val_acc: 0.7100\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 111us/step - loss: 1.0146 - acc: 0.7060 - val_loss: 0.8929 - val_acc: 0.7500\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 0.8765 - acc: 0.7295 - val_loss: 0.8158 - val_acc: 0.7780\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.7711 - acc: 0.7595 - val_loss: 0.7483 - val_acc: 0.7920\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.7169 - acc: 0.7810 - val_loss: 0.7125 - val_acc: 0.7840\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.6374 - acc: 0.8020 - val_loss: 0.6505 - val_acc: 0.8150\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.6071 - acc: 0.8115 - val_loss: 0.6248 - val_acc: 0.8140\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.5541 - acc: 0.8300 - val_loss: 0.6054 - val_acc: 0.8210\n",
      "Test loss: 0.605371361732483\n",
      "Test accuracy: 0.821\n",
      "value:  0.605371361732483\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  90\n",
      "X:  [array([ 4.71735018,  0.78577955, -3.01376975, -5.82777186, -1.48017861,\n",
      "        4.25778091, -1.1539899 ,  3.59723801])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.8383 - acc: 0.4435 - val_loss: 1.4840 - val_acc: 0.5820\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 1.1840 - acc: 0.6815 - val_loss: 1.0937 - val_acc: 0.7030\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.8719 - acc: 0.7685 - val_loss: 0.8874 - val_acc: 0.7360\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.7010 - acc: 0.7975 - val_loss: 0.7861 - val_acc: 0.7550\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.5864 - acc: 0.8365 - val_loss: 0.7090 - val_acc: 0.7800\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 0.5121 - acc: 0.8600 - val_loss: 0.6786 - val_acc: 0.7930\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.4669 - acc: 0.8635 - val_loss: 0.6407 - val_acc: 0.8020\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.4320 - acc: 0.8740 - val_loss: 0.5985 - val_acc: 0.8190\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.3990 - acc: 0.8785 - val_loss: 0.5761 - val_acc: 0.8280\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.3769 - acc: 0.8910 - val_loss: 0.5620 - val_acc: 0.8310\n",
      "Test loss: 0.5619826483726501\n",
      "Test accuracy: 0.831\n",
      "value:  0.5619826483726501\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  91\n",
      "X:  [array([ 2.09215412, -0.0369703 , -1.07877318, -1.60788788,  2.16321951,\n",
      "        4.61861998, -2.7286449 ,  2.75293332])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.8882 - acc: 0.4290 - val_loss: 1.6219 - val_acc: 0.5260\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 1.3496 - acc: 0.6300 - val_loss: 1.2722 - val_acc: 0.6130\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 1.0460 - acc: 0.7015 - val_loss: 1.0257 - val_acc: 0.7050\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.8503 - acc: 0.7605 - val_loss: 0.8716 - val_acc: 0.7390\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 121us/step - loss: 0.7162 - acc: 0.7960 - val_loss: 0.7705 - val_acc: 0.7680\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 0.6399 - acc: 0.8155 - val_loss: 0.6996 - val_acc: 0.7890\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.5612 - acc: 0.8395 - val_loss: 0.6566 - val_acc: 0.7960\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.5047 - acc: 0.8515 - val_loss: 0.6270 - val_acc: 0.8080\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.4874 - acc: 0.8520 - val_loss: 0.6296 - val_acc: 0.7980\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.4458 - acc: 0.8685 - val_loss: 0.5842 - val_acc: 0.8160\n",
      "Test loss: 0.5842125029563904\n",
      "Test accuracy: 0.816\n",
      "value:  0.5842125029563904\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  92\n",
      "X:  [array([ 4.79326064,  1.45188863, -1.72430644, -3.0605516 , -1.15182244,\n",
      "        1.40790559, -2.03172321,  1.95585067])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.7790 - acc: 0.4895 - val_loss: 1.4608 - val_acc: 0.5860\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 1.1559 - acc: 0.6755 - val_loss: 1.0950 - val_acc: 0.6780\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.8505 - acc: 0.7525 - val_loss: 0.9020 - val_acc: 0.7210\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.7037 - acc: 0.7920 - val_loss: 0.7563 - val_acc: 0.7860\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.6029 - acc: 0.8175 - val_loss: 0.6943 - val_acc: 0.7950\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.5272 - acc: 0.8505 - val_loss: 0.6390 - val_acc: 0.7990\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.4871 - acc: 0.8510 - val_loss: 0.6011 - val_acc: 0.8130\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.4235 - acc: 0.8775 - val_loss: 0.5651 - val_acc: 0.8280\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.4195 - acc: 0.8685 - val_loss: 0.5551 - val_acc: 0.8320\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.3729 - acc: 0.8895 - val_loss: 0.5598 - val_acc: 0.8170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5598447875976562\n",
      "Test accuracy: 0.817\n",
      "value:  0.5598447875976562\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  93\n",
      "X:  [array([ 1.61522107, -0.70030627, -0.06818433, -0.84309862, -0.65305563,\n",
      "        5.04614748, -3.86217265,  1.13554189])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 13s 7ms/step - loss: 1.9316 - acc: 0.4110 - val_loss: 1.6352 - val_acc: 0.5510\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 1.3639 - acc: 0.6120 - val_loss: 1.2766 - val_acc: 0.6420\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 1.0539 - acc: 0.7070 - val_loss: 1.0709 - val_acc: 0.6750\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.8754 - acc: 0.7565 - val_loss: 0.9146 - val_acc: 0.7140\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.7559 - acc: 0.7825 - val_loss: 0.8210 - val_acc: 0.7540\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.6547 - acc: 0.8120 - val_loss: 0.7433 - val_acc: 0.7950\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.5913 - acc: 0.8360 - val_loss: 0.6960 - val_acc: 0.8100\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.5291 - acc: 0.8575 - val_loss: 0.6677 - val_acc: 0.8060\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.5008 - acc: 0.8550 - val_loss: 0.6312 - val_acc: 0.8310\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.4675 - acc: 0.8690 - val_loss: 0.6237 - val_acc: 0.8260\n",
      "Test loss: 0.6236981434822082\n",
      "Test accuracy: 0.826\n",
      "value:  0.6236981434822082\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  94\n",
      "X:  [array([ 3.27723913,  0.50541946, -3.60582262, -0.50714344,  1.57803471,\n",
      "        3.71256521, -1.93633727,  2.67728858])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.8048 - acc: 0.4575 - val_loss: 1.4642 - val_acc: 0.6260\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 1.1440 - acc: 0.6910 - val_loss: 1.0647 - val_acc: 0.6990\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.8390 - acc: 0.7725 - val_loss: 0.8261 - val_acc: 0.7740\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.6812 - acc: 0.8140 - val_loss: 0.7066 - val_acc: 0.8060\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.5582 - acc: 0.8380 - val_loss: 0.6657 - val_acc: 0.8030\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 0.5056 - acc: 0.8460 - val_loss: 0.5970 - val_acc: 0.8150\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.4417 - acc: 0.8700 - val_loss: 0.5584 - val_acc: 0.8260\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 113us/step - loss: 0.3910 - acc: 0.8835 - val_loss: 0.5348 - val_acc: 0.8440\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.3718 - acc: 0.8905 - val_loss: 0.5255 - val_acc: 0.8370\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 0.3494 - acc: 0.8940 - val_loss: 0.4949 - val_acc: 0.8570\n",
      "Test loss: 0.49487885618209837\n",
      "Test accuracy: 0.857\n",
      "value:  0.49487885618209837\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  95\n",
      "X:  [array([ 5.15744694,  1.33039074, -2.3411183 , -2.3431805 ,  0.16843843,\n",
      "        6.03713408, -2.32647389,  1.15987999])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.7805 - acc: 0.4605 - val_loss: 1.4333 - val_acc: 0.6120\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 1.1283 - acc: 0.6940 - val_loss: 1.0534 - val_acc: 0.7030\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 0.8257 - acc: 0.7740 - val_loss: 0.8481 - val_acc: 0.7540\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.6582 - acc: 0.8170 - val_loss: 0.7287 - val_acc: 0.7920\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.5578 - acc: 0.8390 - val_loss: 0.6606 - val_acc: 0.8100\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.5006 - acc: 0.8535 - val_loss: 0.6150 - val_acc: 0.8220\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.4589 - acc: 0.8650 - val_loss: 0.5850 - val_acc: 0.8370\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 0.4193 - acc: 0.8750 - val_loss: 0.5515 - val_acc: 0.8400\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.3794 - acc: 0.8910 - val_loss: 0.5252 - val_acc: 0.8480\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.3471 - acc: 0.9020 - val_loss: 0.5158 - val_acc: 0.8540\n",
      "Test loss: 0.5157503192424774\n",
      "Test accuracy: 0.854\n",
      "value:  0.5157503192424774\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  96\n",
      "X:  [array([ 1.02277975, -1.55235308, -2.44549704, -6.2102564 , -1.96097082,\n",
      "        3.62830851, -0.04657044,  2.25178051])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 13s 7ms/step - loss: 1.8869 - acc: 0.4165 - val_loss: 1.6005 - val_acc: 0.5160\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 1.3444 - acc: 0.5895 - val_loss: 1.2284 - val_acc: 0.6520\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 1.0477 - acc: 0.6865 - val_loss: 1.0069 - val_acc: 0.7010\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.8476 - acc: 0.7425 - val_loss: 0.8743 - val_acc: 0.7290\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.7324 - acc: 0.7835 - val_loss: 0.8059 - val_acc: 0.7480\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.6738 - acc: 0.7995 - val_loss: 0.7370 - val_acc: 0.7550\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.5684 - acc: 0.8310 - val_loss: 0.6985 - val_acc: 0.7870\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.5376 - acc: 0.8360 - val_loss: 0.6547 - val_acc: 0.7920\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.4795 - acc: 0.8565 - val_loss: 0.6327 - val_acc: 0.8000\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.4565 - acc: 0.8675 - val_loss: 0.6064 - val_acc: 0.8070\n",
      "Test loss: 0.6063630223274231\n",
      "Test accuracy: 0.807\n",
      "value:  0.6063630223274231\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  97\n",
      "X:  [array([ 2.2204947 , -0.26125352, -2.51267438, -6.2506697 , -0.13658535,\n",
      "        6.73980141, -4.64285944,  2.1231968 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 13s 7ms/step - loss: 1.8714 - acc: 0.4115 - val_loss: 1.5301 - val_acc: 0.5760\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 1.2699 - acc: 0.6590 - val_loss: 1.1533 - val_acc: 0.6800\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.9687 - acc: 0.7295 - val_loss: 0.9458 - val_acc: 0.7330\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.8082 - acc: 0.7640 - val_loss: 0.8323 - val_acc: 0.7630\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.6932 - acc: 0.8010 - val_loss: 0.7581 - val_acc: 0.7750\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.6253 - acc: 0.8155 - val_loss: 0.7014 - val_acc: 0.7910\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.5541 - acc: 0.8335 - val_loss: 0.6581 - val_acc: 0.8050\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.5197 - acc: 0.8460 - val_loss: 0.6366 - val_acc: 0.8060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.4714 - acc: 0.8515 - val_loss: 0.6256 - val_acc: 0.8200\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.4462 - acc: 0.8680 - val_loss: 0.5861 - val_acc: 0.8230\n",
      "Test loss: 0.5861439852714538\n",
      "Test accuracy: 0.823\n",
      "value:  0.5861439852714538\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  98\n",
      "X:  [array([ 3.83407481, -0.70240054, -2.41195206, -3.32729097, -1.64067965,\n",
      "        3.62694932, -1.48743622,  1.63364866])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.8919 - acc: 0.3615 - val_loss: 1.6324 - val_acc: 0.5290\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 1.3623 - acc: 0.6130 - val_loss: 1.2746 - val_acc: 0.6260\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 1.0695 - acc: 0.6820 - val_loss: 1.0969 - val_acc: 0.6580\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 0.8799 - acc: 0.7420 - val_loss: 0.9417 - val_acc: 0.7400\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.7469 - acc: 0.7975 - val_loss: 0.8550 - val_acc: 0.7620\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.6327 - acc: 0.8310 - val_loss: 0.7562 - val_acc: 0.7780\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.5706 - acc: 0.8395 - val_loss: 0.7064 - val_acc: 0.7950\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.5184 - acc: 0.8535 - val_loss: 0.6739 - val_acc: 0.8010\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.4798 - acc: 0.8550 - val_loss: 0.6685 - val_acc: 0.8060\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.4374 - acc: 0.8785 - val_loss: 0.6099 - val_acc: 0.8160\n",
      "Test loss: 0.6099386584758758\n",
      "Test accuracy: 0.816\n",
      "value:  0.6099386584758758\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  99\n",
      "X:  [array([ 3.40425645, -1.4135006 , -3.42021656, -6.25935597,  0.30047694,\n",
      "        3.28720776, -0.8614438 ,  2.21121988])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 2.1514 - acc: 0.2360 - val_loss: 2.0068 - val_acc: 0.3840\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 1.7773 - acc: 0.4385 - val_loss: 1.6552 - val_acc: 0.5090\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 1.4418 - acc: 0.5445 - val_loss: 1.3366 - val_acc: 0.5940\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 1.2099 - acc: 0.6055 - val_loss: 1.1451 - val_acc: 0.6490\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 116us/step - loss: 1.0280 - acc: 0.6615 - val_loss: 1.0244 - val_acc: 0.6760\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.9294 - acc: 0.6845 - val_loss: 0.9413 - val_acc: 0.7050\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.8535 - acc: 0.7220 - val_loss: 0.8739 - val_acc: 0.7240\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 111us/step - loss: 0.7875 - acc: 0.7495 - val_loss: 0.8235 - val_acc: 0.7450\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 111us/step - loss: 0.7338 - acc: 0.7690 - val_loss: 0.7849 - val_acc: 0.7580\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 0.6771 - acc: 0.7815 - val_loss: 0.7490 - val_acc: 0.7700\n",
      "Test loss: 0.7490091099739075\n",
      "Test accuracy: 0.77\n",
      "value:  0.7490091099739075\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  100\n",
      "X:  [array([ 5.69984776,  1.13908093, -2.61891754, -1.81115463,  2.69711049,\n",
      "        4.934929  , -2.97598665,  3.7451673 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 21s 10ms/step - loss: 1.7903 - acc: 0.4680 - val_loss: 1.3963 - val_acc: 0.6160\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 1.1011 - acc: 0.6880 - val_loss: 1.0008 - val_acc: 0.7180\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.7823 - acc: 0.7790 - val_loss: 0.8183 - val_acc: 0.7560\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.6464 - acc: 0.8085 - val_loss: 0.7041 - val_acc: 0.7930\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 111us/step - loss: 0.5361 - acc: 0.8510 - val_loss: 0.6443 - val_acc: 0.8070\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.4731 - acc: 0.8605 - val_loss: 0.6309 - val_acc: 0.8050\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 0.4291 - acc: 0.8785 - val_loss: 0.5777 - val_acc: 0.8260\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 0.3998 - acc: 0.8810 - val_loss: 0.5391 - val_acc: 0.8380\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.3744 - acc: 0.8930 - val_loss: 0.5326 - val_acc: 0.8380\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.3492 - acc: 0.9000 - val_loss: 0.5075 - val_acc: 0.8440\n",
      "Test loss: 0.5074824728965759\n",
      "Test accuracy: 0.844\n",
      "value:  0.5074824728965759\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  101\n",
      "X:  [array([ 5.88474192,  0.75547018, -2.64860065, -1.30082551,  2.14310366,\n",
      "        3.81901067, -2.95603164,  4.14407192])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.6906 - acc: 0.4870 - val_loss: 1.3544 - val_acc: 0.6500\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 1.0664 - acc: 0.7220 - val_loss: 0.9798 - val_acc: 0.7310\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.7902 - acc: 0.7875 - val_loss: 0.7932 - val_acc: 0.7780\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 0.6336 - acc: 0.8255 - val_loss: 0.7150 - val_acc: 0.7890\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.5410 - acc: 0.8545 - val_loss: 0.6459 - val_acc: 0.7980\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.4825 - acc: 0.8565 - val_loss: 0.6069 - val_acc: 0.8100\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.4437 - acc: 0.8690 - val_loss: 0.5849 - val_acc: 0.8120\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.4020 - acc: 0.8850 - val_loss: 0.5673 - val_acc: 0.8200\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.3764 - acc: 0.8860 - val_loss: 0.5689 - val_acc: 0.8180\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.3581 - acc: 0.9000 - val_loss: 0.5365 - val_acc: 0.8330\n",
      "Test loss: 0.5365146656036377\n",
      "Test accuracy: 0.833\n",
      "value:  0.5365146656036377\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  102\n",
      "X:  [array([ 4.59614279,  3.32351623, -1.64774536, -3.55956429, -0.822075  ,\n",
      "        6.26098141, -2.51249048, -0.18356209])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 1.7848 - acc: 0.4910 - val_loss: 1.3967 - val_acc: 0.6340\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 1.0968 - acc: 0.7045 - val_loss: 1.0192 - val_acc: 0.6760\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 0.8060 - acc: 0.7695 - val_loss: 0.8189 - val_acc: 0.7420\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.6540 - acc: 0.8055 - val_loss: 0.7295 - val_acc: 0.7690\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.5556 - acc: 0.8365 - val_loss: 0.6568 - val_acc: 0.7920\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.4818 - acc: 0.8590 - val_loss: 0.6051 - val_acc: 0.8080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.4431 - acc: 0.8695 - val_loss: 0.5667 - val_acc: 0.8210\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.3978 - acc: 0.8785 - val_loss: 0.5488 - val_acc: 0.8300\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.3629 - acc: 0.8970 - val_loss: 0.5212 - val_acc: 0.8400\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.3346 - acc: 0.9070 - val_loss: 0.5273 - val_acc: 0.8360\n",
      "Test loss: 0.52726726603508\n",
      "Test accuracy: 0.836\n",
      "value:  0.52726726603508\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  103\n",
      "X:  [array([ 6.21913248,  2.13389713, -0.78085445, -1.75754791,  2.81593603,\n",
      "        3.62021828, -2.01543184,  1.86825632])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 1.8102 - acc: 0.4450 - val_loss: 1.4409 - val_acc: 0.5800\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 1.1531 - acc: 0.6805 - val_loss: 1.0620 - val_acc: 0.7070\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.8632 - acc: 0.7530 - val_loss: 0.8569 - val_acc: 0.7610\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.6984 - acc: 0.7940 - val_loss: 0.7444 - val_acc: 0.7810\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.5985 - acc: 0.8240 - val_loss: 0.6701 - val_acc: 0.8010\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.5328 - acc: 0.8445 - val_loss: 0.6279 - val_acc: 0.8020\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.4888 - acc: 0.8530 - val_loss: 0.5892 - val_acc: 0.8260\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.4245 - acc: 0.8745 - val_loss: 0.5782 - val_acc: 0.8210\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.4131 - acc: 0.8765 - val_loss: 0.5447 - val_acc: 0.8280\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.3800 - acc: 0.8845 - val_loss: 0.5270 - val_acc: 0.8410\n",
      "Test loss: 0.5269650466442108\n",
      "Test accuracy: 0.841\n",
      "value:  0.5269650466442108\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  104\n",
      "X:  [array([ 6.46485607,  1.77355226, -3.10631143,  2.99035495,  0.37208337,\n",
      "        6.8093693 , -3.32767385,  2.92412485])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 13s 6ms/step - loss: 1.9179 - acc: 0.4250 - val_loss: 1.6298 - val_acc: 0.6010\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 1.3427 - acc: 0.6980 - val_loss: 1.2691 - val_acc: 0.6850\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 1.0383 - acc: 0.7480 - val_loss: 1.0363 - val_acc: 0.7260\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 100us/step - loss: 0.8490 - acc: 0.7895 - val_loss: 0.8810 - val_acc: 0.7570\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 101us/step - loss: 0.7268 - acc: 0.8145 - val_loss: 0.7768 - val_acc: 0.7810\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.6182 - acc: 0.8300 - val_loss: 0.7329 - val_acc: 0.7890\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.5609 - acc: 0.8485 - val_loss: 0.6555 - val_acc: 0.8100\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 0.5000 - acc: 0.8645 - val_loss: 0.6234 - val_acc: 0.8130\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.4548 - acc: 0.8705 - val_loss: 0.5786 - val_acc: 0.8290\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 175us/step - loss: 0.4259 - acc: 0.8800 - val_loss: 0.5707 - val_acc: 0.8290\n",
      "Test loss: 0.5707325887680054\n",
      "Test accuracy: 0.829\n",
      "value:  0.5707325887680054\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  105\n",
      "X:  [array([ 4.97356962,  3.06784683, -0.08471379,  1.20258299,  0.95967369,\n",
      "        3.9310572 , -3.29150359,  2.86063311])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 17s 8ms/step - loss: 1.8337 - acc: 0.4655 - val_loss: 1.5637 - val_acc: 0.6490\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 113us/step - loss: 1.3077 - acc: 0.6840 - val_loss: 1.2334 - val_acc: 0.7040\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 1.0338 - acc: 0.7455 - val_loss: 1.0446 - val_acc: 0.7400\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.8533 - acc: 0.7855 - val_loss: 0.8866 - val_acc: 0.7640\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 0.7437 - acc: 0.8110 - val_loss: 0.7932 - val_acc: 0.7880\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.6521 - acc: 0.8340 - val_loss: 0.7406 - val_acc: 0.7940\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.5802 - acc: 0.8560 - val_loss: 0.6638 - val_acc: 0.8180\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.5259 - acc: 0.8610 - val_loss: 0.6514 - val_acc: 0.8080\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.4885 - acc: 0.8725 - val_loss: 0.6065 - val_acc: 0.8240\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.4426 - acc: 0.8825 - val_loss: 0.5767 - val_acc: 0.8340\n",
      "Test loss: 0.576670114517212\n",
      "Test accuracy: 0.834\n",
      "value:  0.576670114517212\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  106\n",
      "X:  [array([ 2.2150924 , -0.34895606, -4.28482436, -3.79723209,  0.96787185,\n",
      "        5.47868897, -3.20608584,  3.4148387 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.9018 - acc: 0.4015 - val_loss: 1.6410 - val_acc: 0.5380\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 1.3830 - acc: 0.5905 - val_loss: 1.2909 - val_acc: 0.6260\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 1.0796 - acc: 0.7060 - val_loss: 1.0443 - val_acc: 0.7140\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 0.8638 - acc: 0.7610 - val_loss: 0.8730 - val_acc: 0.7750\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 0.7079 - acc: 0.8040 - val_loss: 0.7795 - val_acc: 0.7880\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 111us/step - loss: 0.6226 - acc: 0.8345 - val_loss: 0.7327 - val_acc: 0.7900\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.5597 - acc: 0.8405 - val_loss: 0.6566 - val_acc: 0.8180\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.4954 - acc: 0.8625 - val_loss: 0.6308 - val_acc: 0.8160\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.4583 - acc: 0.8690 - val_loss: 0.6050 - val_acc: 0.8320\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.4411 - acc: 0.8735 - val_loss: 0.5936 - val_acc: 0.8190\n",
      "Test loss: 0.5935932383537292\n",
      "Test accuracy: 0.819\n",
      "value:  0.5935932383537292\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  107\n",
      "X:  [array([ 2.32807031,  0.71977928, -2.27037035, -3.34793453,  1.72146872,\n",
      "        5.24545912, -2.79782637,  3.42053415])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.8236 - acc: 0.4495 - val_loss: 1.4166 - val_acc: 0.5900\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 116us/step - loss: 1.1353 - acc: 0.6845 - val_loss: 1.0339 - val_acc: 0.7040\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 0.8311 - acc: 0.7685 - val_loss: 0.8533 - val_acc: 0.7480\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 0.6745 - acc: 0.8065 - val_loss: 0.7286 - val_acc: 0.7920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 0.5804 - acc: 0.8425 - val_loss: 0.6702 - val_acc: 0.7930\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 0.4982 - acc: 0.8570 - val_loss: 0.6277 - val_acc: 0.8040\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 0.4659 - acc: 0.8725 - val_loss: 0.5927 - val_acc: 0.8140\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.4059 - acc: 0.8815 - val_loss: 0.5995 - val_acc: 0.8190\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 0.4018 - acc: 0.8810 - val_loss: 0.5539 - val_acc: 0.8250\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 111us/step - loss: 0.3771 - acc: 0.8825 - val_loss: 0.5316 - val_acc: 0.8250\n",
      "Test loss: 0.5315847759246826\n",
      "Test accuracy: 0.825\n",
      "value:  0.5315847759246826\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  108\n",
      "X:  [array([ 3.42443444,  0.7654149 , -1.55790476, -0.78629436, -2.29488837,\n",
      "        2.66414501, -1.10360803,  1.33882929])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.8129 - acc: 0.4635 - val_loss: 1.5562 - val_acc: 0.5890\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 1.2834 - acc: 0.6775 - val_loss: 1.2276 - val_acc: 0.6810\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 1.0189 - acc: 0.7440 - val_loss: 1.0197 - val_acc: 0.7240\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.8357 - acc: 0.7730 - val_loss: 0.8780 - val_acc: 0.7560\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 0.7267 - acc: 0.8000 - val_loss: 0.7747 - val_acc: 0.7820\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.6299 - acc: 0.8305 - val_loss: 0.7046 - val_acc: 0.8010\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.5688 - acc: 0.8425 - val_loss: 0.6595 - val_acc: 0.8060\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.5230 - acc: 0.8480 - val_loss: 0.6258 - val_acc: 0.8140\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.4952 - acc: 0.8545 - val_loss: 0.5900 - val_acc: 0.8260\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 102us/step - loss: 0.4402 - acc: 0.8765 - val_loss: 0.5665 - val_acc: 0.8300\n",
      "Test loss: 0.5664532654285431\n",
      "Test accuracy: 0.83\n",
      "value:  0.5664532654285431\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  109\n",
      "X:  [array([ 6.23719527,  0.86132361, -1.90924491, -4.45448008, -1.64001229,\n",
      "        1.92890529, -0.58681599,  2.55692983])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.7843 - acc: 0.4690 - val_loss: 1.4426 - val_acc: 0.6060\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 1.1537 - acc: 0.6935 - val_loss: 1.0631 - val_acc: 0.6940\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.8619 - acc: 0.7580 - val_loss: 0.8677 - val_acc: 0.7450\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.6905 - acc: 0.8005 - val_loss: 0.7528 - val_acc: 0.7720\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 116us/step - loss: 0.5867 - acc: 0.8340 - val_loss: 0.7024 - val_acc: 0.7880\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.5288 - acc: 0.8525 - val_loss: 0.6627 - val_acc: 0.7970\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 0.4586 - acc: 0.8620 - val_loss: 0.6117 - val_acc: 0.8100\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 111us/step - loss: 0.4258 - acc: 0.8720 - val_loss: 0.5965 - val_acc: 0.8270\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.4081 - acc: 0.8830 - val_loss: 0.5734 - val_acc: 0.8250\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.3784 - acc: 0.8890 - val_loss: 0.5595 - val_acc: 0.8330\n",
      "Test loss: 0.5595020270347595\n",
      "Test accuracy: 0.833\n",
      "value:  0.5595020270347595\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  110\n",
      "X:  [array([ 6.5309046 ,  1.5515505 , -0.88216765,  0.53431505, -1.0245357 ,\n",
      "        8.01113932, -3.46180409,  0.09076313])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.7704 - acc: 0.4600 - val_loss: 1.4214 - val_acc: 0.6460\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 1.0993 - acc: 0.7250 - val_loss: 1.0285 - val_acc: 0.7120\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.8145 - acc: 0.7710 - val_loss: 0.8369 - val_acc: 0.7580\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.6666 - acc: 0.8090 - val_loss: 0.7298 - val_acc: 0.7910\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.5718 - acc: 0.8250 - val_loss: 0.6838 - val_acc: 0.7950\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.5031 - acc: 0.8500 - val_loss: 0.6528 - val_acc: 0.7990\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.4601 - acc: 0.8560 - val_loss: 0.5974 - val_acc: 0.8240\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.4207 - acc: 0.8730 - val_loss: 0.5740 - val_acc: 0.8310\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.3979 - acc: 0.8780 - val_loss: 0.5867 - val_acc: 0.8250\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.3706 - acc: 0.8850 - val_loss: 0.5435 - val_acc: 0.8360\n",
      "Test loss: 0.543512188911438\n",
      "Test accuracy: 0.836\n",
      "value:  0.543512188911438\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  111\n",
      "X:  [array([ 5.78265874,  2.11121008, -1.05475215,  1.94074058,  0.33163139,\n",
      "        3.38917832, -1.98779235, -1.19277322])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.0924 - acc: 0.2520 - val_loss: 1.8368 - val_acc: 0.4420\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 1.6776 - acc: 0.4685 - val_loss: 1.5513 - val_acc: 0.6010\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 1.4264 - acc: 0.5900 - val_loss: 1.3328 - val_acc: 0.7060\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 1.2284 - acc: 0.6520 - val_loss: 1.1426 - val_acc: 0.7460\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 1.0761 - acc: 0.6970 - val_loss: 1.0105 - val_acc: 0.7660\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 104us/step - loss: 0.9535 - acc: 0.7315 - val_loss: 0.9111 - val_acc: 0.7850\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 0.8534 - acc: 0.7515 - val_loss: 0.8286 - val_acc: 0.7770\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.7898 - acc: 0.7700 - val_loss: 0.7735 - val_acc: 0.7880\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.7253 - acc: 0.7955 - val_loss: 0.7264 - val_acc: 0.8010\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.6720 - acc: 0.7905 - val_loss: 0.7090 - val_acc: 0.8050\n",
      "Test loss: 0.709015549659729\n",
      "Test accuracy: 0.805\n",
      "value:  0.709015549659729\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  112\n",
      "X:  [array([ 6.23412363,  0.2952361 , -6.60499207, -4.17968761,  3.26299892,\n",
      "        3.43142602, -2.94057936,  5.95537732])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.7800 - acc: 0.4730 - val_loss: 1.3888 - val_acc: 0.6440\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 1.1228 - acc: 0.7025 - val_loss: 1.0122 - val_acc: 0.7230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.8354 - acc: 0.7755 - val_loss: 0.8310 - val_acc: 0.7540\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.6659 - acc: 0.8110 - val_loss: 0.7139 - val_acc: 0.7830\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 0.5673 - acc: 0.8395 - val_loss: 0.6484 - val_acc: 0.8040\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 0.4990 - acc: 0.8580 - val_loss: 0.6370 - val_acc: 0.8070\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.4485 - acc: 0.8765 - val_loss: 0.5737 - val_acc: 0.8250\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.4212 - acc: 0.8845 - val_loss: 0.5681 - val_acc: 0.8250\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 116us/step - loss: 0.3871 - acc: 0.8815 - val_loss: 0.5377 - val_acc: 0.8380\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.3588 - acc: 0.8970 - val_loss: 0.5253 - val_acc: 0.8460\n",
      "Test loss: 0.5252707891464233\n",
      "Test accuracy: 0.846\n",
      "value:  0.5252707891464233\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  113\n",
      "X:  [array([ 5.61598197,  1.81423399, -1.88065251, -4.92576626,  0.38827258,\n",
      "        3.52251195, -2.54398437,  0.64643267])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.7500 - acc: 0.4970 - val_loss: 1.3774 - val_acc: 0.6550\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 1.1072 - acc: 0.7250 - val_loss: 1.0150 - val_acc: 0.7250\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.8174 - acc: 0.7850 - val_loss: 0.8245 - val_acc: 0.7660\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.6643 - acc: 0.8160 - val_loss: 0.7204 - val_acc: 0.7930\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.5648 - acc: 0.8305 - val_loss: 0.6515 - val_acc: 0.7930\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.4876 - acc: 0.8560 - val_loss: 0.6323 - val_acc: 0.8100\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.4376 - acc: 0.8645 - val_loss: 0.5933 - val_acc: 0.8060\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 103us/step - loss: 0.4006 - acc: 0.8840 - val_loss: 0.5537 - val_acc: 0.8270\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.3716 - acc: 0.8940 - val_loss: 0.5641 - val_acc: 0.8340\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.3547 - acc: 0.8935 - val_loss: 0.5471 - val_acc: 0.8440\n",
      "Test loss: 0.5470981981754303\n",
      "Test accuracy: 0.844\n",
      "value:  0.5470981981754303\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  114\n",
      "X:  [array([ 5.81002684,  0.21248402, -2.25151627, -3.06182005, -0.33230492,\n",
      "        4.21062159, -3.48033303,  2.84864447])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.7929 - acc: 0.4590 - val_loss: 1.4474 - val_acc: 0.6020\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 116us/step - loss: 1.1179 - acc: 0.7065 - val_loss: 1.0250 - val_acc: 0.7070\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.8072 - acc: 0.7840 - val_loss: 0.8302 - val_acc: 0.7510\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 0.6425 - acc: 0.8170 - val_loss: 0.7062 - val_acc: 0.7900\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 0.5501 - acc: 0.8430 - val_loss: 0.6424 - val_acc: 0.8020\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 113us/step - loss: 0.4910 - acc: 0.8595 - val_loss: 0.6135 - val_acc: 0.8130\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 116us/step - loss: 0.4398 - acc: 0.8775 - val_loss: 0.5806 - val_acc: 0.8120\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 121us/step - loss: 0.3995 - acc: 0.8815 - val_loss: 0.5787 - val_acc: 0.8130\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 111us/step - loss: 0.3814 - acc: 0.8865 - val_loss: 0.5684 - val_acc: 0.8240\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.3352 - acc: 0.9045 - val_loss: 0.5236 - val_acc: 0.8360\n",
      "Test loss: 0.5235950438976288\n",
      "Test accuracy: 0.836\n",
      "value:  0.5235950438976288\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  115\n",
      "X:  [array([ 4.25210732,  1.2706129 , -2.05209393, -0.75310199,  1.70785604,\n",
      "        3.7628016 , -3.40986745,  2.01230622])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 17s 8ms/step - loss: 1.7591 - acc: 0.4960 - val_loss: 1.3801 - val_acc: 0.5950\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 1.1053 - acc: 0.6870 - val_loss: 1.0206 - val_acc: 0.7010\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.8254 - acc: 0.7590 - val_loss: 0.8207 - val_acc: 0.7850\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.6530 - acc: 0.8140 - val_loss: 0.7215 - val_acc: 0.7940\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 111us/step - loss: 0.5511 - acc: 0.8405 - val_loss: 0.6506 - val_acc: 0.8130\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.4741 - acc: 0.8655 - val_loss: 0.6173 - val_acc: 0.8160\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 111us/step - loss: 0.4237 - acc: 0.8835 - val_loss: 0.5712 - val_acc: 0.8280\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 113us/step - loss: 0.4024 - acc: 0.8820 - val_loss: 0.5809 - val_acc: 0.8310\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.3743 - acc: 0.8860 - val_loss: 0.5585 - val_acc: 0.8370\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 0.3538 - acc: 0.8975 - val_loss: 0.5584 - val_acc: 0.8450\n",
      "Test loss: 0.5584336712360382\n",
      "Test accuracy: 0.845\n",
      "value:  0.5584336712360382\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  116\n",
      "X:  [array([ 3.05666962,  2.78683172, -1.9876255 , -0.5310049 ,  4.60302761,\n",
      "        3.96915217, -2.85375322, -0.30859595])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 38s 19ms/step - loss: 1.7780 - acc: 0.4670 - val_loss: 1.3850 - val_acc: 0.6260\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 178us/step - loss: 1.1421 - acc: 0.6985 - val_loss: 1.0346 - val_acc: 0.7110\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 200us/step - loss: 0.8571 - acc: 0.7640 - val_loss: 0.8715 - val_acc: 0.7480\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 203us/step - loss: 0.7099 - acc: 0.7915 - val_loss: 0.7512 - val_acc: 0.7780\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 216us/step - loss: 0.5851 - acc: 0.8335 - val_loss: 0.7052 - val_acc: 0.7840\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 230us/step - loss: 0.5309 - acc: 0.8460 - val_loss: 0.6494 - val_acc: 0.8080\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 190us/step - loss: 0.4786 - acc: 0.8650 - val_loss: 0.6212 - val_acc: 0.8200\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.4313 - acc: 0.8710 - val_loss: 0.5946 - val_acc: 0.8210\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.4025 - acc: 0.8815 - val_loss: 0.5988 - val_acc: 0.8250\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 0.3832 - acc: 0.8910 - val_loss: 0.5631 - val_acc: 0.8420\n",
      "Test loss: 0.5631337742805481\n",
      "Test accuracy: 0.842\n",
      "value:  0.5631337742805481\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  117\n",
      "X:  [array([ 5.33280325,  1.2231012 , -0.92139016, -4.74035386,  3.99683596,\n",
      "        5.60420746, -2.90174285,  4.38788255])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 30s 15ms/step - loss: 1.8365 - acc: 0.4715 - val_loss: 1.4882 - val_acc: 0.6250\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 1.1463 - acc: 0.7180 - val_loss: 1.0661 - val_acc: 0.7160\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.8452 - acc: 0.7790 - val_loss: 0.8548 - val_acc: 0.7550\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 0.6535 - acc: 0.8195 - val_loss: 0.7197 - val_acc: 0.7950\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.5666 - acc: 0.8330 - val_loss: 0.6702 - val_acc: 0.8010\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.4964 - acc: 0.8650 - val_loss: 0.6261 - val_acc: 0.8000\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.4381 - acc: 0.8730 - val_loss: 0.5780 - val_acc: 0.8310\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.4027 - acc: 0.8830 - val_loss: 0.5989 - val_acc: 0.8060\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 0.3730 - acc: 0.8975 - val_loss: 0.5379 - val_acc: 0.8280\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.3504 - acc: 0.8950 - val_loss: 0.5260 - val_acc: 0.8410\n",
      "Test loss: 0.5260270953178405\n",
      "Test accuracy: 0.841\n",
      "value:  0.5260270953178405\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  118\n",
      "X:  [array([ 2.95080194, -0.82765317, -5.85895763, -4.82033438,  1.6098045 ,\n",
      "        6.56751126, -1.25741816,  3.95191152])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 1.9004 - acc: 0.4385 - val_loss: 1.5920 - val_acc: 0.6040\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 1.3016 - acc: 0.6650 - val_loss: 1.1930 - val_acc: 0.7210\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.9835 - acc: 0.7530 - val_loss: 0.9531 - val_acc: 0.7520\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.7879 - acc: 0.7910 - val_loss: 0.8240 - val_acc: 0.7740\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.6760 - acc: 0.8080 - val_loss: 0.7523 - val_acc: 0.7830\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.5821 - acc: 0.8410 - val_loss: 0.7018 - val_acc: 0.7940\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.5377 - acc: 0.8480 - val_loss: 0.6536 - val_acc: 0.8010\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 0.4897 - acc: 0.8575 - val_loss: 0.6304 - val_acc: 0.8050\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 116us/step - loss: 0.4471 - acc: 0.8700 - val_loss: 0.6158 - val_acc: 0.8120\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 0.4349 - acc: 0.8705 - val_loss: 0.5866 - val_acc: 0.8220\n",
      "Test loss: 0.5866342720985412\n",
      "Test accuracy: 0.822\n",
      "value:  0.5866342720985412\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  119\n",
      "X:  [array([10.0721148 ,  1.24892961, -2.47784563, -2.82641368,  0.84971768,\n",
      "        9.94774507, -5.05588623,  4.13226366])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 1.8388 - acc: 0.4240 - val_loss: 1.4375 - val_acc: 0.5970\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 1.1806 - acc: 0.6670 - val_loss: 1.0372 - val_acc: 0.7050\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.8589 - acc: 0.7530 - val_loss: 0.8330 - val_acc: 0.7670\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.6848 - acc: 0.8035 - val_loss: 0.7221 - val_acc: 0.7860\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.5743 - acc: 0.8355 - val_loss: 0.6541 - val_acc: 0.8030\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.5096 - acc: 0.8510 - val_loss: 0.5892 - val_acc: 0.8190\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.4585 - acc: 0.8665 - val_loss: 0.5710 - val_acc: 0.8240\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.4083 - acc: 0.8810 - val_loss: 0.5566 - val_acc: 0.8250\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 0.3908 - acc: 0.8865 - val_loss: 0.5393 - val_acc: 0.8380\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.3535 - acc: 0.8980 - val_loss: 0.5282 - val_acc: 0.8380\n",
      "Test loss: 0.5281899089813232\n",
      "Test accuracy: 0.838\n",
      "value:  0.5281899089813232\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  120\n",
      "X:  [array([ 6.81647503, -0.07615418, -4.59270927, -4.41692302,  2.16708229,\n",
      "        4.43620399, -0.25011756,  3.60065932])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 22s 11ms/step - loss: 1.8766 - acc: 0.4350 - val_loss: 1.6121 - val_acc: 0.6000\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 172us/step - loss: 1.3283 - acc: 0.6705 - val_loss: 1.2386 - val_acc: 0.6720\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 233us/step - loss: 1.0171 - acc: 0.7260 - val_loss: 1.0037 - val_acc: 0.7270\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.8240 - acc: 0.7785 - val_loss: 0.8775 - val_acc: 0.7500\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 153us/step - loss: 0.6909 - acc: 0.8220 - val_loss: 0.7791 - val_acc: 0.7680\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.5956 - acc: 0.8345 - val_loss: 0.7216 - val_acc: 0.7900\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 155us/step - loss: 0.5360 - acc: 0.8535 - val_loss: 0.6725 - val_acc: 0.8010\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 158us/step - loss: 0.4887 - acc: 0.8625 - val_loss: 0.6333 - val_acc: 0.8050\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.4515 - acc: 0.8655 - val_loss: 0.6108 - val_acc: 0.8080\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.4184 - acc: 0.8770 - val_loss: 0.6066 - val_acc: 0.8110\n",
      "Test loss: 0.6066470031738281\n",
      "Test accuracy: 0.811\n",
      "value:  0.6066470031738281\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  121\n",
      "X:  [array([ 6.4047153 ,  0.09517342,  0.58794757, -4.60719967, -2.36914225,\n",
      "        5.50197719, -4.34667546,  2.83412753])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 24s 12ms/step - loss: 1.9934 - acc: 0.3465 - val_loss: 1.6021 - val_acc: 0.6240\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 1.4751 - acc: 0.5710 - val_loss: 1.1995 - val_acc: 0.7040\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 1.1639 - acc: 0.6485 - val_loss: 0.9622 - val_acc: 0.7450\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 1.0543 - acc: 0.6760 - val_loss: 0.8380 - val_acc: 0.7730\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.9297 - acc: 0.7055 - val_loss: 0.7519 - val_acc: 0.7970\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.8796 - acc: 0.7170 - val_loss: 0.6877 - val_acc: 0.8070\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.8345 - acc: 0.7160 - val_loss: 0.6758 - val_acc: 0.8020\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.7622 - acc: 0.7515 - val_loss: 0.6275 - val_acc: 0.8130\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.7000 - acc: 0.7675 - val_loss: 0.5933 - val_acc: 0.8350\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.7207 - acc: 0.7605 - val_loss: 0.5869 - val_acc: 0.8120\n",
      "Test loss: 0.5868525202274323\n",
      "Test accuracy: 0.812\n",
      "value:  0.5868525202274323\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  122\n",
      "X:  [array([ 2.25574641, -0.7699617 , -5.37358482, -4.84815708,  2.19457577,\n",
      "        4.97692241, -3.11257129,  2.63281395])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.8902 - acc: 0.4165 - val_loss: 1.5922 - val_acc: 0.5440\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 1.3226 - acc: 0.6410 - val_loss: 1.2121 - val_acc: 0.6560\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 1.0160 - acc: 0.7235 - val_loss: 0.9968 - val_acc: 0.7260\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 116us/step - loss: 0.8456 - acc: 0.7630 - val_loss: 0.8720 - val_acc: 0.7520\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.7081 - acc: 0.7985 - val_loss: 0.7722 - val_acc: 0.7780\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.6242 - acc: 0.8315 - val_loss: 0.7087 - val_acc: 0.7860\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 158us/step - loss: 0.5601 - acc: 0.8535 - val_loss: 0.6818 - val_acc: 0.7930\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.5149 - acc: 0.8535 - val_loss: 0.6432 - val_acc: 0.8010\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 121us/step - loss: 0.4727 - acc: 0.8595 - val_loss: 0.6458 - val_acc: 0.8110\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.4582 - acc: 0.8680 - val_loss: 0.6216 - val_acc: 0.8090\n",
      "Test loss: 0.6215929484367371\n",
      "Test accuracy: 0.809\n",
      "value:  0.6215929484367371\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  123\n",
      "X:  [array([ 7.21353357,  1.20266689, -4.0412532 , -2.95481994,  0.59657133,\n",
      "        6.14319024, -1.96503116,  0.75817951])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 20s 10ms/step - loss: 1.7518 - acc: 0.4690 - val_loss: 1.3976 - val_acc: 0.6330\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 1.1112 - acc: 0.6925 - val_loss: 1.0419 - val_acc: 0.7040\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.8320 - acc: 0.7620 - val_loss: 0.8458 - val_acc: 0.7620\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.6832 - acc: 0.7955 - val_loss: 0.7445 - val_acc: 0.7690\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.5773 - acc: 0.8285 - val_loss: 0.7027 - val_acc: 0.7770\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.5080 - acc: 0.8545 - val_loss: 0.6278 - val_acc: 0.8090\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.4681 - acc: 0.8600 - val_loss: 0.5995 - val_acc: 0.8140\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.4267 - acc: 0.8700 - val_loss: 0.5707 - val_acc: 0.8240\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 116us/step - loss: 0.3976 - acc: 0.8880 - val_loss: 0.5469 - val_acc: 0.8390\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.3712 - acc: 0.8930 - val_loss: 0.5327 - val_acc: 0.8360\n",
      "Test loss: 0.5327401804924011\n",
      "Test accuracy: 0.836\n",
      "value:  0.5327401804924011\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  124\n",
      "X:  [array([ 4.74776119, -1.06405269, -6.91559596, -3.7433817 ,  3.07366364,\n",
      "        3.6067618 , -6.21655549,  3.72219712])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.9201 - acc: 0.3705 - val_loss: 1.6236 - val_acc: 0.5230\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 170us/step - loss: 1.3520 - acc: 0.5915 - val_loss: 1.2364 - val_acc: 0.6260\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 202us/step - loss: 1.0642 - acc: 0.6670 - val_loss: 1.0423 - val_acc: 0.6730\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 157us/step - loss: 0.8928 - acc: 0.7125 - val_loss: 0.9046 - val_acc: 0.7390\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 233us/step - loss: 0.7890 - acc: 0.7510 - val_loss: 0.8314 - val_acc: 0.7390\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 183us/step - loss: 0.7011 - acc: 0.7770 - val_loss: 0.7939 - val_acc: 0.7480\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 191us/step - loss: 0.6604 - acc: 0.7845 - val_loss: 0.7332 - val_acc: 0.7650\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 165us/step - loss: 0.5796 - acc: 0.8205 - val_loss: 0.6926 - val_acc: 0.7830\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 176us/step - loss: 0.5312 - acc: 0.8375 - val_loss: 0.6598 - val_acc: 0.7980\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 167us/step - loss: 0.5030 - acc: 0.8455 - val_loss: 0.6522 - val_acc: 0.7920\n",
      "Test loss: 0.6522130780220031\n",
      "Test accuracy: 0.792\n",
      "value:  0.6522130780220031\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  125\n",
      "X:  [array([ 8.63608188, -1.29717941, -3.95556973, -4.72019254,  2.72761173,\n",
      "        4.52419628, -6.81667344,  4.20198987])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 20s 10ms/step - loss: 1.8911 - acc: 0.4220 - val_loss: 1.6271 - val_acc: 0.4880\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 1.3379 - acc: 0.5925 - val_loss: 1.2891 - val_acc: 0.5970\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 1.0566 - acc: 0.6865 - val_loss: 1.0545 - val_acc: 0.6830\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.8629 - acc: 0.7525 - val_loss: 0.9068 - val_acc: 0.7250\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.7372 - acc: 0.7820 - val_loss: 0.8082 - val_acc: 0.7460\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.6404 - acc: 0.8215 - val_loss: 0.7395 - val_acc: 0.7650\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 116us/step - loss: 0.5759 - acc: 0.8325 - val_loss: 0.6895 - val_acc: 0.7790\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.5168 - acc: 0.8465 - val_loss: 0.6470 - val_acc: 0.7970\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.4750 - acc: 0.8630 - val_loss: 0.6172 - val_acc: 0.7990\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 121us/step - loss: 0.4474 - acc: 0.8670 - val_loss: 0.5948 - val_acc: 0.8110\n",
      "Test loss: 0.5947833647727966\n",
      "Test accuracy: 0.811\n",
      "value:  0.5947833647727966\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  126\n",
      "X:  [array([ 4.42960879, -1.02203238, -6.497026  , -1.27049445,  5.43251215,\n",
      "        1.13799741, -2.71232044,  2.78826077])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 28s 14ms/step - loss: 1.9633 - acc: 0.3840 - val_loss: 1.6959 - val_acc: 0.4910\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 149us/step - loss: 1.4183 - acc: 0.6010 - val_loss: 1.3267 - val_acc: 0.6230\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 1.0825 - acc: 0.7010 - val_loss: 1.0687 - val_acc: 0.6890\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.8598 - acc: 0.7590 - val_loss: 0.9143 - val_acc: 0.7330\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 0.7292 - acc: 0.7910 - val_loss: 0.8088 - val_acc: 0.7690\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 121us/step - loss: 0.6179 - acc: 0.8240 - val_loss: 0.7298 - val_acc: 0.7880\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.5571 - acc: 0.8380 - val_loss: 0.6816 - val_acc: 0.7930\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.5082 - acc: 0.8525 - val_loss: 0.6393 - val_acc: 0.8080\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 162us/step - loss: 0.4793 - acc: 0.8630 - val_loss: 0.6086 - val_acc: 0.8160\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 0.4311 - acc: 0.8790 - val_loss: 0.5908 - val_acc: 0.8140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5907617163658142\n",
      "Test accuracy: 0.814\n",
      "value:  0.5907617163658142\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  127\n",
      "X:  [array([ 6.36582162,  0.39449516, -3.24868479, -5.31907289, -1.18218184,\n",
      "        6.30939312, -4.20020724,  1.70626313])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 22s 11ms/step - loss: 1.7389 - acc: 0.4710 - val_loss: 1.3865 - val_acc: 0.6130\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 158us/step - loss: 1.1121 - acc: 0.6905 - val_loss: 1.0028 - val_acc: 0.7280\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.8223 - acc: 0.7750 - val_loss: 0.7789 - val_acc: 0.7950\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.6486 - acc: 0.8260 - val_loss: 0.6597 - val_acc: 0.8250\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 115us/step - loss: 0.5418 - acc: 0.8505 - val_loss: 0.6079 - val_acc: 0.8130\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 0.4775 - acc: 0.8590 - val_loss: 0.5719 - val_acc: 0.8230\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.4186 - acc: 0.8810 - val_loss: 0.5301 - val_acc: 0.8380\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.3836 - acc: 0.8900 - val_loss: 0.5178 - val_acc: 0.8430\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.3661 - acc: 0.8905 - val_loss: 0.5250 - val_acc: 0.8330\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.3462 - acc: 0.8925 - val_loss: 0.4978 - val_acc: 0.8460\n",
      "Test loss: 0.4977524590492248\n",
      "Test accuracy: 0.846\n",
      "value:  0.4977524590492248\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  128\n",
      "X:  [array([ 3.13437311, -0.24181024, -4.37593447, -1.04997563,  3.60248165,\n",
      "        5.76909076, -2.50732972,  2.15552268])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 31s 16ms/step - loss: 1.8383 - acc: 0.4560 - val_loss: 1.5385 - val_acc: 0.6030\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 1.2713 - acc: 0.6655 - val_loss: 1.1924 - val_acc: 0.6660\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.9985 - acc: 0.7270 - val_loss: 0.9958 - val_acc: 0.7190\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.8351 - acc: 0.7610 - val_loss: 0.8703 - val_acc: 0.7340\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 0.7145 - acc: 0.8025 - val_loss: 0.8086 - val_acc: 0.7490\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 0.6574 - acc: 0.8050 - val_loss: 0.7526 - val_acc: 0.7760\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.5942 - acc: 0.8235 - val_loss: 0.7004 - val_acc: 0.7860\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.5428 - acc: 0.8440 - val_loss: 0.6690 - val_acc: 0.8060\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.5074 - acc: 0.8465 - val_loss: 0.6427 - val_acc: 0.8160\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.4653 - acc: 0.8605 - val_loss: 0.6069 - val_acc: 0.8210\n",
      "Test loss: 0.6068514037132263\n",
      "Test accuracy: 0.821\n",
      "value:  0.6068514037132263\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  129\n",
      "X:  [array([ 5.58627083,  0.54818315, -2.09134127, -4.89561073,  3.93244714,\n",
      "        6.60696138, -3.16976609,  8.2587011 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 21s 10ms/step - loss: 1.8292 - acc: 0.4515 - val_loss: 1.4418 - val_acc: 0.6320\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 1.1422 - acc: 0.7190 - val_loss: 1.0027 - val_acc: 0.7500\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.8029 - acc: 0.7980 - val_loss: 0.8079 - val_acc: 0.7590\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 249us/step - loss: 0.6585 - acc: 0.8150 - val_loss: 0.7098 - val_acc: 0.7940\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.5716 - acc: 0.8375 - val_loss: 0.6484 - val_acc: 0.8060\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 192us/step - loss: 0.4969 - acc: 0.8625 - val_loss: 0.6260 - val_acc: 0.8100\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.4563 - acc: 0.8605 - val_loss: 0.5816 - val_acc: 0.8240\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.4087 - acc: 0.8830 - val_loss: 0.5925 - val_acc: 0.8040\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.3813 - acc: 0.8810 - val_loss: 0.5497 - val_acc: 0.8340\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.3691 - acc: 0.8910 - val_loss: 0.5388 - val_acc: 0.8350\n",
      "Test loss: 0.5387520217895507\n",
      "Test accuracy: 0.835\n",
      "value:  0.5387520217895507\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  130\n",
      "X:  [array([ 4.05777456,  0.99039393, -5.84459139, -4.81555895,  1.38503545,\n",
      "        4.44923466, -2.00692369,  2.5164312 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 21s 11ms/step - loss: 1.8043 - acc: 0.4720 - val_loss: 1.4071 - val_acc: 0.6040\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 1.1162 - acc: 0.6955 - val_loss: 1.0188 - val_acc: 0.7170\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.8160 - acc: 0.7805 - val_loss: 0.8052 - val_acc: 0.7750\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.6452 - acc: 0.8145 - val_loss: 0.7031 - val_acc: 0.7970\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.5538 - acc: 0.8360 - val_loss: 0.6489 - val_acc: 0.8110\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.4924 - acc: 0.8630 - val_loss: 0.6042 - val_acc: 0.8110\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.4366 - acc: 0.8715 - val_loss: 0.5872 - val_acc: 0.8240\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.4192 - acc: 0.8720 - val_loss: 0.5498 - val_acc: 0.8340\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.3790 - acc: 0.8955 - val_loss: 0.5322 - val_acc: 0.8260\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.3614 - acc: 0.8950 - val_loss: 0.5194 - val_acc: 0.8450\n",
      "Test loss: 0.5193825068473816\n",
      "Test accuracy: 0.845\n",
      "value:  0.5193825068473816\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  131\n",
      "X:  [array([ 6.85342905, -0.37044126, -3.69317821, -7.45406089,  2.25792854,\n",
      "        7.06767892, -2.54601174,  2.54506518])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 29s 14ms/step - loss: 1.9238 - acc: 0.4165 - val_loss: 1.5843 - val_acc: 0.6100\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 195us/step - loss: 1.3353 - acc: 0.6775 - val_loss: 1.2167 - val_acc: 0.6760\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 178us/step - loss: 1.0283 - acc: 0.7300 - val_loss: 0.9916 - val_acc: 0.7430\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 155us/step - loss: 0.8415 - acc: 0.7730 - val_loss: 0.8681 - val_acc: 0.7540\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 152us/step - loss: 0.7281 - acc: 0.7880 - val_loss: 0.7753 - val_acc: 0.7820\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 169us/step - loss: 0.6605 - acc: 0.8050 - val_loss: 0.7218 - val_acc: 0.7800\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 167us/step - loss: 0.5919 - acc: 0.8230 - val_loss: 0.6933 - val_acc: 0.7870\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.5366 - acc: 0.8360 - val_loss: 0.6588 - val_acc: 0.7940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.4862 - acc: 0.8550 - val_loss: 0.6281 - val_acc: 0.8070\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 154us/step - loss: 0.4719 - acc: 0.8610 - val_loss: 0.6138 - val_acc: 0.8070\n",
      "Test loss: 0.6138137512207031\n",
      "Test accuracy: 0.807\n",
      "value:  0.6138137512207031\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  132\n",
      "X:  [array([ 9.61086907,  1.88530356, -1.75632364, -6.82201998, -0.77500139,\n",
      "        6.40650459, -5.34875179,  2.66649627])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 29s 14ms/step - loss: 1.7997 - acc: 0.4515 - val_loss: 1.4558 - val_acc: 0.5890\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 224us/step - loss: 1.1250 - acc: 0.7000 - val_loss: 1.0802 - val_acc: 0.6860\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 0.8472 - acc: 0.7640 - val_loss: 0.8841 - val_acc: 0.7260\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.6794 - acc: 0.8025 - val_loss: 0.7708 - val_acc: 0.7690\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 0.5848 - acc: 0.8275 - val_loss: 0.7005 - val_acc: 0.7820\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 121us/step - loss: 0.5158 - acc: 0.8450 - val_loss: 0.6440 - val_acc: 0.7990\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.4536 - acc: 0.8710 - val_loss: 0.6400 - val_acc: 0.8000\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.4311 - acc: 0.8635 - val_loss: 0.6165 - val_acc: 0.8090\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.3907 - acc: 0.8765 - val_loss: 0.5942 - val_acc: 0.8260\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.3704 - acc: 0.8940 - val_loss: 0.5976 - val_acc: 0.8130\n",
      "Test loss: 0.597639371395111\n",
      "Test accuracy: 0.813\n",
      "value:  0.597639371395111\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  133\n",
      "X:  [array([ 3.96350083, -1.25502735,  0.41694269, -6.11110587, -1.93458796,\n",
      "        6.12379297, -1.88370528,  1.77408941])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 19s 10ms/step - loss: 2.0810 - acc: 0.2760 - val_loss: 1.7785 - val_acc: 0.4860\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 1.6452 - acc: 0.4655 - val_loss: 1.4177 - val_acc: 0.6050\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 1.3409 - acc: 0.5595 - val_loss: 1.1932 - val_acc: 0.6290\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 1.2006 - acc: 0.5795 - val_loss: 1.0611 - val_acc: 0.6780\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 1.0928 - acc: 0.6150 - val_loss: 0.9567 - val_acc: 0.7150\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 121us/step - loss: 1.0430 - acc: 0.6380 - val_loss: 0.8977 - val_acc: 0.7260\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.9741 - acc: 0.6510 - val_loss: 0.8544 - val_acc: 0.7420\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.9241 - acc: 0.6790 - val_loss: 0.7969 - val_acc: 0.7650\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.8818 - acc: 0.6925 - val_loss: 0.7815 - val_acc: 0.7680\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 160us/step - loss: 0.8700 - acc: 0.6955 - val_loss: 0.7324 - val_acc: 0.7910\n",
      "Test loss: 0.7324194474220276\n",
      "Test accuracy: 0.791\n",
      "value:  0.7324194474220276\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  134\n",
      "X:  [array([ 6.08710243,  1.1064971 , -4.94798519, -3.05557641, -0.11009849,\n",
      "        9.2434712 , -3.35382564,  2.915663  ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 20s 10ms/step - loss: 1.7938 - acc: 0.4335 - val_loss: 1.4014 - val_acc: 0.6290\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 121us/step - loss: 1.1167 - acc: 0.6885 - val_loss: 1.0225 - val_acc: 0.7180\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 0.8223 - acc: 0.7765 - val_loss: 0.8240 - val_acc: 0.7770\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.6561 - acc: 0.8210 - val_loss: 0.7456 - val_acc: 0.7880\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 0.5712 - acc: 0.8315 - val_loss: 0.6435 - val_acc: 0.8280\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.4985 - acc: 0.8590 - val_loss: 0.6070 - val_acc: 0.8250\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.4512 - acc: 0.8725 - val_loss: 0.5876 - val_acc: 0.8300\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.4090 - acc: 0.8810 - val_loss: 0.5491 - val_acc: 0.8500\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 0.3794 - acc: 0.8865 - val_loss: 0.5459 - val_acc: 0.8440\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 176us/step - loss: 0.3390 - acc: 0.9040 - val_loss: 0.5311 - val_acc: 0.8430\n",
      "Test loss: 0.5311318688392639\n",
      "Test accuracy: 0.843\n",
      "value:  0.5311318688392639\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  135\n",
      "X:  [array([ 8.5297387 , -0.1081356 , -1.39438174, -6.44220053, -2.25049455,\n",
      "        6.2068134 , -4.72170383,  3.37953875])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 20s 10ms/step - loss: 1.9173 - acc: 0.3900 - val_loss: 1.6031 - val_acc: 0.6050\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 1.3570 - acc: 0.6235 - val_loss: 1.2372 - val_acc: 0.6800\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 1.0448 - acc: 0.7130 - val_loss: 1.0172 - val_acc: 0.7210\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 157us/step - loss: 0.8316 - acc: 0.7610 - val_loss: 0.8529 - val_acc: 0.7520\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.7022 - acc: 0.8075 - val_loss: 0.7680 - val_acc: 0.7630\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 0.6253 - acc: 0.8175 - val_loss: 0.7091 - val_acc: 0.7760\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.5626 - acc: 0.8360 - val_loss: 0.6624 - val_acc: 0.7880\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 202us/step - loss: 0.5160 - acc: 0.8460 - val_loss: 0.6358 - val_acc: 0.8020\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.4646 - acc: 0.8570 - val_loss: 0.6148 - val_acc: 0.7990\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.4555 - acc: 0.8630 - val_loss: 0.5777 - val_acc: 0.8090\n",
      "Test loss: 0.5776530499458313\n",
      "Test accuracy: 0.809\n",
      "value:  0.5776530499458313\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  136\n",
      "X:  [array([ 7.17170681,  0.65929043, -1.74269125, -4.84762345, -2.76961495,\n",
      "        4.64231065, -5.25243873,  2.72000317])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 19s 10ms/step - loss: 1.9524 - acc: 0.3610 - val_loss: 1.7107 - val_acc: 0.5260\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 161us/step - loss: 1.4138 - acc: 0.6670 - val_loss: 1.3657 - val_acc: 0.6600\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 1.0958 - acc: 0.7530 - val_loss: 1.1192 - val_acc: 0.7260\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 186us/step - loss: 0.8927 - acc: 0.7985 - val_loss: 0.9471 - val_acc: 0.7540\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.7484 - acc: 0.8135 - val_loss: 0.8306 - val_acc: 0.7660\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.6470 - acc: 0.837 - 0s 164us/step - loss: 0.6480 - acc: 0.8350 - val_loss: 0.7669 - val_acc: 0.7840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 0.5743 - acc: 0.8530 - val_loss: 0.6871 - val_acc: 0.7980\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.5208 - acc: 0.862 - 0s 132us/step - loss: 0.5191 - acc: 0.8625 - val_loss: 0.6465 - val_acc: 0.8140\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.4739 - acc: 0.8680 - val_loss: 0.6144 - val_acc: 0.8190\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 0.4458 - acc: 0.8735 - val_loss: 0.5868 - val_acc: 0.8290\n",
      "Test loss: 0.5868436901569366\n",
      "Test accuracy: 0.829\n",
      "value:  0.5868436901569366\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  137\n",
      "X:  [array([ 8.02868695,  2.38846659, -4.95172512, -8.42715683,  0.96871073,\n",
      "        7.74187157, -3.79297271,  1.2791495 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.8840 - acc: 0.4195 - val_loss: 1.5354 - val_acc: 0.5660\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 1.2485 - acc: 0.6650 - val_loss: 1.1469 - val_acc: 0.6840\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.9151 - acc: 0.7530 - val_loss: 0.9395 - val_acc: 0.7260\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.7266 - acc: 0.7955 - val_loss: 0.7704 - val_acc: 0.7770\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.5969 - acc: 0.8270 - val_loss: 0.6789 - val_acc: 0.8020\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 0.5312 - acc: 0.8400 - val_loss: 0.6343 - val_acc: 0.8110\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.4590 - acc: 0.8645 - val_loss: 0.6001 - val_acc: 0.8190\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 197us/step - loss: 0.4496 - acc: 0.8710 - val_loss: 0.5808 - val_acc: 0.8230\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 164us/step - loss: 0.3822 - acc: 0.8860 - val_loss: 0.5451 - val_acc: 0.8280\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 152us/step - loss: 0.3718 - acc: 0.8885 - val_loss: 0.5418 - val_acc: 0.8380\n",
      "Test loss: 0.5417812223434448\n",
      "Test accuracy: 0.838\n",
      "value:  0.5417812223434448\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  138\n",
      "X:  [array([ 8.6768063 ,  1.36837974, -2.83176543, -3.53021556,  2.58131879,\n",
      "        7.93473924, -4.27005309,  2.91657774])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 1.8098 - acc: 0.4320 - val_loss: 1.4809 - val_acc: 0.5570\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 1.1898 - acc: 0.6770 - val_loss: 1.1026 - val_acc: 0.7090\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 0.9060 - acc: 0.7610 - val_loss: 0.9100 - val_acc: 0.7420\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 0.7193 - acc: 0.8005 - val_loss: 0.7613 - val_acc: 0.7820\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.6066 - acc: 0.8315 - val_loss: 0.6972 - val_acc: 0.7950\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.5355 - acc: 0.8410 - val_loss: 0.6212 - val_acc: 0.8190\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.4647 - acc: 0.8710 - val_loss: 0.5864 - val_acc: 0.8270\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.4271 - acc: 0.8700 - val_loss: 0.5619 - val_acc: 0.8360\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 0.4051 - acc: 0.8830 - val_loss: 0.5526 - val_acc: 0.8340\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.3701 - acc: 0.8900 - val_loss: 0.5277 - val_acc: 0.8450\n",
      "Test loss: 0.5276754479408264\n",
      "Test accuracy: 0.845\n",
      "value:  0.5276754479408264\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  139\n",
      "X:  [array([ 7.93590415,  0.86511798, -2.17767066, -1.7175999 , -0.12235035,\n",
      "        6.48454177, -3.18368726,  1.73596397])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.8378 - acc: 0.4410 - val_loss: 1.5693 - val_acc: 0.6020\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 1.2772 - acc: 0.6895 - val_loss: 1.2205 - val_acc: 0.6910\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 1.0198 - acc: 0.7445 - val_loss: 1.0170 - val_acc: 0.7270\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.8246 - acc: 0.7900 - val_loss: 0.8792 - val_acc: 0.7590\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 0.7119 - acc: 0.8120 - val_loss: 0.7933 - val_acc: 0.7850\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 0.6505 - acc: 0.8235 - val_loss: 0.7295 - val_acc: 0.7960\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.5705 - acc: 0.8465 - val_loss: 0.6854 - val_acc: 0.8010\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.5205 - acc: 0.8640 - val_loss: 0.6414 - val_acc: 0.8140\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.4735 - acc: 0.8730 - val_loss: 0.6190 - val_acc: 0.8080\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.4481 - acc: 0.8830 - val_loss: 0.5999 - val_acc: 0.8150\n",
      "Test loss: 0.5999332513809205\n",
      "Test accuracy: 0.815\n",
      "value:  0.5999332513809205\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  140\n",
      "X:  [array([ 7.04477377,  2.26189823, -2.25026332, -4.7300546 , -0.8146698 ,\n",
      "        7.80886016, -3.37289685,  3.9254207 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.9413 - acc: 0.3865 - val_loss: 1.5613 - val_acc: 0.6490\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 1.2338 - acc: 0.7150 - val_loss: 1.1054 - val_acc: 0.7160\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.8770 - acc: 0.7810 - val_loss: 0.8760 - val_acc: 0.7610\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.6952 - acc: 0.8090 - val_loss: 0.7431 - val_acc: 0.7930\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.5691 - acc: 0.8425 - val_loss: 0.6679 - val_acc: 0.8130\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.4922 - acc: 0.8575 - val_loss: 0.6261 - val_acc: 0.8260\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.4588 - acc: 0.8585 - val_loss: 0.5908 - val_acc: 0.8240\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 0.4137 - acc: 0.8870 - val_loss: 0.5757 - val_acc: 0.8400\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.3934 - acc: 0.8910 - val_loss: 0.5726 - val_acc: 0.8270\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.3500 - acc: 0.8945 - val_loss: 0.5749 - val_acc: 0.8210\n",
      "Test loss: 0.5749449725151062\n",
      "Test accuracy: 0.821\n",
      "value:  0.5749449725151062\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  141\n",
      "X:  [array([ 4.14717299,  1.66495555, -3.69313221, -4.71263051,  3.45290696,\n",
      "        4.02130699, -3.54582262,  2.28647911])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.7368 - acc: 0.5105 - val_loss: 1.3820 - val_acc: 0.6490\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 1.0886 - acc: 0.7135 - val_loss: 1.0151 - val_acc: 0.7080\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 0.8066 - acc: 0.7830 - val_loss: 0.8050 - val_acc: 0.7680\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 134us/step - loss: 0.6569 - acc: 0.8170 - val_loss: 0.7132 - val_acc: 0.7940\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.5476 - acc: 0.8420 - val_loss: 0.6451 - val_acc: 0.8030\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.4796 - acc: 0.8565 - val_loss: 0.5963 - val_acc: 0.8230\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.4324 - acc: 0.8735 - val_loss: 0.5797 - val_acc: 0.8150\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.4017 - acc: 0.8820 - val_loss: 0.5333 - val_acc: 0.8410\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.3757 - acc: 0.8915 - val_loss: 0.5414 - val_acc: 0.8330\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.3434 - acc: 0.9040 - val_loss: 0.5154 - val_acc: 0.8500\n",
      "Test loss: 0.5153992247581481\n",
      "Test accuracy: 0.85\n",
      "value:  0.5153992247581481\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  142\n",
      "X:  [array([ 5.84692284, -0.13921402, -1.8476169 , -4.5630084 , -2.02197626,\n",
      "        4.58059158, -4.3693334 ,  4.69654672])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.8631 - acc: 0.4295 - val_loss: 1.5866 - val_acc: 0.5790\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 1.3028 - acc: 0.6465 - val_loss: 1.2084 - val_acc: 0.6770\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 152us/step - loss: 0.9903 - acc: 0.7300 - val_loss: 0.9888 - val_acc: 0.7360\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 183us/step - loss: 0.8074 - acc: 0.7640 - val_loss: 0.8571 - val_acc: 0.7620\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.6881 - acc: 0.8120 - val_loss: 0.7989 - val_acc: 0.7610\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 154us/step - loss: 0.6013 - acc: 0.8370 - val_loss: 0.7235 - val_acc: 0.7860\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.5445 - acc: 0.8490 - val_loss: 0.6897 - val_acc: 0.7830\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 0.5050 - acc: 0.8430 - val_loss: 0.6458 - val_acc: 0.8090\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.4534 - acc: 0.8600 - val_loss: 0.6306 - val_acc: 0.8140\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 158us/step - loss: 0.4293 - acc: 0.8700 - val_loss: 0.6019 - val_acc: 0.8230\n",
      "Test loss: 0.6018857460021972\n",
      "Test accuracy: 0.823\n",
      "value:  0.6018857460021972\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  143\n",
      "X:  [array([ 5.73086016,  3.79702278, -4.48870168, -7.32107286,  1.11879749,\n",
      "        8.29178158, -3.73394453,  1.12760093])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 1.7161 - acc: 0.5380 - val_loss: 1.3363 - val_acc: 0.6540\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 1.0653 - acc: 0.7270 - val_loss: 0.9902 - val_acc: 0.7270\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.7989 - acc: 0.7850 - val_loss: 0.8013 - val_acc: 0.7650\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.6597 - acc: 0.8145 - val_loss: 0.6978 - val_acc: 0.7870\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.5517 - acc: 0.8445 - val_loss: 0.6479 - val_acc: 0.8010\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.4954 - acc: 0.8510 - val_loss: 0.5807 - val_acc: 0.8230\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 0.4415 - acc: 0.8710 - val_loss: 0.5564 - val_acc: 0.8200\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.3936 - acc: 0.8845 - val_loss: 0.5492 - val_acc: 0.8290\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 0.3673 - acc: 0.8930 - val_loss: 0.5247 - val_acc: 0.8350\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 0.3525 - acc: 0.8935 - val_loss: 0.5271 - val_acc: 0.8260\n",
      "Test loss: 0.5270823917388916\n",
      "Test accuracy: 0.826\n",
      "value:  0.5270823917388916\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  144\n",
      "X:  [array([ 5.18967649,  1.55998183, -3.01642902, -4.15012085,  0.42537403,\n",
      "        4.95343557, -3.18136091,  1.70416957])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 22s 11ms/step - loss: 1.7778 - acc: 0.4785 - val_loss: 1.3691 - val_acc: 0.6310\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 1.1120 - acc: 0.6950 - val_loss: 0.9872 - val_acc: 0.7360\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 0.8048 - acc: 0.7775 - val_loss: 0.8198 - val_acc: 0.7460\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 0.6663 - acc: 0.8075 - val_loss: 0.7135 - val_acc: 0.7750\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.5501 - acc: 0.8395 - val_loss: 0.6498 - val_acc: 0.8030\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.4923 - acc: 0.8525 - val_loss: 0.6008 - val_acc: 0.8120\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.4325 - acc: 0.8625 - val_loss: 0.5754 - val_acc: 0.8340\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.3976 - acc: 0.8820 - val_loss: 0.5608 - val_acc: 0.8320\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.3628 - acc: 0.8850 - val_loss: 0.5679 - val_acc: 0.8270\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.3512 - acc: 0.8865 - val_loss: 0.5555 - val_acc: 0.8260\n",
      "Test loss: 0.5554790019989013\n",
      "Test accuracy: 0.826\n",
      "value:  0.5554790019989013\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  145\n",
      "X:  [array([ 6.62870683,  0.29870014, -9.21257241, -1.3835345 ,  3.26922478,\n",
      "        2.89225244, -3.42935849,  4.74609547])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 19s 10ms/step - loss: 1.8068 - acc: 0.4940 - val_loss: 1.4420 - val_acc: 0.6660\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 1.1388 - acc: 0.7135 - val_loss: 1.0484 - val_acc: 0.7150\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.8345 - acc: 0.7835 - val_loss: 0.8428 - val_acc: 0.7670\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.6704 - acc: 0.8095 - val_loss: 0.7383 - val_acc: 0.7940\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.5765 - acc: 0.8395 - val_loss: 0.6853 - val_acc: 0.8030\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.4997 - acc: 0.8570 - val_loss: 0.6401 - val_acc: 0.8130\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.4433 - acc: 0.8710 - val_loss: 0.6082 - val_acc: 0.8150\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.4164 - acc: 0.8780 - val_loss: 0.5714 - val_acc: 0.8330\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 0.3838 - acc: 0.8920 - val_loss: 0.5711 - val_acc: 0.8320\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.3497 - acc: 0.8990 - val_loss: 0.5632 - val_acc: 0.8290\n",
      "Test loss: 0.5632277038097382\n",
      "Test accuracy: 0.829\n",
      "value:  0.5632277038097382\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  146\n",
      "X:  [array([ 5.74614222,  2.64585603, -4.57585062, -0.53006166,  2.04446522,\n",
      "        4.96811782, -3.07927307,  1.41275043])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 1.7087 - acc: 0.5040 - val_loss: 1.3460 - val_acc: 0.6480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 1.0512 - acc: 0.7330 - val_loss: 0.9754 - val_acc: 0.7240\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.7851 - acc: 0.7865 - val_loss: 0.8140 - val_acc: 0.7470\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.6447 - acc: 0.8225 - val_loss: 0.7111 - val_acc: 0.7940\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.5526 - acc: 0.8400 - val_loss: 0.6606 - val_acc: 0.8040\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 0.4993 - acc: 0.8485 - val_loss: 0.6214 - val_acc: 0.8100\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.4453 - acc: 0.8640 - val_loss: 0.5845 - val_acc: 0.8290\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.4110 - acc: 0.8745 - val_loss: 0.5436 - val_acc: 0.8430\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.3606 - acc: 0.8910 - val_loss: 0.5616 - val_acc: 0.8260\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.3547 - acc: 0.8910 - val_loss: 0.5422 - val_acc: 0.8390\n",
      "Test loss: 0.5421569862365723\n",
      "Test accuracy: 0.839\n",
      "value:  0.5421569862365723\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  147\n",
      "X:  [array([ 5.37252226,  0.78978882, -4.80779051, -3.48060474,  0.9847261 ,\n",
      "        4.90611182, -2.39626965,  1.378743  ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 17s 8ms/step - loss: 1.7830 - acc: 0.4660 - val_loss: 1.4245 - val_acc: 0.5680\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 1.1413 - acc: 0.6770 - val_loss: 1.0650 - val_acc: 0.6800\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.8432 - acc: 0.7535 - val_loss: 0.8681 - val_acc: 0.7490\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.6753 - acc: 0.8040 - val_loss: 0.7275 - val_acc: 0.7910\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.5726 - acc: 0.8340 - val_loss: 0.6776 - val_acc: 0.7990\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.5093 - acc: 0.8465 - val_loss: 0.6185 - val_acc: 0.8070\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.4441 - acc: 0.8690 - val_loss: 0.5800 - val_acc: 0.8290\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.4258 - acc: 0.8775 - val_loss: 0.5750 - val_acc: 0.8240\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.3804 - acc: 0.8900 - val_loss: 0.5468 - val_acc: 0.8350\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.3550 - acc: 0.8990 - val_loss: 0.5317 - val_acc: 0.8400\n",
      "Test loss: 0.5316625089645386\n",
      "Test accuracy: 0.84\n",
      "value:  0.5316625089645386\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  148\n",
      "X:  [array([ 7.81127435,  1.27460621, -4.82118939, -6.16611849,  2.68118551,\n",
      "        8.11030816, -4.40315552,  1.35629759])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 1.8045 - acc: 0.4800 - val_loss: 1.4800 - val_acc: 0.6040\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 1.1395 - acc: 0.7145 - val_loss: 1.0672 - val_acc: 0.7140\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 0.8158 - acc: 0.7910 - val_loss: 0.8626 - val_acc: 0.7530\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 0.6242 - acc: 0.8355 - val_loss: 0.7340 - val_acc: 0.7830\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 166us/step - loss: 0.5395 - acc: 0.8500 - val_loss: 0.6770 - val_acc: 0.7890\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 161us/step - loss: 0.4888 - acc: 0.8500 - val_loss: 0.6357 - val_acc: 0.7980\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 172us/step - loss: 0.4379 - acc: 0.8755 - val_loss: 0.5892 - val_acc: 0.8060\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 174us/step - loss: 0.3894 - acc: 0.8730 - val_loss: 0.5638 - val_acc: 0.8200\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 158us/step - loss: 0.3634 - acc: 0.8890 - val_loss: 0.5589 - val_acc: 0.8220\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 167us/step - loss: 0.3412 - acc: 0.9015 - val_loss: 0.5582 - val_acc: 0.8250\n",
      "Test loss: 0.558225801229477\n",
      "Test accuracy: 0.825\n",
      "value:  0.558225801229477\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  149\n",
      "X:  [array([ 6.91493555,  1.24487783, -5.02706103, -3.72540513,  2.6389685 ,\n",
      "        7.28995241, -3.63593525,  2.83251778])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 20s 10ms/step - loss: 1.7655 - acc: 0.4740 - val_loss: 1.4018 - val_acc: 0.5710\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 168us/step - loss: 1.1132 - acc: 0.6825 - val_loss: 1.0257 - val_acc: 0.7040\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.8220 - acc: 0.7715 - val_loss: 0.8336 - val_acc: 0.7370\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.6641 - acc: 0.8190 - val_loss: 0.7148 - val_acc: 0.7950\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.5624 - acc: 0.8350 - val_loss: 0.6515 - val_acc: 0.8060\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.4817 - acc: 0.8615 - val_loss: 0.5894 - val_acc: 0.8320\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.4444 - acc: 0.8690 - val_loss: 0.5533 - val_acc: 0.8390\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.4101 - acc: 0.8755 - val_loss: 0.5228 - val_acc: 0.8460\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.3666 - acc: 0.8955 - val_loss: 0.5178 - val_acc: 0.8480\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.3398 - acc: 0.8960 - val_loss: 0.4986 - val_acc: 0.8420\n",
      "Test loss: 0.4985659942626953\n",
      "Test accuracy: 0.842\n",
      "value:  0.4985659942626953\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  150\n",
      "X:  [array([ 4.90238657,  0.83136702, -5.91783243, -4.30131529,  3.80297633,\n",
      "        6.11843473, -4.25851925,  4.02214568])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.7975 - acc: 0.4805 - val_loss: 1.4596 - val_acc: 0.6470\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 1.1311 - acc: 0.7125 - val_loss: 1.0303 - val_acc: 0.7280\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.8055 - acc: 0.7805 - val_loss: 0.8026 - val_acc: 0.7970\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.6311 - acc: 0.8215 - val_loss: 0.7140 - val_acc: 0.7990\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.5297 - acc: 0.8465 - val_loss: 0.6196 - val_acc: 0.8130\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.4552 - acc: 0.8665 - val_loss: 0.5861 - val_acc: 0.8210\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.4091 - acc: 0.8810 - val_loss: 0.5561 - val_acc: 0.8260\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.3776 - acc: 0.8960 - val_loss: 0.5359 - val_acc: 0.8430\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.3474 - acc: 0.8965 - val_loss: 0.5318 - val_acc: 0.8440\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.3271 - acc: 0.9075 - val_loss: 0.5242 - val_acc: 0.8300\n",
      "Test loss: 0.5241763343811036\n",
      "Test accuracy: 0.83\n",
      "value:  0.5241763343811036\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  151\n",
      "X:  [array([ 4.80817908,  1.53647665, -3.11540273, -2.12989785,  3.34978889,\n",
      "        6.51624366, -1.30625797,  2.66754671])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 19s 10ms/step - loss: 1.9158 - acc: 0.4390 - val_loss: 1.5565 - val_acc: 0.6120\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 1.2183 - acc: 0.7045 - val_loss: 1.0838 - val_acc: 0.7140\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 163us/step - loss: 0.8673 - acc: 0.7785 - val_loss: 0.8490 - val_acc: 0.7650\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 188us/step - loss: 0.6807 - acc: 0.8160 - val_loss: 0.7242 - val_acc: 0.8010\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 158us/step - loss: 0.5661 - acc: 0.8400 - val_loss: 0.6381 - val_acc: 0.8140\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.4905 - acc: 0.8500 - val_loss: 0.5879 - val_acc: 0.8390\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 163us/step - loss: 0.4559 - acc: 0.8680 - val_loss: 0.5660 - val_acc: 0.8480\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 157us/step - loss: 0.3991 - acc: 0.8900 - val_loss: 0.5538 - val_acc: 0.8410\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 157us/step - loss: 0.3858 - acc: 0.8830 - val_loss: 0.5285 - val_acc: 0.8360\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 161us/step - loss: 0.3506 - acc: 0.8950 - val_loss: 0.5128 - val_acc: 0.8470\n",
      "Test loss: 0.5127893614768982\n",
      "Test accuracy: 0.847\n",
      "value:  0.5127893614768982\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  152\n",
      "X:  [array([ 7.74243743,  1.30050886, -7.11214922, -1.86109903,  1.4187137 ,\n",
      "        8.38519629, -3.71653043,  2.6523582 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.7666 - acc: 0.4600 - val_loss: 1.3997 - val_acc: 0.6370\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 1.1465 - acc: 0.7015 - val_loss: 1.0476 - val_acc: 0.7040\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.8627 - acc: 0.7670 - val_loss: 0.8737 - val_acc: 0.7580\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.6848 - acc: 0.8140 - val_loss: 0.7410 - val_acc: 0.7830\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.5777 - acc: 0.8435 - val_loss: 0.6600 - val_acc: 0.8160\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 178us/step - loss: 0.5120 - acc: 0.8530 - val_loss: 0.6307 - val_acc: 0.8210\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 152us/step - loss: 0.4459 - acc: 0.8690 - val_loss: 0.5911 - val_acc: 0.8270\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.4148 - acc: 0.8845 - val_loss: 0.5811 - val_acc: 0.8240\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.3861 - acc: 0.8865 - val_loss: 0.5388 - val_acc: 0.8400\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 157us/step - loss: 0.3509 - acc: 0.9020 - val_loss: 0.5356 - val_acc: 0.8470\n",
      "Test loss: 0.5356270191669464\n",
      "Test accuracy: 0.847\n",
      "value:  0.5356270191669464\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  153\n",
      "X:  [array([ 5.2976064 ,  1.04319769, -4.12518829, -4.36608101,  5.33592397,\n",
      "        3.57824874, -3.43089194,  3.29691678])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.9300 - acc: 0.3855 - val_loss: 1.5533 - val_acc: 0.6060\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 1.2396 - acc: 0.6810 - val_loss: 1.1170 - val_acc: 0.7120\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.8847 - acc: 0.7715 - val_loss: 0.8649 - val_acc: 0.7510\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.6660 - acc: 0.8265 - val_loss: 0.7330 - val_acc: 0.7880\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.5761 - acc: 0.8445 - val_loss: 0.6470 - val_acc: 0.7950\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 149us/step - loss: 0.4957 - acc: 0.8635 - val_loss: 0.6001 - val_acc: 0.8140\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.4536 - acc: 0.8655 - val_loss: 0.5671 - val_acc: 0.8190\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.4006 - acc: 0.8855 - val_loss: 0.5553 - val_acc: 0.8230\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.3816 - acc: 0.8825 - val_loss: 0.5164 - val_acc: 0.8400\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.3576 - acc: 0.8945 - val_loss: 0.5079 - val_acc: 0.8350\n",
      "Test loss: 0.5079165315628051\n",
      "Test accuracy: 0.835\n",
      "value:  0.5079165315628051\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  154\n",
      "X:  [array([ 9.79181165,  1.35678898, -4.73091212, -5.38426411,  4.7973178 ,\n",
      "        7.63861437, -4.44745966,  2.87892601])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.7563 - acc: 0.4790 - val_loss: 1.3709 - val_acc: 0.6150\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 1.1103 - acc: 0.6770 - val_loss: 1.0040 - val_acc: 0.7220\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.8427 - acc: 0.7555 - val_loss: 0.8310 - val_acc: 0.7480\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.6808 - acc: 0.8065 - val_loss: 0.7313 - val_acc: 0.7740\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 0.5809 - acc: 0.8320 - val_loss: 0.6628 - val_acc: 0.7990\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.5079 - acc: 0.8560 - val_loss: 0.6221 - val_acc: 0.8080\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 156us/step - loss: 0.4444 - acc: 0.8780 - val_loss: 0.5890 - val_acc: 0.8220\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.4099 - acc: 0.8815 - val_loss: 0.5740 - val_acc: 0.8280\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.3934 - acc: 0.8830 - val_loss: 0.5338 - val_acc: 0.8330\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 152us/step - loss: 0.3524 - acc: 0.8935 - val_loss: 0.5503 - val_acc: 0.8250\n",
      "Test loss: 0.5503059740066528\n",
      "Test accuracy: 0.825\n",
      "value:  0.5503059740066528\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  155\n",
      "X:  [array([ 4.43022084,  1.86205356, -3.20106083, -3.28884831,  3.9820757 ,\n",
      "        8.45073633, -5.17889994, -0.4363466 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.7791 - acc: 0.4615 - val_loss: 1.4139 - val_acc: 0.6240\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 1.1510 - acc: 0.6895 - val_loss: 1.0420 - val_acc: 0.7200\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.8512 - acc: 0.7685 - val_loss: 0.8600 - val_acc: 0.7550\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.6891 - acc: 0.7995 - val_loss: 0.7358 - val_acc: 0.7830\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.5829 - acc: 0.8340 - val_loss: 0.6675 - val_acc: 0.7950\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 0.5126 - acc: 0.8470 - val_loss: 0.6397 - val_acc: 0.8030\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.4594 - acc: 0.8675 - val_loss: 0.6038 - val_acc: 0.8120\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.4253 - acc: 0.8710 - val_loss: 0.6065 - val_acc: 0.8130\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.3815 - acc: 0.8855 - val_loss: 0.5442 - val_acc: 0.8360\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.3658 - acc: 0.8950 - val_loss: 0.5307 - val_acc: 0.8410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5307364416122436\n",
      "Test accuracy: 0.841\n",
      "value:  0.5307364416122436\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  156\n",
      "X:  [array([ 2.87329354,  2.64444414, -3.44977414, -2.87218803,  2.39001569,\n",
      "        3.69531353, -0.85626327,  2.57116658])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.8695 - acc: 0.4365 - val_loss: 1.4690 - val_acc: 0.6200\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 1.1696 - acc: 0.7030 - val_loss: 1.0334 - val_acc: 0.7130\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.8240 - acc: 0.7785 - val_loss: 0.8512 - val_acc: 0.7400\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.6591 - acc: 0.8165 - val_loss: 0.7091 - val_acc: 0.7820\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 0.5539 - acc: 0.8440 - val_loss: 0.6570 - val_acc: 0.7950\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 0.4890 - acc: 0.8510 - val_loss: 0.6050 - val_acc: 0.8150\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 153us/step - loss: 0.4354 - acc: 0.8770 - val_loss: 0.5807 - val_acc: 0.8290\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.4137 - acc: 0.8755 - val_loss: 0.5774 - val_acc: 0.8240\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 0.3805 - acc: 0.8840 - val_loss: 0.5758 - val_acc: 0.8100\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.3613 - acc: 0.8890 - val_loss: 0.5604 - val_acc: 0.8200\n",
      "Test loss: 0.5604185409545899\n",
      "Test accuracy: 0.82\n",
      "value:  0.5604185409545899\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  157\n",
      "X:  [array([ 7.30200694,  3.90689287, -2.73912666, -2.90524282,  2.09711482,\n",
      "        5.17107613, -3.52187465,  3.64218202])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 1.7892 - acc: 0.4490 - val_loss: 1.4259 - val_acc: 0.5680\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 1.1337 - acc: 0.6810 - val_loss: 1.0446 - val_acc: 0.6850\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.8313 - acc: 0.7840 - val_loss: 0.8340 - val_acc: 0.7680\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.6641 - acc: 0.8175 - val_loss: 0.7502 - val_acc: 0.7810\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 0.5596 - acc: 0.8460 - val_loss: 0.6515 - val_acc: 0.8000\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 0.4861 - acc: 0.8645 - val_loss: 0.6100 - val_acc: 0.8220\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 0.4431 - acc: 0.8730 - val_loss: 0.5791 - val_acc: 0.8260\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.4019 - acc: 0.8820 - val_loss: 0.5864 - val_acc: 0.8250\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 0.3727 - acc: 0.8980 - val_loss: 0.5541 - val_acc: 0.8260\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 0.3555 - acc: 0.8925 - val_loss: 0.5332 - val_acc: 0.8360\n",
      "Test loss: 0.5332356929779053\n",
      "Test accuracy: 0.836\n",
      "value:  0.5332356929779053\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  158\n",
      "X:  [array([ 7.63531957,  3.06378625, -3.32130193, -5.30144064,  2.18858604,\n",
      "        7.55546672, -4.83203959,  1.13128425])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 21s 10ms/step - loss: 1.8148 - acc: 0.4660 - val_loss: 1.4255 - val_acc: 0.6100\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 1.1277 - acc: 0.7145 - val_loss: 1.0414 - val_acc: 0.6960\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.8157 - acc: 0.7895 - val_loss: 0.8034 - val_acc: 0.7740\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.6412 - acc: 0.8350 - val_loss: 0.6798 - val_acc: 0.7970\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.5362 - acc: 0.8525 - val_loss: 0.6194 - val_acc: 0.8140\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.4845 - acc: 0.8550 - val_loss: 0.5657 - val_acc: 0.8260\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.4305 - acc: 0.8705 - val_loss: 0.5349 - val_acc: 0.8400\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.3958 - acc: 0.8865 - val_loss: 0.5228 - val_acc: 0.8450\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 0.3669 - acc: 0.8930 - val_loss: 0.5230 - val_acc: 0.8390\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 0.3477 - acc: 0.8970 - val_loss: 0.5020 - val_acc: 0.8410\n",
      "Test loss: 0.5019898436069489\n",
      "Test accuracy: 0.841\n",
      "value:  0.5019898436069489\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  159\n",
      "X:  [array([ 6.5401776 ,  0.81038815, -6.00067964, -3.27359745,  3.13631484,\n",
      "        5.04452022, -2.37741918,  0.91331757])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 1.7658 - acc: 0.5060 - val_loss: 1.4313 - val_acc: 0.6320\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 158us/step - loss: 1.1233 - acc: 0.7065 - val_loss: 1.0471 - val_acc: 0.6860\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.8096 - acc: 0.7705 - val_loss: 0.8635 - val_acc: 0.7580\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.6306 - acc: 0.8300 - val_loss: 0.7540 - val_acc: 0.7730\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.5369 - acc: 0.8485 - val_loss: 0.6870 - val_acc: 0.7930\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 0.4805 - acc: 0.8600 - val_loss: 0.6247 - val_acc: 0.8050\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.4232 - acc: 0.8860 - val_loss: 0.5936 - val_acc: 0.8180\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.3794 - acc: 0.8925 - val_loss: 0.5675 - val_acc: 0.8270\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.3708 - acc: 0.8865 - val_loss: 0.5581 - val_acc: 0.8390\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.3404 - acc: 0.9000 - val_loss: 0.5543 - val_acc: 0.8300\n",
      "Test loss: 0.5543006887435913\n",
      "Test accuracy: 0.83\n",
      "value:  0.5543006887435913\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  160\n",
      "X:  [array([ 6.45461428,  2.64530163, -1.39654664, -5.07142905,  7.19119428,\n",
      "        5.6986273 , -5.04365443,  3.60086775])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 1.8117 - acc: 0.4505 - val_loss: 1.4701 - val_acc: 0.6150\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 1.1792 - acc: 0.6975 - val_loss: 1.0876 - val_acc: 0.7060\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.8638 - acc: 0.7665 - val_loss: 0.8583 - val_acc: 0.7490\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.6928 - acc: 0.8015 - val_loss: 0.7524 - val_acc: 0.7690\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 0.5744 - acc: 0.8300 - val_loss: 0.6831 - val_acc: 0.7960\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 0.5041 - acc: 0.8510 - val_loss: 0.6258 - val_acc: 0.8120\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.4581 - acc: 0.8540 - val_loss: 0.6218 - val_acc: 0.8040\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 0.4154 - acc: 0.8775 - val_loss: 0.5618 - val_acc: 0.8300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.3900 - acc: 0.8830 - val_loss: 0.5375 - val_acc: 0.8350\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.3677 - acc: 0.8955 - val_loss: 0.5282 - val_acc: 0.8410\n",
      "Test loss: 0.5281991305351257\n",
      "Test accuracy: 0.841\n",
      "value:  0.5281991305351257\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  161\n",
      "X:  [array([ 7.67529805,  1.16924067, -3.63040356, -5.09536287,  4.53254312,\n",
      "        6.87307463, -4.09089761,  1.47745163])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 1.6773 - acc: 0.5215 - val_loss: 1.3349 - val_acc: 0.6180\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 1.0479 - acc: 0.7415 - val_loss: 1.0218 - val_acc: 0.6930\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 0.7832 - acc: 0.7880 - val_loss: 0.8252 - val_acc: 0.7350\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.6327 - acc: 0.8200 - val_loss: 0.7475 - val_acc: 0.7730\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 0.5595 - acc: 0.8350 - val_loss: 0.6684 - val_acc: 0.8060\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 0.4784 - acc: 0.8620 - val_loss: 0.6377 - val_acc: 0.8170\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.4426 - acc: 0.8735 - val_loss: 0.5978 - val_acc: 0.8130\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.4187 - acc: 0.8775 - val_loss: 0.5767 - val_acc: 0.8290\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.3934 - acc: 0.8885 - val_loss: 0.5964 - val_acc: 0.8130\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.3433 - acc: 0.9055 - val_loss: 0.5673 - val_acc: 0.8360\n",
      "Test loss: 0.567327006816864\n",
      "Test accuracy: 0.836\n",
      "value:  0.567327006816864\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  162\n",
      "X:  [array([ 4.85298258,  3.95362703, -0.62582727, -4.01055894,  1.80309999,\n",
      "        6.18550965, -4.39793854,  1.12524615])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.8800 - acc: 0.4020 - val_loss: 1.5346 - val_acc: 0.5990\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 1.2473 - acc: 0.6680 - val_loss: 1.1320 - val_acc: 0.6930\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.9165 - acc: 0.7555 - val_loss: 0.9144 - val_acc: 0.7390\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 0.7232 - acc: 0.7990 - val_loss: 0.7612 - val_acc: 0.7800\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.6085 - acc: 0.8285 - val_loss: 0.6817 - val_acc: 0.8000\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 0.5365 - acc: 0.8370 - val_loss: 0.6347 - val_acc: 0.8110\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 0.4821 - acc: 0.8590 - val_loss: 0.6063 - val_acc: 0.8050\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.4329 - acc: 0.8760 - val_loss: 0.5617 - val_acc: 0.8260\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 0.4074 - acc: 0.8830 - val_loss: 0.5646 - val_acc: 0.8270\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.3783 - acc: 0.8875 - val_loss: 0.5278 - val_acc: 0.8350\n",
      "Test loss: 0.5278060567378998\n",
      "Test accuracy: 0.835\n",
      "value:  0.5278060567378998\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  163\n",
      "X:  [array([ 5.45957549,  0.27378493, -4.01512635, -5.12409822,  4.40574862,\n",
      "        2.07308826, -1.01770226,  4.37713008])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 17s 8ms/step - loss: 1.8126 - acc: 0.4650 - val_loss: 1.4321 - val_acc: 0.6190\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 1.1216 - acc: 0.7065 - val_loss: 1.0136 - val_acc: 0.7370\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 0.8080 - acc: 0.7815 - val_loss: 0.7981 - val_acc: 0.7740\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.6281 - acc: 0.8190 - val_loss: 0.6818 - val_acc: 0.8080\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.5089 - acc: 0.8505 - val_loss: 0.6205 - val_acc: 0.8210\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.4612 - acc: 0.8655 - val_loss: 0.6095 - val_acc: 0.8180\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.4058 - acc: 0.8855 - val_loss: 0.5512 - val_acc: 0.8360\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.3917 - acc: 0.8805 - val_loss: 0.5531 - val_acc: 0.8250\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.3467 - acc: 0.9015 - val_loss: 0.5224 - val_acc: 0.8440\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.3414 - acc: 0.9055 - val_loss: 0.5199 - val_acc: 0.8510\n",
      "Test loss: 0.5198831198215484\n",
      "Test accuracy: 0.851\n",
      "value:  0.5198831198215484\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  164\n",
      "X:  [array([ 7.69795358,  4.40793199,  1.017756  , -3.71198663,  5.42015402,\n",
      "        8.03197099, -3.50610676, -0.14985255])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 17s 8ms/step - loss: 1.9605 - acc: 0.3325 - val_loss: 1.6074 - val_acc: 0.5720\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 1.4729 - acc: 0.5235 - val_loss: 1.2354 - val_acc: 0.6570\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 1.2016 - acc: 0.6220 - val_loss: 1.0131 - val_acc: 0.7040\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 1.0551 - acc: 0.6645 - val_loss: 0.8910 - val_acc: 0.7470\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.9581 - acc: 0.6825 - val_loss: 0.8041 - val_acc: 0.7550\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.9054 - acc: 0.7045 - val_loss: 0.7451 - val_acc: 0.7840\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.8271 - acc: 0.7325 - val_loss: 0.7012 - val_acc: 0.7930\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.8025 - acc: 0.7265 - val_loss: 0.6715 - val_acc: 0.7980\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.7426 - acc: 0.7550 - val_loss: 0.6398 - val_acc: 0.8010\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.7264 - acc: 0.7635 - val_loss: 0.6296 - val_acc: 0.8140\n",
      "Test loss: 0.6296106278896332\n",
      "Test accuracy: 0.814\n",
      "value:  0.6296106278896332\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  165\n",
      "X:  [array([ 7.43772005,  2.91414885, -4.81098615, -3.95942302,  6.23143508,\n",
      "        4.65409898, -3.39479922,  3.5834368 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 1.7860 - acc: 0.4535 - val_loss: 1.4457 - val_acc: 0.6130\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 1.1260 - acc: 0.6960 - val_loss: 1.0478 - val_acc: 0.7170\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.8169 - acc: 0.7775 - val_loss: 0.8311 - val_acc: 0.7730\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 159us/step - loss: 0.6480 - acc: 0.8170 - val_loss: 0.7167 - val_acc: 0.7960\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 0.5597 - acc: 0.8330 - val_loss: 0.6602 - val_acc: 0.7990\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 132us/step - loss: 0.4977 - acc: 0.8585 - val_loss: 0.5993 - val_acc: 0.8260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.4393 - acc: 0.8665 - val_loss: 0.5527 - val_acc: 0.8430\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.4058 - acc: 0.8770 - val_loss: 0.5539 - val_acc: 0.8340\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 0.3681 - acc: 0.8910 - val_loss: 0.5505 - val_acc: 0.8340\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.3496 - acc: 0.9060 - val_loss: 0.5288 - val_acc: 0.8520\n",
      "Test loss: 0.5287870609760285\n",
      "Test accuracy: 0.852\n",
      "value:  0.5287870609760285\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  166\n",
      "X:  [array([ 5.70304827,  0.90250407, -6.33163785, -4.47332536,  4.06477968,\n",
      "        7.02878138, -3.28518574,  3.28160134])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 1.7728 - acc: 0.4910 - val_loss: 1.4326 - val_acc: 0.6250\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 1.1164 - acc: 0.7230 - val_loss: 1.0424 - val_acc: 0.7190\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.8032 - acc: 0.7830 - val_loss: 0.8401 - val_acc: 0.7680\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 0.6283 - acc: 0.8275 - val_loss: 0.7258 - val_acc: 0.7870\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.5367 - acc: 0.8535 - val_loss: 0.6567 - val_acc: 0.7920\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.4579 - acc: 0.8670 - val_loss: 0.6123 - val_acc: 0.8190\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 0.4115 - acc: 0.8780 - val_loss: 0.6098 - val_acc: 0.8120\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 0.3819 - acc: 0.8915 - val_loss: 0.5735 - val_acc: 0.8370\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.3527 - acc: 0.8975 - val_loss: 0.5547 - val_acc: 0.8400\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.3284 - acc: 0.9060 - val_loss: 0.5284 - val_acc: 0.8460\n",
      "Test loss: 0.5284457480907441\n",
      "Test accuracy: 0.846\n",
      "value:  0.5284457480907441\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  167\n",
      "X:  [array([ 6.48952042,  2.76506989, -5.60806277, -5.11120112,  3.43547454,\n",
      "        7.25312271, -3.13632575,  2.29581188])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 1.8006 - acc: 0.5065 - val_loss: 1.4100 - val_acc: 0.6540\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 1.1260 - acc: 0.7330 - val_loss: 1.0112 - val_acc: 0.7320\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.8193 - acc: 0.7850 - val_loss: 0.8075 - val_acc: 0.7710\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.6601 - acc: 0.8235 - val_loss: 0.7078 - val_acc: 0.7950\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 155us/step - loss: 0.5728 - acc: 0.8375 - val_loss: 0.6522 - val_acc: 0.8110\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.4918 - acc: 0.8540 - val_loss: 0.6214 - val_acc: 0.8120\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 0.4619 - acc: 0.8600 - val_loss: 0.5986 - val_acc: 0.8120\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 134us/step - loss: 0.4184 - acc: 0.8815 - val_loss: 0.5677 - val_acc: 0.8290\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.3796 - acc: 0.8910 - val_loss: 0.5621 - val_acc: 0.8360\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.3784 - acc: 0.8925 - val_loss: 0.5396 - val_acc: 0.8370\n",
      "Test loss: 0.539597464799881\n",
      "Test accuracy: 0.837\n",
      "value:  0.539597464799881\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  168\n",
      "X:  [array([ 4.32272796,  1.20985683, -4.29957281, -3.79019819,  6.5631043 ,\n",
      "        5.67540427, -1.3348874 ,  1.38430546])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.8111 - acc: 0.4735 - val_loss: 1.4212 - val_acc: 0.6530\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 1.0953 - acc: 0.7340 - val_loss: 1.0066 - val_acc: 0.7170\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 0.7907 - acc: 0.7980 - val_loss: 0.7942 - val_acc: 0.7740\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.6232 - acc: 0.8180 - val_loss: 0.7147 - val_acc: 0.7810\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.5442 - acc: 0.8365 - val_loss: 0.6622 - val_acc: 0.8010\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 0.4763 - acc: 0.8580 - val_loss: 0.6183 - val_acc: 0.8110\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 137us/step - loss: 0.4299 - acc: 0.8815 - val_loss: 0.5953 - val_acc: 0.8230\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.4059 - acc: 0.8810 - val_loss: 0.5828 - val_acc: 0.8170\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.3784 - acc: 0.8880 - val_loss: 0.5640 - val_acc: 0.8300\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.3524 - acc: 0.8990 - val_loss: 0.5522 - val_acc: 0.8280\n",
      "Test loss: 0.5522155499458313\n",
      "Test accuracy: 0.828\n",
      "value:  0.5522155499458313\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  169\n",
      "X:  [array([ 8.17666578,  1.61498417, -3.39652913, -3.56894649,  4.06474953,\n",
      "        5.49219068, -2.72870468,  3.69407624])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 1.8147 - acc: 0.4785 - val_loss: 1.4101 - val_acc: 0.6610\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 1.1220 - acc: 0.7225 - val_loss: 1.0369 - val_acc: 0.7340\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.8255 - acc: 0.7750 - val_loss: 0.8303 - val_acc: 0.7660\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.6532 - acc: 0.8210 - val_loss: 0.7302 - val_acc: 0.7950\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.5646 - acc: 0.8395 - val_loss: 0.6534 - val_acc: 0.8120\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.4926 - acc: 0.8585 - val_loss: 0.6389 - val_acc: 0.7980\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.4509 - acc: 0.8715 - val_loss: 0.5999 - val_acc: 0.8250\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 152us/step - loss: 0.4071 - acc: 0.8760 - val_loss: 0.5653 - val_acc: 0.8330\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.3830 - acc: 0.8890 - val_loss: 0.5440 - val_acc: 0.8370\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.3558 - acc: 0.8955 - val_loss: 0.5377 - val_acc: 0.8350\n",
      "Test loss: 0.5377210083007813\n",
      "Test accuracy: 0.835\n",
      "value:  0.5377210083007813\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  170\n",
      "X:  [array([ 2.49317551,  1.44325126, -2.38437855, -3.0678647 ,  4.69413879,\n",
      "        1.8410261 , -1.90746   ,  3.3600687 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 1.7992 - acc: 0.4405 - val_loss: 1.4436 - val_acc: 0.6050\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 1.1575 - acc: 0.6650 - val_loss: 1.0671 - val_acc: 0.6860\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.8541 - acc: 0.7645 - val_loss: 0.8504 - val_acc: 0.7720\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 243us/step - loss: 0.6973 - acc: 0.8025 - val_loss: 0.7532 - val_acc: 0.7810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 242us/step - loss: 0.5800 - acc: 0.8350 - val_loss: 0.6684 - val_acc: 0.8100\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 1s 250us/step - loss: 0.5137 - acc: 0.8545 - val_loss: 0.6242 - val_acc: 0.8060\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 234us/step - loss: 0.4549 - acc: 0.8650 - val_loss: 0.5753 - val_acc: 0.8320\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.4203 - acc: 0.8740 - val_loss: 0.5648 - val_acc: 0.8370\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 1s 269us/step - loss: 0.3922 - acc: 0.8845 - val_loss: 0.5467 - val_acc: 0.8360\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 226us/step - loss: 0.3656 - acc: 0.8885 - val_loss: 0.5412 - val_acc: 0.8320\n",
      "Test loss: 0.5411638760566712\n",
      "Test accuracy: 0.832\n",
      "value:  0.5411638760566712\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  171\n",
      "X:  [array([ 6.90915168,  0.14897811, -6.31831793, -5.62393067,  3.7525984 ,\n",
      "        7.21967613, -2.77759656,  6.21905756])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 24s 12ms/step - loss: 1.8118 - acc: 0.4645 - val_loss: 1.4741 - val_acc: 0.6030\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 149us/step - loss: 1.1588 - acc: 0.7045 - val_loss: 1.1039 - val_acc: 0.6890\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 0.8491 - acc: 0.7710 - val_loss: 0.8693 - val_acc: 0.7490\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.6678 - acc: 0.8050 - val_loss: 0.7469 - val_acc: 0.7770\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.5615 - acc: 0.8365 - val_loss: 0.6589 - val_acc: 0.8070\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.4830 - acc: 0.8525 - val_loss: 0.6196 - val_acc: 0.8120\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.4465 - acc: 0.8695 - val_loss: 0.5981 - val_acc: 0.8180\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 157us/step - loss: 0.4054 - acc: 0.8780 - val_loss: 0.5954 - val_acc: 0.8150\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 149us/step - loss: 0.3825 - acc: 0.8885 - val_loss: 0.5333 - val_acc: 0.8410\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 214us/step - loss: 0.3661 - acc: 0.8940 - val_loss: 0.5247 - val_acc: 0.8530\n",
      "Test loss: 0.5246601691246032\n",
      "Test accuracy: 0.853\n",
      "value:  0.5246601691246032\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  172\n",
      "X:  [array([ 3.56234719,  0.39931298, -3.43506867, -5.44808901,  2.03949342,\n",
      "        4.22197791, -1.29592159,  2.56684111])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 21s 10ms/step - loss: 1.7365 - acc: 0.4985 - val_loss: 1.3808 - val_acc: 0.6720\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 1.1136 - acc: 0.7105 - val_loss: 1.0156 - val_acc: 0.7420\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.8114 - acc: 0.7830 - val_loss: 0.8427 - val_acc: 0.7660\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.6609 - acc: 0.8190 - val_loss: 0.7231 - val_acc: 0.7920\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 160us/step - loss: 0.5656 - acc: 0.8360 - val_loss: 0.6548 - val_acc: 0.8130\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.4881 - acc: 0.8605 - val_loss: 0.6210 - val_acc: 0.8220\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.4436 - acc: 0.8755 - val_loss: 0.5670 - val_acc: 0.8350\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 164us/step - loss: 0.4035 - acc: 0.8855 - val_loss: 0.5569 - val_acc: 0.8520\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.3768 - acc: 0.8890 - val_loss: 0.5385 - val_acc: 0.8510\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.3573 - acc: 0.8990 - val_loss: 0.5487 - val_acc: 0.8460\n",
      "Test loss: 0.5486837885379792\n",
      "Test accuracy: 0.846\n",
      "value:  0.5486837885379792\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  173\n",
      "X:  [array([ 9.85287609,  3.17761753, -4.61012488, -9.48011468,  5.370713  ,\n",
      "        6.64945484, -3.46548206,  4.84346146])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.7876 - acc: 0.4690 - val_loss: 1.4289 - val_acc: 0.5700\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 1.1362 - acc: 0.6890 - val_loss: 1.0655 - val_acc: 0.6740\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.8514 - acc: 0.7605 - val_loss: 0.9033 - val_acc: 0.7140\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.7008 - acc: 0.7915 - val_loss: 0.7778 - val_acc: 0.7700\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.5898 - acc: 0.8185 - val_loss: 0.6968 - val_acc: 0.7900\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.5114 - acc: 0.8455 - val_loss: 0.6446 - val_acc: 0.8060\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.4574 - acc: 0.8725 - val_loss: 0.6097 - val_acc: 0.8140\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.4117 - acc: 0.8805 - val_loss: 0.5599 - val_acc: 0.8300\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.3884 - acc: 0.8855 - val_loss: 0.5666 - val_acc: 0.8380\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.3700 - acc: 0.8940 - val_loss: 0.5455 - val_acc: 0.8440\n",
      "Test loss: 0.5455070803165436\n",
      "Test accuracy: 0.844\n",
      "value:  0.5455070803165436\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  174\n",
      "X:  [array([ 5.07456036,  0.64084752, -1.91914974, -5.43384245,  1.72107825,\n",
      "        1.88366639, -0.82955412,  3.46519729])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 17s 8ms/step - loss: 1.7648 - acc: 0.4580 - val_loss: 1.3993 - val_acc: 0.6050\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 1.1077 - acc: 0.6945 - val_loss: 1.0151 - val_acc: 0.7000\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.8100 - acc: 0.7760 - val_loss: 0.8154 - val_acc: 0.7510\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.6546 - acc: 0.8085 - val_loss: 0.6903 - val_acc: 0.7930\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.5545 - acc: 0.8290 - val_loss: 0.6254 - val_acc: 0.8200\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.4803 - acc: 0.8640 - val_loss: 0.5773 - val_acc: 0.8220\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.4339 - acc: 0.8685 - val_loss: 0.5513 - val_acc: 0.8300\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.3883 - acc: 0.8840 - val_loss: 0.5200 - val_acc: 0.8410\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.3677 - acc: 0.8985 - val_loss: 0.5077 - val_acc: 0.8400\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.3353 - acc: 0.9020 - val_loss: 0.4946 - val_acc: 0.8450\n",
      "Test loss: 0.4945677134990692\n",
      "Test accuracy: 0.845\n",
      "value:  0.4945677134990692\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  175\n",
      "X:  [array([ 4.97691655,  1.53000135, -1.24577249, -3.63231047,  2.83400255,\n",
      "        3.724955  , -2.80109671,  3.94765095])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.7669 - acc: 0.4820 - val_loss: 1.4104 - val_acc: 0.6340\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 1.1211 - acc: 0.7125 - val_loss: 1.0637 - val_acc: 0.6900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.8635 - acc: 0.7585 - val_loss: 0.8824 - val_acc: 0.7370\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.6873 - acc: 0.8055 - val_loss: 0.7566 - val_acc: 0.7720\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.5881 - acc: 0.8380 - val_loss: 0.6903 - val_acc: 0.7960\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.5176 - acc: 0.8375 - val_loss: 0.6071 - val_acc: 0.8170\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.4630 - acc: 0.8635 - val_loss: 0.6144 - val_acc: 0.8080\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.4061 - acc: 0.8810 - val_loss: 0.5709 - val_acc: 0.8280\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.3936 - acc: 0.8820 - val_loss: 0.5389 - val_acc: 0.8450\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.3494 - acc: 0.8925 - val_loss: 0.5069 - val_acc: 0.8430\n",
      "Test loss: 0.5068891344070434\n",
      "Test accuracy: 0.843\n",
      "value:  0.5068891344070434\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  176\n",
      "X:  [array([ 7.7030199 ,  2.0959042 , -3.93509818, -4.22315797,  4.39004033,\n",
      "        5.9116896 , -4.93777768,  4.98069672])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.8080 - acc: 0.4455 - val_loss: 1.4252 - val_acc: 0.6170\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 1.1278 - acc: 0.6940 - val_loss: 1.0282 - val_acc: 0.7050\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.8173 - acc: 0.7725 - val_loss: 0.8125 - val_acc: 0.7670\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.6512 - acc: 0.8205 - val_loss: 0.7090 - val_acc: 0.7920\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.5555 - acc: 0.8410 - val_loss: 0.6468 - val_acc: 0.8020\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.4766 - acc: 0.8605 - val_loss: 0.6224 - val_acc: 0.8030\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.4351 - acc: 0.8715 - val_loss: 0.5937 - val_acc: 0.8190\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.3939 - acc: 0.8790 - val_loss: 0.5662 - val_acc: 0.8300\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.3626 - acc: 0.8885 - val_loss: 0.5528 - val_acc: 0.8310\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.3416 - acc: 0.9040 - val_loss: 0.5484 - val_acc: 0.8230\n",
      "Test loss: 0.5484406862258911\n",
      "Test accuracy: 0.823\n",
      "value:  0.5484406862258911\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  177\n",
      "X:  [array([ 4.60339123,  0.34196919, -3.54416171, -5.21205546,  4.86737796,\n",
      "        3.5810645 , -3.47078721,  4.18750172])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.7668 - acc: 0.4810 - val_loss: 1.4133 - val_acc: 0.6350\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 1.0968 - acc: 0.7130 - val_loss: 1.0177 - val_acc: 0.7040\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 154us/step - loss: 0.8067 - acc: 0.7765 - val_loss: 0.8256 - val_acc: 0.7460\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.6551 - acc: 0.8190 - val_loss: 0.7178 - val_acc: 0.7860\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.5441 - acc: 0.8470 - val_loss: 0.6508 - val_acc: 0.7920\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.4822 - acc: 0.8690 - val_loss: 0.6024 - val_acc: 0.8100\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.4158 - acc: 0.8740 - val_loss: 0.5773 - val_acc: 0.8170\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.3970 - acc: 0.8850 - val_loss: 0.5535 - val_acc: 0.8360\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.3530 - acc: 0.8965 - val_loss: 0.5342 - val_acc: 0.8400\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.3403 - acc: 0.8940 - val_loss: 0.5541 - val_acc: 0.8390\n",
      "Test loss: 0.5541190948486329\n",
      "Test accuracy: 0.839\n",
      "value:  0.5541190948486329\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  178\n",
      "X:  [array([ 4.91186916,  1.9440854 , -2.65116201, -5.88521326,  3.56519816,\n",
      "        2.5846813 , -3.05499773,  2.8246849 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.8261 - acc: 0.4705 - val_loss: 1.3869 - val_acc: 0.6310\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 1.0935 - acc: 0.7215 - val_loss: 0.9750 - val_acc: 0.7280\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.8060 - acc: 0.7790 - val_loss: 0.7900 - val_acc: 0.7730\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.6486 - acc: 0.8145 - val_loss: 0.6821 - val_acc: 0.8110\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.5527 - acc: 0.8440 - val_loss: 0.6125 - val_acc: 0.8180\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.5017 - acc: 0.8550 - val_loss: 0.5794 - val_acc: 0.8300\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.4450 - acc: 0.8690 - val_loss: 0.5465 - val_acc: 0.8380\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.3884 - acc: 0.8830 - val_loss: 0.5389 - val_acc: 0.8380\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.3754 - acc: 0.8850 - val_loss: 0.5141 - val_acc: 0.8530\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.3437 - acc: 0.8980 - val_loss: 0.5180 - val_acc: 0.8490\n",
      "Test loss: 0.5180218966007233\n",
      "Test accuracy: 0.849\n",
      "value:  0.5180218966007233\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  179\n",
      "X:  [array([ 5.5528071 ,  1.89631837, -2.02401336, -4.18691985,  5.69920928,\n",
      "        4.97618145, -2.4645254 ,  5.40494646])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.7842 - acc: 0.4740 - val_loss: 1.4670 - val_acc: 0.6380\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 1.1158 - acc: 0.7385 - val_loss: 1.0365 - val_acc: 0.7340\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.7964 - acc: 0.7935 - val_loss: 0.8223 - val_acc: 0.7780\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.6420 - acc: 0.8275 - val_loss: 0.7191 - val_acc: 0.7960\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.5458 - acc: 0.8450 - val_loss: 0.6583 - val_acc: 0.8140\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.4799 - acc: 0.8655 - val_loss: 0.6116 - val_acc: 0.8310\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.4385 - acc: 0.8680 - val_loss: 0.6072 - val_acc: 0.8270\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.4127 - acc: 0.8780 - val_loss: 0.5792 - val_acc: 0.8340\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 152us/step - loss: 0.3714 - acc: 0.8975 - val_loss: 0.5932 - val_acc: 0.8110\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.3644 - acc: 0.8805 - val_loss: 0.5657 - val_acc: 0.8230\n",
      "Test loss: 0.5657119402885437\n",
      "Test accuracy: 0.823\n",
      "value:  0.5657119402885437\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  180\n",
      "X:  [array([ 5.60166172,  0.08429249, -3.42933431, -7.06068004,  1.61819042,\n",
      "        1.96071149, -1.59996611,  5.85681436])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.8024 - acc: 0.4395 - val_loss: 1.4086 - val_acc: 0.6310\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 1.1468 - acc: 0.6795 - val_loss: 1.0325 - val_acc: 0.7030\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.8513 - acc: 0.7545 - val_loss: 0.8372 - val_acc: 0.7550\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.6847 - acc: 0.7955 - val_loss: 0.7527 - val_acc: 0.7720\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.5708 - acc: 0.8325 - val_loss: 0.6755 - val_acc: 0.7880\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 0.5170 - acc: 0.8495 - val_loss: 0.6250 - val_acc: 0.8110\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.4586 - acc: 0.8675 - val_loss: 0.6047 - val_acc: 0.8090\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.4162 - acc: 0.8835 - val_loss: 0.5745 - val_acc: 0.8230\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.3963 - acc: 0.8820 - val_loss: 0.5788 - val_acc: 0.8270\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 162us/step - loss: 0.3520 - acc: 0.8960 - val_loss: 0.5567 - val_acc: 0.8300\n",
      "Test loss: 0.5566981158256531\n",
      "Test accuracy: 0.83\n",
      "value:  0.5566981158256531\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  181\n",
      "X:  [array([ 5.95497022,  2.00816398, -3.06068659, -3.06075121,  2.42037431,\n",
      "        3.16910866, -2.92387292,  3.5179394 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.8848 - acc: 0.4315 - val_loss: 1.4980 - val_acc: 0.6310\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 1.2088 - acc: 0.6910 - val_loss: 1.0783 - val_acc: 0.7120\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.8768 - acc: 0.7645 - val_loss: 0.8448 - val_acc: 0.7790\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.7062 - acc: 0.7985 - val_loss: 0.7269 - val_acc: 0.8070\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 149us/step - loss: 0.5820 - acc: 0.8310 - val_loss: 0.6759 - val_acc: 0.7990\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.5191 - acc: 0.8545 - val_loss: 0.6089 - val_acc: 0.8250\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 149us/step - loss: 0.4664 - acc: 0.8695 - val_loss: 0.5739 - val_acc: 0.8300\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 161us/step - loss: 0.4169 - acc: 0.8815 - val_loss: 0.5635 - val_acc: 0.8310\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.3881 - acc: 0.8935 - val_loss: 0.5517 - val_acc: 0.8300\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.3573 - acc: 0.8995 - val_loss: 0.5175 - val_acc: 0.8530\n",
      "Test loss: 0.5174951248168945\n",
      "Test accuracy: 0.853\n",
      "value:  0.5174951248168945\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  182\n",
      "X:  [array([ 5.34139252,  0.86024142, -2.63324391, -5.2661629 ,  2.79230759,\n",
      "        2.98429693, -4.054296  ,  3.12108984])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 1.7909 - acc: 0.4770 - val_loss: 1.4497 - val_acc: 0.6090\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 1.1470 - acc: 0.6820 - val_loss: 1.0367 - val_acc: 0.7320\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.8319 - acc: 0.7830 - val_loss: 0.8157 - val_acc: 0.7690\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.6509 - acc: 0.8255 - val_loss: 0.7072 - val_acc: 0.8020\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.5507 - acc: 0.8405 - val_loss: 0.6559 - val_acc: 0.8010\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.4791 - acc: 0.8595 - val_loss: 0.5963 - val_acc: 0.8310\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.4400 - acc: 0.8750 - val_loss: 0.5695 - val_acc: 0.8300\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.3992 - acc: 0.8815 - val_loss: 0.5514 - val_acc: 0.8270\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.3559 - acc: 0.8955 - val_loss: 0.5323 - val_acc: 0.8340\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.3471 - acc: 0.9015 - val_loss: 0.5397 - val_acc: 0.8310\n",
      "Test loss: 0.5396591267585754\n",
      "Test accuracy: 0.831\n",
      "value:  0.5396591267585754\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  183\n",
      "X:  [array([ 4.92713249,  0.82752109, -2.7405451 , -5.24479749,  0.40718579,\n",
      "        2.91270027, -0.41694733,  4.11867846])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 1.7816 - acc: 0.4575 - val_loss: 1.4209 - val_acc: 0.5920\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 153us/step - loss: 1.1255 - acc: 0.7015 - val_loss: 1.0107 - val_acc: 0.7280\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.7936 - acc: 0.7895 - val_loss: 0.7948 - val_acc: 0.7720\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.6116 - acc: 0.8240 - val_loss: 0.6764 - val_acc: 0.8040\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.5235 - acc: 0.8490 - val_loss: 0.6288 - val_acc: 0.8160\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.4616 - acc: 0.8595 - val_loss: 0.6181 - val_acc: 0.8100\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.4144 - acc: 0.8805 - val_loss: 0.5725 - val_acc: 0.8270\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.3700 - acc: 0.8880 - val_loss: 0.5564 - val_acc: 0.8330\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.3667 - acc: 0.8925 - val_loss: 0.5299 - val_acc: 0.8410\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.3428 - acc: 0.8945 - val_loss: 0.5343 - val_acc: 0.8420\n",
      "Test loss: 0.5342920665740967\n",
      "Test accuracy: 0.842\n",
      "value:  0.5342920665740967\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  184\n",
      "X:  [array([ 6.39867883,  2.58125064, -1.69872641, -3.65415153,  3.69097439,\n",
      "        1.93256032, -2.98366818,  3.64913762])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.8405 - acc: 0.4620 - val_loss: 1.4929 - val_acc: 0.5930\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 1.1642 - acc: 0.7015 - val_loss: 1.0673 - val_acc: 0.6930\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.8471 - acc: 0.7565 - val_loss: 0.8821 - val_acc: 0.7390\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.6751 - acc: 0.8015 - val_loss: 0.7602 - val_acc: 0.7860\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.5742 - acc: 0.8310 - val_loss: 0.6773 - val_acc: 0.7990\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.5088 - acc: 0.8460 - val_loss: 0.6725 - val_acc: 0.8020\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.4613 - acc: 0.8660 - val_loss: 0.5959 - val_acc: 0.8280\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.4171 - acc: 0.8715 - val_loss: 0.5831 - val_acc: 0.8170\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.3892 - acc: 0.8835 - val_loss: 0.5529 - val_acc: 0.8320\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 152us/step - loss: 0.3631 - acc: 0.8935 - val_loss: 0.5418 - val_acc: 0.8440\n",
      "Test loss: 0.5417504119873047\n",
      "Test accuracy: 0.844\n",
      "value:  0.5417504119873047\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  185\n",
      "X:  [array([ 6.67462477,  2.80616601, -0.47143705, -6.48975936,  4.33843062,\n",
      "        3.49649115, -4.0434844 ,  3.48222061])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.7939 - acc: 0.4440 - val_loss: 1.4217 - val_acc: 0.6350\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 162us/step - loss: 1.1303 - acc: 0.7035 - val_loss: 1.0237 - val_acc: 0.7260\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 155us/step - loss: 0.8422 - acc: 0.7685 - val_loss: 0.8264 - val_acc: 0.7700\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.6717 - acc: 0.8155 - val_loss: 0.7247 - val_acc: 0.8000\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.5643 - acc: 0.8360 - val_loss: 0.6461 - val_acc: 0.8140\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.4990 - acc: 0.8465 - val_loss: 0.6089 - val_acc: 0.8150\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.4373 - acc: 0.8775 - val_loss: 0.5692 - val_acc: 0.8270\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.4125 - acc: 0.8760 - val_loss: 0.5554 - val_acc: 0.8270\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.3790 - acc: 0.8860 - val_loss: 0.5336 - val_acc: 0.8260\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.3507 - acc: 0.9015 - val_loss: 0.5020 - val_acc: 0.8500\n",
      "Test loss: 0.5019793183803558\n",
      "Test accuracy: 0.85\n",
      "value:  0.5019793183803558\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  186\n",
      "X:  [array([ 5.95529548,  1.02620746, -3.18647693, -2.75293442,  3.73597475,\n",
      "        3.45095617, -3.87165798,  5.60506185])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 21s 11ms/step - loss: 1.8284 - acc: 0.4545 - val_loss: 1.4812 - val_acc: 0.5790\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 1.1606 - acc: 0.6805 - val_loss: 1.0728 - val_acc: 0.7110\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.8410 - acc: 0.7650 - val_loss: 0.8668 - val_acc: 0.7590\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.6813 - acc: 0.8010 - val_loss: 0.7564 - val_acc: 0.7810\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.5817 - acc: 0.8340 - val_loss: 0.6658 - val_acc: 0.8150\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.5019 - acc: 0.8620 - val_loss: 0.6365 - val_acc: 0.8080\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.4561 - acc: 0.8565 - val_loss: 0.5776 - val_acc: 0.8230\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 149us/step - loss: 0.4272 - acc: 0.8760 - val_loss: 0.5749 - val_acc: 0.8300\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.3950 - acc: 0.8840 - val_loss: 0.5586 - val_acc: 0.8280\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 155us/step - loss: 0.3642 - acc: 0.8905 - val_loss: 0.5385 - val_acc: 0.8400\n",
      "Test loss: 0.538487253189087\n",
      "Test accuracy: 0.84\n",
      "value:  0.538487253189087\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  187\n",
      "X:  [array([ 4.4846134 ,  1.65229991,  1.18280373, -7.27461065,  1.81745058,\n",
      "        0.42377543, -1.7084701 ,  1.37390638])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 2.0133 - acc: 0.3035 - val_loss: 1.6744 - val_acc: 0.5420\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 1.5771 - acc: 0.5120 - val_loss: 1.3209 - val_acc: 0.6380\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 1.2978 - acc: 0.5885 - val_loss: 1.1002 - val_acc: 0.6830\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 1.1111 - acc: 0.6460 - val_loss: 0.9492 - val_acc: 0.7240\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.9887 - acc: 0.6755 - val_loss: 0.8353 - val_acc: 0.7690\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.9090 - acc: 0.7040 - val_loss: 0.7662 - val_acc: 0.7750\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.8558 - acc: 0.7135 - val_loss: 0.7199 - val_acc: 0.8060\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.8054 - acc: 0.7395 - val_loss: 0.6746 - val_acc: 0.8120\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.7600 - acc: 0.7610 - val_loss: 0.6379 - val_acc: 0.8110\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.7165 - acc: 0.7660 - val_loss: 0.6176 - val_acc: 0.8190\n",
      "Test loss: 0.6176333439350128\n",
      "Test accuracy: 0.819\n",
      "value:  0.6176333439350128\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  188\n",
      "X:  [array([ 4.68713575,  0.46090763, -2.15551815, -4.60812183,  4.27484765,\n",
      "        3.61624463, -2.32598541,  5.38591852])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 19s 10ms/step - loss: 1.8482 - acc: 0.4555 - val_loss: 1.5183 - val_acc: 0.6040\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 1.2037 - acc: 0.6975 - val_loss: 1.0942 - val_acc: 0.7120\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.8535 - acc: 0.7730 - val_loss: 0.8394 - val_acc: 0.7680\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 0.6622 - acc: 0.8215 - val_loss: 0.7395 - val_acc: 0.7840\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.5675 - acc: 0.8385 - val_loss: 0.6592 - val_acc: 0.7840\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.4898 - acc: 0.8630 - val_loss: 0.6113 - val_acc: 0.8010\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.4234 - acc: 0.8750 - val_loss: 0.5948 - val_acc: 0.8130\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.4106 - acc: 0.8740 - val_loss: 0.5727 - val_acc: 0.8240\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.3672 - acc: 0.8960 - val_loss: 0.5521 - val_acc: 0.8210\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 149us/step - loss: 0.3332 - acc: 0.9090 - val_loss: 0.5309 - val_acc: 0.8440\n",
      "Test loss: 0.5309481663703919\n",
      "Test accuracy: 0.844\n",
      "value:  0.5309481663703919\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  189\n",
      "X:  [array([ 3.52524308, -1.49734195,  0.11134451, -2.51256112,  0.9752618 ,\n",
      "       -0.9096499 ,  1.28478682,  2.49431832])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 2.1912 - acc: 0.2220 - val_loss: 1.9389 - val_acc: 0.3610\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 1.8624 - acc: 0.3790 - val_loss: 1.7497 - val_acc: 0.4470\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 1.7057 - acc: 0.4390 - val_loss: 1.6027 - val_acc: 0.5260\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 1.6062 - acc: 0.4750 - val_loss: 1.4849 - val_acc: 0.5650\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 1.4758 - acc: 0.5450 - val_loss: 1.3815 - val_acc: 0.6080\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 1.3982 - acc: 0.5530 - val_loss: 1.2929 - val_acc: 0.6610\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 1.3333 - acc: 0.5925 - val_loss: 1.2109 - val_acc: 0.6880\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 1.2583 - acc: 0.5980 - val_loss: 1.1425 - val_acc: 0.6950\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 153us/step - loss: 1.2055 - acc: 0.6295 - val_loss: 1.0915 - val_acc: 0.7030\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 1.1443 - acc: 0.6415 - val_loss: 1.0385 - val_acc: 0.7280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.0385424294471741\n",
      "Test accuracy: 0.728\n",
      "value:  1.0385424294471741\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  190\n",
      "X:  [array([ 5.06240774,  3.72857723,  0.97665849, -5.92371261,  4.01421968,\n",
      "        3.59496782, -1.64649076,  3.3175075 ])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.9948 - acc: 0.3160 - val_loss: 1.6216 - val_acc: 0.5840\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 1.5137 - acc: 0.5230 - val_loss: 1.2533 - val_acc: 0.6540\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 1.2560 - acc: 0.5970 - val_loss: 1.0357 - val_acc: 0.6860\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 1.0687 - acc: 0.6605 - val_loss: 0.8801 - val_acc: 0.7460\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.9771 - acc: 0.6630 - val_loss: 0.8111 - val_acc: 0.7560\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.8808 - acc: 0.7120 - val_loss: 0.7635 - val_acc: 0.7550\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.8552 - acc: 0.7110 - val_loss: 0.7211 - val_acc: 0.7730\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.7991 - acc: 0.7375 - val_loss: 0.6668 - val_acc: 0.7900\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.7739 - acc: 0.7380 - val_loss: 0.6498 - val_acc: 0.8030\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.7244 - acc: 0.7605 - val_loss: 0.6310 - val_acc: 0.8050\n",
      "Test loss: 0.6309955279827117\n",
      "Test accuracy: 0.805\n",
      "value:  0.6309955279827117\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  191\n",
      "X:  [array([ 4.33465549,  1.3376271 , -0.77379629, -4.95931266,  2.72854096,\n",
      "        2.35622727, -1.44338332,  2.57216143])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.7907 - acc: 0.4475 - val_loss: 1.4666 - val_acc: 0.5470\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 1.1537 - acc: 0.6810 - val_loss: 1.0936 - val_acc: 0.6560\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.8461 - acc: 0.7720 - val_loss: 0.8999 - val_acc: 0.7390\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.6844 - acc: 0.8110 - val_loss: 0.7522 - val_acc: 0.7760\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.5737 - acc: 0.8365 - val_loss: 0.6737 - val_acc: 0.8040\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.4984 - acc: 0.8645 - val_loss: 0.6310 - val_acc: 0.8070\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.4492 - acc: 0.8710 - val_loss: 0.5938 - val_acc: 0.8160\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 152us/step - loss: 0.4069 - acc: 0.8830 - val_loss: 0.5851 - val_acc: 0.8220\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 149us/step - loss: 0.3875 - acc: 0.8885 - val_loss: 0.5485 - val_acc: 0.8270\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.3580 - acc: 0.8960 - val_loss: 0.5343 - val_acc: 0.8310\n",
      "Test loss: 0.5342959895133972\n",
      "Test accuracy: 0.831\n",
      "value:  0.5342959895133972\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  192\n",
      "X:  [array([ 4.9921811 ,  2.14192848, -0.36432995, -6.80693803,  2.32201948,\n",
      "        1.13911899, -2.28769332,  0.86187094])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 1.8371 - acc: 0.4280 - val_loss: 1.4471 - val_acc: 0.6000\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 149us/step - loss: 1.1669 - acc: 0.6805 - val_loss: 1.0782 - val_acc: 0.7030\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.8369 - acc: 0.7825 - val_loss: 0.8459 - val_acc: 0.7630\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 149us/step - loss: 0.6522 - acc: 0.8140 - val_loss: 0.7052 - val_acc: 0.7980\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 0.5427 - acc: 0.8460 - val_loss: 0.6341 - val_acc: 0.8060\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.4721 - acc: 0.8625 - val_loss: 0.6127 - val_acc: 0.8110\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.4328 - acc: 0.8680 - val_loss: 0.5741 - val_acc: 0.8160\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.4043 - acc: 0.8760 - val_loss: 0.5477 - val_acc: 0.8260\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.3618 - acc: 0.8950 - val_loss: 0.5566 - val_acc: 0.8340\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.3427 - acc: 0.8970 - val_loss: 0.5242 - val_acc: 0.8280\n",
      "Test loss: 0.5242195010185242\n",
      "Test accuracy: 0.828\n",
      "value:  0.5242195010185242\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  193\n",
      "X:  [array([ 7.34034274,  3.21086759, -2.05974529, -4.85736641,  2.61024909,\n",
      "        4.18018935, -4.39876875,  3.83813794])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 1.7951 - acc: 0.4520 - val_loss: 1.4956 - val_acc: 0.5830\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 158us/step - loss: 1.1527 - acc: 0.6865 - val_loss: 1.0936 - val_acc: 0.6950\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 149us/step - loss: 0.8482 - acc: 0.7545 - val_loss: 0.8649 - val_acc: 0.7470\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.6779 - acc: 0.8045 - val_loss: 0.7623 - val_acc: 0.7560\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 154us/step - loss: 0.5926 - acc: 0.8185 - val_loss: 0.6788 - val_acc: 0.7890\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.5233 - acc: 0.8415 - val_loss: 0.6266 - val_acc: 0.8070\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.4641 - acc: 0.8575 - val_loss: 0.5917 - val_acc: 0.8130\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 169us/step - loss: 0.4250 - acc: 0.8745 - val_loss: 0.5851 - val_acc: 0.8210\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 164us/step - loss: 0.3935 - acc: 0.8760 - val_loss: 0.5590 - val_acc: 0.8260\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.3705 - acc: 0.8890 - val_loss: 0.5374 - val_acc: 0.8290\n",
      "Test loss: 0.5373608841896057\n",
      "Test accuracy: 0.829\n",
      "value:  0.5373608841896057\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  194\n",
      "X:  [array([ 5.78950683,  2.34809876, -1.19238558, -6.70007616,  3.81864163,\n",
      "        7.33676508, -2.82522192,  4.61072833])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 20s 10ms/step - loss: 1.7766 - acc: 0.4970 - val_loss: 1.4212 - val_acc: 0.6280\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 153us/step - loss: 1.1119 - acc: 0.7110 - val_loss: 1.0357 - val_acc: 0.7010\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 162us/step - loss: 0.8311 - acc: 0.7680 - val_loss: 0.8414 - val_acc: 0.7530\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 175us/step - loss: 0.6553 - acc: 0.8175 - val_loss: 0.7301 - val_acc: 0.7790\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.5513 - acc: 0.8485 - val_loss: 0.6546 - val_acc: 0.8080\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 153us/step - loss: 0.4879 - acc: 0.8595 - val_loss: 0.6035 - val_acc: 0.8240\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 152us/step - loss: 0.4561 - acc: 0.8705 - val_loss: 0.5761 - val_acc: 0.8320\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.3926 - acc: 0.8850 - val_loss: 0.5812 - val_acc: 0.8270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 154us/step - loss: 0.3707 - acc: 0.8910 - val_loss: 0.5536 - val_acc: 0.8300\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 156us/step - loss: 0.3533 - acc: 0.8890 - val_loss: 0.5512 - val_acc: 0.8350\n",
      "Test loss: 0.5511936802864075\n",
      "Test accuracy: 0.835\n",
      "value:  0.5511936802864075\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  195\n",
      "X:  [array([ 5.80706103,  0.0808022 , -1.29529207, -2.7238266 ,  1.98591818,\n",
      "        2.87513409, -2.5218143 ,  3.34025033])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 20s 10ms/step - loss: 1.8034 - acc: 0.4805 - val_loss: 1.4663 - val_acc: 0.6330\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 1.1609 - acc: 0.6960 - val_loss: 1.0838 - val_acc: 0.6920\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 0.8653 - acc: 0.7610 - val_loss: 0.8712 - val_acc: 0.7540\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 175us/step - loss: 0.6858 - acc: 0.8045 - val_loss: 0.7585 - val_acc: 0.7740\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 153us/step - loss: 0.5938 - acc: 0.8280 - val_loss: 0.6877 - val_acc: 0.8020\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 0.5300 - acc: 0.8485 - val_loss: 0.6513 - val_acc: 0.7910\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.4630 - acc: 0.8700 - val_loss: 0.6162 - val_acc: 0.8180\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.4272 - acc: 0.8720 - val_loss: 0.5862 - val_acc: 0.8290\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.4018 - acc: 0.8780 - val_loss: 0.5781 - val_acc: 0.8280\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.3673 - acc: 0.8895 - val_loss: 0.5630 - val_acc: 0.8340\n",
      "Test loss: 0.5629953060150147\n",
      "Test accuracy: 0.834\n",
      "value:  0.5629953060150147\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  196\n",
      "X:  [array([ 5.75516801,  2.30072522, -1.83406942, -6.65889794,  1.56011127,\n",
      "        2.50924351, -1.70827243,  2.50604581])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.8151 - acc: 0.4065 - val_loss: 1.4913 - val_acc: 0.5100\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 1.2096 - acc: 0.6365 - val_loss: 1.1167 - val_acc: 0.6350\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.9065 - acc: 0.7420 - val_loss: 0.9023 - val_acc: 0.7260\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.7233 - acc: 0.7980 - val_loss: 0.7706 - val_acc: 0.7670\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 152us/step - loss: 0.6076 - acc: 0.8245 - val_loss: 0.6832 - val_acc: 0.7870\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.5194 - acc: 0.8530 - val_loss: 0.6080 - val_acc: 0.8250\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 184us/step - loss: 0.4615 - acc: 0.8680 - val_loss: 0.6232 - val_acc: 0.8080\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 152us/step - loss: 0.4156 - acc: 0.8840 - val_loss: 0.5616 - val_acc: 0.8310\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.3899 - acc: 0.8785 - val_loss: 0.5496 - val_acc: 0.8260\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.3574 - acc: 0.8990 - val_loss: 0.5479 - val_acc: 0.8410\n",
      "Test loss: 0.5478662481307983\n",
      "Test accuracy: 0.841\n",
      "value:  0.5478662481307983\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  197\n",
      "X:  [array([ 6.36845616,  1.18061895, -1.61783541, -4.99897075,  3.09946975,\n",
      "        4.71796088, -4.41732148,  1.91221443])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 19s 10ms/step - loss: 1.8255 - acc: 0.4480 - val_loss: 1.4348 - val_acc: 0.5800\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 155us/step - loss: 1.1427 - acc: 0.7105 - val_loss: 1.0230 - val_acc: 0.7090\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.8374 - acc: 0.7755 - val_loss: 0.8150 - val_acc: 0.7640\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.6638 - acc: 0.8175 - val_loss: 0.6913 - val_acc: 0.7990\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.5500 - acc: 0.8450 - val_loss: 0.6157 - val_acc: 0.8100\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 165us/step - loss: 0.4912 - acc: 0.8610 - val_loss: 0.5830 - val_acc: 0.8210\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 150us/step - loss: 0.4305 - acc: 0.8765 - val_loss: 0.5751 - val_acc: 0.8130\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 152us/step - loss: 0.3973 - acc: 0.8950 - val_loss: 0.5484 - val_acc: 0.8270\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 154us/step - loss: 0.3665 - acc: 0.9005 - val_loss: 0.5326 - val_acc: 0.8340\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 0.3329 - acc: 0.9010 - val_loss: 0.5301 - val_acc: 0.8320\n",
      "Test loss: 0.5300519208908081\n",
      "Test accuracy: 0.832\n",
      "value:  0.5300519208908081\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  198\n",
      "X:  [array([ 7.52298321,  1.99819583, -1.3754492 , -4.78126476,  1.33268642,\n",
      "        1.19746685, -4.13178533,  4.76090936])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 29s 14ms/step - loss: 1.7823 - acc: 0.4955 - val_loss: 1.4005 - val_acc: 0.6200\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 170us/step - loss: 1.1092 - acc: 0.7110 - val_loss: 0.9913 - val_acc: 0.7150\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.7867 - acc: 0.7830 - val_loss: 0.8097 - val_acc: 0.7490\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.6388 - acc: 0.8200 - val_loss: 0.6881 - val_acc: 0.7970\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.5418 - acc: 0.8475 - val_loss: 0.6385 - val_acc: 0.8080\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.4760 - acc: 0.8595 - val_loss: 0.5853 - val_acc: 0.8300\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.4264 - acc: 0.8715 - val_loss: 0.5733 - val_acc: 0.8180\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.4041 - acc: 0.8815 - val_loss: 0.5434 - val_acc: 0.8290\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.3576 - acc: 0.8965 - val_loss: 0.5585 - val_acc: 0.8260\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.3400 - acc: 0.9005 - val_loss: 0.5487 - val_acc: 0.8260\n",
      "Test loss: 0.5487470483779907\n",
      "Test accuracy: 0.826\n",
      "value:  0.5487470483779907\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n",
      "\n",
      "\n",
      "Generation:  199\n",
      "X:  [array([ 7.56139581,  3.10932663, -0.35028048, -4.90760605,  1.96176792,\n",
      "        4.16172629, -2.9927492 ,  2.61556096])]\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 24s 12ms/step - loss: 1.8040 - acc: 0.4630 - val_loss: 1.4760 - val_acc: 0.6520\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 1.1714 - acc: 0.7020 - val_loss: 1.0863 - val_acc: 0.7370\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.8445 - acc: 0.7845 - val_loss: 0.8501 - val_acc: 0.7720\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 142us/step - loss: 0.6707 - acc: 0.8070 - val_loss: 0.7294 - val_acc: 0.7950\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 147us/step - loss: 0.5530 - acc: 0.8410 - val_loss: 0.6517 - val_acc: 0.8210\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 157us/step - loss: 0.4894 - acc: 0.8655 - val_loss: 0.5982 - val_acc: 0.8260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 157us/step - loss: 0.4442 - acc: 0.8735 - val_loss: 0.5926 - val_acc: 0.8200\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 143us/step - loss: 0.3984 - acc: 0.8865 - val_loss: 0.5521 - val_acc: 0.8300\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 0.3718 - acc: 0.8870 - val_loss: 0.5335 - val_acc: 0.8390\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.3484 - acc: 0.8995 - val_loss: 0.5398 - val_acc: 0.8400\n",
      "Test loss: 0.5397809641361236\n",
      "Test accuracy: 0.84\n",
      "value:  0.5397809641361236\n",
      "optimizer:  Instance of CMA(dimension=8, budget=200, num_workers=1)\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "\n",
    "for _ in range(optimizer.budget):\n",
    "        print('\\n\\nGeneration: ', _)\n",
    "        x = [0]*optimizer.num_workers\n",
    "        for worker in range(optimizer.num_workers):\n",
    "            x[worker] = optimizer.ask()\n",
    "            print('X: ', x)\n",
    "            value = ifunc(x[worker])\n",
    "        history.append([_, x[worker], value])\n",
    "        print('value: ', value)\n",
    "        optimizer.tell(x[worker], value)\n",
    "        print('optimizer: ', optimizer)\n",
    "\n",
    "recommendation = optimizer.provide_recommendation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, array([-0.09269268,  0.55326621, -0.42939344,  1.05164666,  0.51763137,\n",
       "          0.64493807, -0.9906441 ,  1.58133418]), 0.7328361854553223],\n",
       " [1, array([ 0.21298084, -0.64473395, -0.41387274, -2.02706103, -0.05518718,\n",
       "          0.12452323,  0.92247129,  1.04696677]), 0.6340950512886048],\n",
       " [2, array([ 0.28965749,  0.15380109,  0.02414381, -0.21871814,  0.53369279,\n",
       "          0.06990533, -0.28987532,  0.15773437]), 0.6551961889266967],\n",
       " [3, array([ 0.62385514, -1.09495526, -0.35331223,  0.2441219 ,  1.17612997,\n",
       "         -0.37738797, -0.57284834, -0.87968619]), 0.9860171785354614],\n",
       " [4, array([ 0.5244142 ,  0.91928177,  1.12844669,  0.21487503, -0.36481139,\n",
       "          0.55842785, -1.17821118, -0.11893813]), 0.731190789937973],\n",
       " [5, array([ 1.81053031,  1.13430547,  0.47886339,  0.73525086,  0.47211186,\n",
       "         -0.40755953,  0.57646331, -0.09720461]), 0.9453985605239869],\n",
       " [6, array([ 0.27490747,  0.01745295, -0.24211933, -0.8499886 ,  1.07691014,\n",
       "         -1.11389829,  0.95905237, -0.88730265]), 0.6899283213615417],\n",
       " [7, array([ 0.63727544,  1.0175647 , -1.60243881, -0.40318969,  0.98844436,\n",
       "         -1.16659987, -0.50661263, -0.66528572]), 1.2684972891807555],\n",
       " [8, array([-1.08041021,  0.21922919,  0.30301077, -0.25227692, -0.6832679 ,\n",
       "          0.08022235,  2.80902131,  0.35737184]), 1.439442907333374],\n",
       " [9, array([ 0.81639642, -1.20567269,  1.86047874,  0.27378403, -0.05203723,\n",
       "         -0.3207135 , -0.87307797, -1.31281093]), 1.2567339963912965],\n",
       " [10, array([ 1.14202787, -1.08729605,  0.68345415, -0.13565247, -0.29477201,\n",
       "          1.9386896 ,  0.88609916, -0.05462069]), 1.159443039894104],\n",
       " [11, array([ 1.25539543, -0.40452311, -1.18030323, -0.39193933, -1.67952292,\n",
       "         -0.98873451,  1.97730251,  1.08134663]), 1.032375214099884],\n",
       " [12, array([ 1.12572465,  0.01975709, -0.32362221, -1.75046457, -0.67786131,\n",
       "         -0.83155089, -1.01941366,  1.4317467 ]), 0.7890836458206176],\n",
       " [13, array([ 1.45004141, -0.85726089, -2.86574869, -0.15317516,  0.5710983 ,\n",
       "         -0.04987153,  0.60695716,  1.1230574 ]), 0.895812539100647],\n",
       " [14, array([ 0.41635032, -0.85623786,  0.67103953, -2.66892622, -0.16111222,\n",
       "          0.32923237,  0.56279881,  0.0442318 ]), 0.6928043441772461],\n",
       " [15, array([ 1.16605777, -0.5706412 , -0.64767999,  0.09336188,  0.27771034,\n",
       "         -0.41687396,  0.83799074,  0.75341313]), 0.864314013004303],\n",
       " [16, array([ 0.77865058, -0.42174239, -1.05509185, -3.09356207, -0.36655368,\n",
       "         -0.94919289,  1.22215305, -0.0940158 ]), 0.8978030796051025],\n",
       " [17, array([-0.00959574, -1.12399197, -1.16998495, -2.31330944,  0.32455166,\n",
       "          0.48692677, -0.21449297,  0.23390757]), 0.9490737228393554],\n",
       " [18, array([-0.99422522,  0.92681088, -1.5414679 , -3.10337179,  2.00190271,\n",
       "          1.49393425, -0.45669605, -0.96541593]), 0.8030706219673157],\n",
       " [19, array([ 0.35333893,  1.21238503,  0.37160002, -3.26538764,  0.42713573,\n",
       "         -0.83997413,  0.22006203,  0.45719713]), 1.3908320531845093],\n",
       " [20, array([ 0.51748746,  0.42089782,  0.1711911 , -3.206645  , -1.01483138,\n",
       "          1.11799856, -1.26703942,  0.6594474 ]), 0.6569387140274048],\n",
       " [21, array([ 1.60558901,  0.25329283,  1.66673182, -2.85521397, -0.80886087,\n",
       "          0.68948264, -0.48153757,  0.48747006]), 0.5955339164733887],\n",
       " [22, array([ 0.00808321, -0.72662903,  0.46994847, -1.65239896,  2.88787351,\n",
       "         -0.42328406, -1.34759329,  0.54277124]), 0.7653580327033996],\n",
       " [23, array([ 0.756588  ,  0.54543808, -0.74427757, -2.84331963,  0.3561457 ,\n",
       "         -0.49942684,  0.26959955,  0.50215652]), 0.7431003746986389],\n",
       " [24, array([-0.52474251, -0.78972806, -0.57240373, -2.63063215, -0.57505536,\n",
       "          0.42784324, -0.89156176, -0.55259753]), 0.996847978591919],\n",
       " [25, array([-0.10328472,  0.16601436, -2.09499985, -2.75254745, -1.63242971,\n",
       "          0.39049102,  0.35930636, -0.68659582]), 0.7539857225418091],\n",
       " [26, array([-0.29076793, -0.17088875,  0.19143454, -1.82545956,  0.60440566,\n",
       "         -0.3640707 ,  0.66900027, -0.09965014]), 1.4590978574752809],\n",
       " [27, array([ 2.3150699 , -0.34831851, -2.44333297, -3.08155362, -1.62955979,\n",
       "          0.33947829,  0.20968822, -0.83182945]), 0.6577464814186096],\n",
       " [28, array([ 0.31574849,  0.43835419, -0.32729899, -2.56652078, -0.122415  ,\n",
       "          0.80372881,  0.7477288 , -0.29746647]), 0.6147148942947388],\n",
       " [29, array([-0.42436571, -1.31617394, -0.57382122, -0.81335654, -0.75053355,\n",
       "          1.92242929, -0.99312502,  0.66946387]), 0.701399920463562],\n",
       " [30, array([ 0.8246256 ,  0.0208691 ,  0.19890615, -1.32777999,  1.30586423,\n",
       "         -0.07789132, -0.01581269, -2.05204435]), 0.9378352174758912],\n",
       " [31, array([ 0.45708003, -1.07550399,  0.66204696, -3.66022183,  1.11110307,\n",
       "          0.75889726,  0.37287868,  1.08937934]), 1.0375330862998962],\n",
       " [32, array([ 0.39030454, -0.14143268,  2.0993587 , -3.27550866, -1.83507015,\n",
       "          0.47238355, -1.28049658,  0.18961969]), 0.7195991201400757],\n",
       " [33, array([ 1.56541509,  0.53284535, -0.21536317, -3.54208423,  0.53811154,\n",
       "         -0.04787193, -0.9235255 ,  1.33154316]), 0.520056704044342],\n",
       " [34, array([ 1.5323534 , -0.62815683,  1.86198494, -4.21405034, -0.42809275,\n",
       "          0.99980283, -0.1081776 , -0.62588234]), 0.9726359119415283],\n",
       " [35, array([-0.04174474,  0.8743338 ,  1.18934489, -2.15703034,  0.61511425,\n",
       "          0.37980877, -0.84757744,  0.97987274]), 1.4096308345794677],\n",
       " [36, array([ 2.46034743,  1.147719  ,  1.42441264, -3.86424425,  0.27336671,\n",
       "          1.05299863,  0.9853307 ,  0.56954611]), 0.9279166145324707],\n",
       " [37, array([ 1.51030451,  0.37744224, -0.48149611, -2.15278907,  0.19527302,\n",
       "         -0.41102538, -0.79418625,  0.23770934]), 0.5058998556137085],\n",
       " [38, array([ 1.02426506,  1.52335644,  1.78333951, -2.23170818,  1.54234207,\n",
       "          0.80664506,  0.40266854,  0.77568057]), 0.8770010938644409],\n",
       " [39, array([ 1.11806076, -0.36006368,  1.37073751, -3.77023134, -0.87156758,\n",
       "          1.19356662,  0.74644433,  1.72850546]), 1.1236909770965575],\n",
       " [40, array([-0.10759984,  1.0162685 ,  0.42591725, -2.05693944, -0.19694955,\n",
       "          0.15723131, -0.47904393,  0.77624032]), 0.8281450591087341],\n",
       " [41, array([ 0.49029965,  0.24325236,  0.12053956, -2.82324743,  0.62365374,\n",
       "          1.08038185, -1.62681783,  1.91724548]), 0.6487774105072022],\n",
       " [42, array([ 2.04996995,  0.11465616, -0.02806642, -1.76789418, -1.0941688 ,\n",
       "          2.22773554, -0.94971019, -0.12705572]), 0.5874707436561585],\n",
       " [43, array([ 1.97944557,  0.28786759,  0.10647442, -2.73666721,  0.7605585 ,\n",
       "         -1.04311944, -1.62590862,  0.9210404 ]), 0.865682035446167],\n",
       " [44, array([-0.02682157,  0.28515549,  0.61644359, -3.84333565, -1.75024259,\n",
       "          0.43761167, -1.06826222, -0.45962082]), 0.926406536102295],\n",
       " [45, array([ 0.13609652,  2.11406837,  0.6692622 , -3.74950149, -0.05961125,\n",
       "          1.37066687, -1.84928833,  0.37325652]), 0.6709898180961609],\n",
       " [46, array([ 0.99485025,  1.16593043,  1.406864  , -4.65660156, -1.53004755,\n",
       "          1.68813964, -1.48384599,  0.19046278]), 0.630279260635376],\n",
       " [47, array([ 1.76220209,  0.56034504,  0.69770904, -5.42013826, -0.85784633,\n",
       "         -0.22186464, -0.92890303,  1.19300172]), 0.6700393772125244],\n",
       " [48, array([ 2.37037465,  1.13007165,  1.07897682, -2.8735586 ,  1.76127065,\n",
       "          0.40250537,  1.11377829,  0.52821612]), 1.0015019416809081],\n",
       " [49, array([ 1.20946495, -1.20954874, -0.56879835, -3.55326339, -0.10211046,\n",
       "         -0.40871376, -1.21598608,  0.98720265]), 0.6339644265174865],\n",
       " [50, array([ 3.18730386, -1.09242573,  1.29195263, -2.72395612, -0.31100121,\n",
       "          0.86282139, -2.54392624,  0.70917063]), 0.7735457377433776],\n",
       " [51, array([ 2.07969508,  1.73601885,  0.95513976, -3.88744139, -1.63444612,\n",
       "          0.2501613 , -1.50342331, -0.08258337]), 0.6856090092658996],\n",
       " [52, array([ 0.84265345, -0.49089025,  0.05838559, -2.69060134,  0.49075995,\n",
       "          3.07263144, -1.18514316, -0.46563475]), 0.7306123986244202],\n",
       " [53, array([ 2.78421254,  0.86423707,  0.10673327, -1.59435981, -0.15959445,\n",
       "          2.58189527, -1.32392111,  0.55201052]), 0.6291264350414276],\n",
       " [54, array([ 0.30782288, -1.20783141,  1.83223623, -2.22825622, -0.89864137,\n",
       "          0.31439505, -1.10497313, -0.59833232]), 1.1881724472045898],\n",
       " [55, array([ 4.41563562,  1.47332145,  0.03765885, -3.40323472, -1.97246302,\n",
       "          2.09629375, -2.33023474, -0.88621816]), 0.6377812728881836],\n",
       " [56, array([ 0.92695242, -0.57951523,  0.10396251, -3.75416254, -1.64886698,\n",
       "          0.37234412, -0.56162984, -0.14372717]), 0.7554785504341125],\n",
       " [57, array([ 1.5555076 , -0.37576122,  1.36924282, -3.27339905, -1.10777024,\n",
       "          2.93518492, -1.5767656 , -0.16022214]), 0.9997304515838623],\n",
       " [58, array([ 0.17564267, -1.11112344,  0.82735525, -2.89672522, -0.74171469,\n",
       "          0.75174632, -0.15699057,  1.39726025]), 1.070279586791992],\n",
       " [59, array([ 2.92927544,  1.39946343,  1.26831072, -1.1650114 , -1.03427136,\n",
       "          0.46308559, -1.61301813,  1.28855425]), 0.6060016765594483],\n",
       " [60, array([ 3.5440188 ,  0.84543734,  0.8508538 , -1.29181815, -0.83195633,\n",
       "         -0.87493716, -0.85175267,  0.47183771]), 0.7051867136955261],\n",
       " [61, array([ 3.89161143,  0.25966629,  0.82266552, -0.49297637, -1.80595912,\n",
       "          1.81210062, -1.28640915,  0.08603521]), 1.0577046642303467],\n",
       " [62, array([ 2.8436322 ,  2.24607199,  1.93062712, -3.38528758, -1.99478681,\n",
       "         -0.50849138, -2.99023815,  1.20307164]), 0.9070820927619934],\n",
       " [63, array([ 4.80414307,  0.99780937,  1.4642576 , -1.8696746 , -2.73943249,\n",
       "          0.40276411, -1.56966739, -0.0881968 ]), 0.7251115140914917],\n",
       " [64, array([ 1.8120189 ,  1.00121784,  2.43912207, -3.38264472, -2.43364005,\n",
       "          0.60231191, -1.49501729, -0.26454269]), 0.6325192601680756],\n",
       " [65, array([ 1.72680652,  0.76390913,  0.94004469, -1.61124743, -1.24852652,\n",
       "          2.64684286, -2.21647472,  1.69261295]), 0.7636477851867676],\n",
       " [66, array([ 3.67074083,  2.25217962,  0.38598627, -2.25917855, -2.60612272,\n",
       "          1.01456277, -1.25291552, -0.72017844]), 0.7230440139770508],\n",
       " [67, array([ 3.32691619,  1.53733982, -0.30809807, -2.77464146, -1.47954478,\n",
       "          0.83621251, -0.03912857,  1.29349572]), 0.5793622305393219],\n",
       " [68, array([ 3.82222073,  0.7211193 ,  0.87529062, -1.15332802, -1.05487545,\n",
       "          0.21822655, -3.0170832 , -0.05388049]), 0.7259521913528443],\n",
       " [69, array([ 2.26511492,  2.10113585,  0.47598134, -2.68143839, -2.9896546 ,\n",
       "          2.34597847, -3.3357611 ,  1.3430619 ]), 0.6239077498912812],\n",
       " [70, array([ 1.48678849,  2.92105858,  0.95910065, -2.11742369, -3.59260894,\n",
       "          1.40444322, -1.33503355,  2.12375205]), 0.7228664886951447],\n",
       " [71, array([ 2.39037579,  2.28156574,  0.75275274, -2.85389963, -2.96203652,\n",
       "          0.86868942, -3.25003334,  1.15056741]), 0.741227352142334],\n",
       " [72, array([ 4.27778308,  2.80707624,  0.94725923, -2.84854766, -1.58274704,\n",
       "          2.55568315, -1.30008223, -0.96092734]), 0.6054144387245178],\n",
       " [73, array([ 3.77876711,  0.87786927,  0.95471207, -2.72149927, -1.74571499,\n",
       "          2.5610389 , -1.778614  ,  2.30723436]), 0.5940132074356079],\n",
       " [74, array([ 2.98507431,  2.94865191,  0.10535769, -2.2704296 , -3.52787326,\n",
       "         -0.07351972, -2.82467053,  1.29225905]), 0.7372424449920655],\n",
       " [75, array([ 0.24648744,  2.06031851,  1.51865545, -1.31060644, -1.1673427 ,\n",
       "          0.49787962, -2.15840186,  0.17951783]), 0.6464420447349548],\n",
       " [76, array([ 2.97446202,  0.85606095,  1.48376502, -3.12939815, -2.35326215,\n",
       "          0.98634201, -1.45606966, -0.42765787]), 0.7589750204086304],\n",
       " [77, array([ 2.30456417,  2.72426615,  0.06057529, -1.21906792, -0.88433904,\n",
       "          0.6055158 , -2.25461594,  2.90285346]), 0.6225328178405761],\n",
       " [78, array([ 3.78631667,  0.95420378, -2.01276182, -3.90068669, -0.58380242,\n",
       "          2.63546594, -2.08674166,  1.87206354]), 0.5548022160530091],\n",
       " [79, array([ 2.94021161,  1.16416126,  0.58270158, -4.32163294, -2.60324027,\n",
       "         -0.06181035, -2.08364002,  0.94444456]), 0.6262414984703064],\n",
       " [80, array([ 6.09971568,  1.4738992 ,  1.3074295 , -4.18720328, -3.55533842,\n",
       "          1.95615265, -2.54564421,  2.244217  ]), 0.6191890931129456],\n",
       " [81, array([ 2.06206633,  0.19636901, -3.08984037, -2.96472731, -0.67830791,\n",
       "          3.63925214, -1.97223421,  1.71882582]), 0.5212077980041504],\n",
       " [82, array([ 4.5858139 ,  0.72556867, -0.25643228, -3.35233424, -0.71196514,\n",
       "          3.6691439 , -3.3424144 ,  1.42257852]), 0.5782352848052978],\n",
       " [83, array([ 5.06636131,  1.77036758, -0.82444393, -3.3815377 , -0.08502776,\n",
       "          0.53432524, -3.40618954,  3.13523386]), 0.5461482362747192],\n",
       " [84, array([ 4.10683676,  0.60031239, -2.06894155, -5.01599925, -1.13586864,\n",
       "          3.93556989, -1.53334787,  2.05751573]), 0.5365931224822998],\n",
       " [85, array([ 2.3571611 ,  0.71114767,  0.12503535, -3.20534223, -2.44650659,\n",
       "          1.38418519, -1.94477644,  2.50001993]), 0.6567098784446717],\n",
       " [86, array([ 3.2014277 ,  2.06226878, -0.28258901, -4.49373372, -3.15329928,\n",
       "          3.81878875, -4.04313702, -0.70294745]), 0.5524451816082001],\n",
       " [87, array([ 3.72578728,  0.90892139, -2.382588  , -5.58644163, -0.77577454,\n",
       "          4.53234102, -0.06234381,  1.84231153]), 0.5372935509681702],\n",
       " [88, array([ 3.19200553, -1.51495277,  0.53956406, -3.87113681, -0.08933373,\n",
       "          1.90474264, -1.86077526,  1.20190836]), 0.7362502367496491],\n",
       " [89, array([ 4.1733622 ,  3.09852372, -1.32615114, -3.86637672, -1.84528737,\n",
       "          4.78084057, -1.32098953, -0.63089398]), 0.605371361732483],\n",
       " [90, array([ 4.71735018,  0.78577955, -3.01376975, -5.82777186, -1.48017861,\n",
       "          4.25778091, -1.1539899 ,  3.59723801]), 0.5619826483726501],\n",
       " [91, array([ 2.09215412, -0.0369703 , -1.07877318, -1.60788788,  2.16321951,\n",
       "          4.61861998, -2.7286449 ,  2.75293332]), 0.5842125029563904],\n",
       " [92, array([ 4.79326064,  1.45188863, -1.72430644, -3.0605516 , -1.15182244,\n",
       "          1.40790559, -2.03172321,  1.95585067]), 0.5598447875976562],\n",
       " [93, array([ 1.61522107, -0.70030627, -0.06818433, -0.84309862, -0.65305563,\n",
       "          5.04614748, -3.86217265,  1.13554189]), 0.6236981434822082],\n",
       " [94, array([ 3.27723913,  0.50541946, -3.60582262, -0.50714344,  1.57803471,\n",
       "          3.71256521, -1.93633727,  2.67728858]), 0.49487885618209837],\n",
       " [95, array([ 5.15744694,  1.33039074, -2.3411183 , -2.3431805 ,  0.16843843,\n",
       "          6.03713408, -2.32647389,  1.15987999]), 0.5157503192424774],\n",
       " [96, array([ 1.02277975, -1.55235308, -2.44549704, -6.2102564 , -1.96097082,\n",
       "          3.62830851, -0.04657044,  2.25178051]), 0.6063630223274231],\n",
       " [97, array([ 2.2204947 , -0.26125352, -2.51267438, -6.2506697 , -0.13658535,\n",
       "          6.73980141, -4.64285944,  2.1231968 ]), 0.5861439852714538],\n",
       " [98, array([ 3.83407481, -0.70240054, -2.41195206, -3.32729097, -1.64067965,\n",
       "          3.62694932, -1.48743622,  1.63364866]), 0.6099386584758758],\n",
       " [99, array([ 3.40425645, -1.4135006 , -3.42021656, -6.25935597,  0.30047694,\n",
       "          3.28720776, -0.8614438 ,  2.21121988]), 0.7490091099739075],\n",
       " [100, array([ 5.69984776,  1.13908093, -2.61891754, -1.81115463,  2.69711049,\n",
       "          4.934929  , -2.97598665,  3.7451673 ]), 0.5074824728965759],\n",
       " [101, array([ 5.88474192,  0.75547018, -2.64860065, -1.30082551,  2.14310366,\n",
       "          3.81901067, -2.95603164,  4.14407192]), 0.5365146656036377],\n",
       " [102, array([ 4.59614279,  3.32351623, -1.64774536, -3.55956429, -0.822075  ,\n",
       "          6.26098141, -2.51249048, -0.18356209]), 0.52726726603508],\n",
       " [103, array([ 6.21913248,  2.13389713, -0.78085445, -1.75754791,  2.81593603,\n",
       "          3.62021828, -2.01543184,  1.86825632]), 0.5269650466442108],\n",
       " [104, array([ 6.46485607,  1.77355226, -3.10631143,  2.99035495,  0.37208337,\n",
       "          6.8093693 , -3.32767385,  2.92412485]), 0.5707325887680054],\n",
       " [105, array([ 4.97356962,  3.06784683, -0.08471379,  1.20258299,  0.95967369,\n",
       "          3.9310572 , -3.29150359,  2.86063311]), 0.576670114517212],\n",
       " [106, array([ 2.2150924 , -0.34895606, -4.28482436, -3.79723209,  0.96787185,\n",
       "          5.47868897, -3.20608584,  3.4148387 ]), 0.5935932383537292],\n",
       " [107, array([ 2.32807031,  0.71977928, -2.27037035, -3.34793453,  1.72146872,\n",
       "          5.24545912, -2.79782637,  3.42053415]), 0.5315847759246826],\n",
       " [108, array([ 3.42443444,  0.7654149 , -1.55790476, -0.78629436, -2.29488837,\n",
       "          2.66414501, -1.10360803,  1.33882929]), 0.5664532654285431],\n",
       " [109, array([ 6.23719527,  0.86132361, -1.90924491, -4.45448008, -1.64001229,\n",
       "          1.92890529, -0.58681599,  2.55692983]), 0.5595020270347595],\n",
       " [110, array([ 6.5309046 ,  1.5515505 , -0.88216765,  0.53431505, -1.0245357 ,\n",
       "          8.01113932, -3.46180409,  0.09076313]), 0.543512188911438],\n",
       " [111, array([ 5.78265874,  2.11121008, -1.05475215,  1.94074058,  0.33163139,\n",
       "          3.38917832, -1.98779235, -1.19277322]), 0.709015549659729],\n",
       " [112, array([ 6.23412363,  0.2952361 , -6.60499207, -4.17968761,  3.26299892,\n",
       "          3.43142602, -2.94057936,  5.95537732]), 0.5252707891464233],\n",
       " [113, array([ 5.61598197,  1.81423399, -1.88065251, -4.92576626,  0.38827258,\n",
       "          3.52251195, -2.54398437,  0.64643267]), 0.5470981981754303],\n",
       " [114, array([ 5.81002684,  0.21248402, -2.25151627, -3.06182005, -0.33230492,\n",
       "          4.21062159, -3.48033303,  2.84864447]), 0.5235950438976288],\n",
       " [115, array([ 4.25210732,  1.2706129 , -2.05209393, -0.75310199,  1.70785604,\n",
       "          3.7628016 , -3.40986745,  2.01230622]), 0.5584336712360382],\n",
       " [116, array([ 3.05666962,  2.78683172, -1.9876255 , -0.5310049 ,  4.60302761,\n",
       "          3.96915217, -2.85375322, -0.30859595]), 0.5631337742805481],\n",
       " [117, array([ 5.33280325,  1.2231012 , -0.92139016, -4.74035386,  3.99683596,\n",
       "          5.60420746, -2.90174285,  4.38788255]), 0.5260270953178405],\n",
       " [118, array([ 2.95080194, -0.82765317, -5.85895763, -4.82033438,  1.6098045 ,\n",
       "          6.56751126, -1.25741816,  3.95191152]), 0.5866342720985412],\n",
       " [119, array([10.0721148 ,  1.24892961, -2.47784563, -2.82641368,  0.84971768,\n",
       "          9.94774507, -5.05588623,  4.13226366]), 0.5281899089813232],\n",
       " [120, array([ 6.81647503, -0.07615418, -4.59270927, -4.41692302,  2.16708229,\n",
       "          4.43620399, -0.25011756,  3.60065932]), 0.6066470031738281],\n",
       " [121, array([ 6.4047153 ,  0.09517342,  0.58794757, -4.60719967, -2.36914225,\n",
       "          5.50197719, -4.34667546,  2.83412753]), 0.5868525202274323],\n",
       " [122, array([ 2.25574641, -0.7699617 , -5.37358482, -4.84815708,  2.19457577,\n",
       "          4.97692241, -3.11257129,  2.63281395]), 0.6215929484367371],\n",
       " [123, array([ 7.21353357,  1.20266689, -4.0412532 , -2.95481994,  0.59657133,\n",
       "          6.14319024, -1.96503116,  0.75817951]), 0.5327401804924011],\n",
       " [124, array([ 4.74776119, -1.06405269, -6.91559596, -3.7433817 ,  3.07366364,\n",
       "          3.6067618 , -6.21655549,  3.72219712]), 0.6522130780220031],\n",
       " [125, array([ 8.63608188, -1.29717941, -3.95556973, -4.72019254,  2.72761173,\n",
       "          4.52419628, -6.81667344,  4.20198987]), 0.5947833647727966],\n",
       " [126, array([ 4.42960879, -1.02203238, -6.497026  , -1.27049445,  5.43251215,\n",
       "          1.13799741, -2.71232044,  2.78826077]), 0.5907617163658142],\n",
       " [127, array([ 6.36582162,  0.39449516, -3.24868479, -5.31907289, -1.18218184,\n",
       "          6.30939312, -4.20020724,  1.70626313]), 0.4977524590492248],\n",
       " [128, array([ 3.13437311, -0.24181024, -4.37593447, -1.04997563,  3.60248165,\n",
       "          5.76909076, -2.50732972,  2.15552268]), 0.6068514037132263],\n",
       " [129, array([ 5.58627083,  0.54818315, -2.09134127, -4.89561073,  3.93244714,\n",
       "          6.60696138, -3.16976609,  8.2587011 ]), 0.5387520217895507],\n",
       " [130, array([ 4.05777456,  0.99039393, -5.84459139, -4.81555895,  1.38503545,\n",
       "          4.44923466, -2.00692369,  2.5164312 ]), 0.5193825068473816],\n",
       " [131, array([ 6.85342905, -0.37044126, -3.69317821, -7.45406089,  2.25792854,\n",
       "          7.06767892, -2.54601174,  2.54506518]), 0.6138137512207031],\n",
       " [132, array([ 9.61086907,  1.88530356, -1.75632364, -6.82201998, -0.77500139,\n",
       "          6.40650459, -5.34875179,  2.66649627]), 0.597639371395111],\n",
       " [133, array([ 3.96350083, -1.25502735,  0.41694269, -6.11110587, -1.93458796,\n",
       "          6.12379297, -1.88370528,  1.77408941]), 0.7324194474220276],\n",
       " [134, array([ 6.08710243,  1.1064971 , -4.94798519, -3.05557641, -0.11009849,\n",
       "          9.2434712 , -3.35382564,  2.915663  ]), 0.5311318688392639],\n",
       " [135, array([ 8.5297387 , -0.1081356 , -1.39438174, -6.44220053, -2.25049455,\n",
       "          6.2068134 , -4.72170383,  3.37953875]), 0.5776530499458313],\n",
       " [136, array([ 7.17170681,  0.65929043, -1.74269125, -4.84762345, -2.76961495,\n",
       "          4.64231065, -5.25243873,  2.72000317]), 0.5868436901569366],\n",
       " [137, array([ 8.02868695,  2.38846659, -4.95172512, -8.42715683,  0.96871073,\n",
       "          7.74187157, -3.79297271,  1.2791495 ]), 0.5417812223434448],\n",
       " [138, array([ 8.6768063 ,  1.36837974, -2.83176543, -3.53021556,  2.58131879,\n",
       "          7.93473924, -4.27005309,  2.91657774]), 0.5276754479408264],\n",
       " [139, array([ 7.93590415,  0.86511798, -2.17767066, -1.7175999 , -0.12235035,\n",
       "          6.48454177, -3.18368726,  1.73596397]), 0.5999332513809205],\n",
       " [140, array([ 7.04477377,  2.26189823, -2.25026332, -4.7300546 , -0.8146698 ,\n",
       "          7.80886016, -3.37289685,  3.9254207 ]), 0.5749449725151062],\n",
       " [141, array([ 4.14717299,  1.66495555, -3.69313221, -4.71263051,  3.45290696,\n",
       "          4.02130699, -3.54582262,  2.28647911]), 0.5153992247581481],\n",
       " [142, array([ 5.84692284, -0.13921402, -1.8476169 , -4.5630084 , -2.02197626,\n",
       "          4.58059158, -4.3693334 ,  4.69654672]), 0.6018857460021972],\n",
       " [143, array([ 5.73086016,  3.79702278, -4.48870168, -7.32107286,  1.11879749,\n",
       "          8.29178158, -3.73394453,  1.12760093]), 0.5270823917388916],\n",
       " [144, array([ 5.18967649,  1.55998183, -3.01642902, -4.15012085,  0.42537403,\n",
       "          4.95343557, -3.18136091,  1.70416957]), 0.5554790019989013],\n",
       " [145, array([ 6.62870683,  0.29870014, -9.21257241, -1.3835345 ,  3.26922478,\n",
       "          2.89225244, -3.42935849,  4.74609547]), 0.5632277038097382],\n",
       " [146, array([ 5.74614222,  2.64585603, -4.57585062, -0.53006166,  2.04446522,\n",
       "          4.96811782, -3.07927307,  1.41275043]), 0.5421569862365723],\n",
       " [147, array([ 5.37252226,  0.78978882, -4.80779051, -3.48060474,  0.9847261 ,\n",
       "          4.90611182, -2.39626965,  1.378743  ]), 0.5316625089645386],\n",
       " [148, array([ 7.81127435,  1.27460621, -4.82118939, -6.16611849,  2.68118551,\n",
       "          8.11030816, -4.40315552,  1.35629759]), 0.558225801229477],\n",
       " [149, array([ 6.91493555,  1.24487783, -5.02706103, -3.72540513,  2.6389685 ,\n",
       "          7.28995241, -3.63593525,  2.83251778]), 0.4985659942626953],\n",
       " [150, array([ 4.90238657,  0.83136702, -5.91783243, -4.30131529,  3.80297633,\n",
       "          6.11843473, -4.25851925,  4.02214568]), 0.5241763343811036],\n",
       " [151, array([ 4.80817908,  1.53647665, -3.11540273, -2.12989785,  3.34978889,\n",
       "          6.51624366, -1.30625797,  2.66754671]), 0.5127893614768982],\n",
       " [152, array([ 7.74243743,  1.30050886, -7.11214922, -1.86109903,  1.4187137 ,\n",
       "          8.38519629, -3.71653043,  2.6523582 ]), 0.5356270191669464],\n",
       " [153, array([ 5.2976064 ,  1.04319769, -4.12518829, -4.36608101,  5.33592397,\n",
       "          3.57824874, -3.43089194,  3.29691678]), 0.5079165315628051],\n",
       " [154, array([ 9.79181165,  1.35678898, -4.73091212, -5.38426411,  4.7973178 ,\n",
       "          7.63861437, -4.44745966,  2.87892601]), 0.5503059740066528],\n",
       " [155, array([ 4.43022084,  1.86205356, -3.20106083, -3.28884831,  3.9820757 ,\n",
       "          8.45073633, -5.17889994, -0.4363466 ]), 0.5307364416122436],\n",
       " [156, array([ 2.87329354,  2.64444414, -3.44977414, -2.87218803,  2.39001569,\n",
       "          3.69531353, -0.85626327,  2.57116658]), 0.5604185409545899],\n",
       " [157, array([ 7.30200694,  3.90689287, -2.73912666, -2.90524282,  2.09711482,\n",
       "          5.17107613, -3.52187465,  3.64218202]), 0.5332356929779053],\n",
       " [158, array([ 7.63531957,  3.06378625, -3.32130193, -5.30144064,  2.18858604,\n",
       "          7.55546672, -4.83203959,  1.13128425]), 0.5019898436069489],\n",
       " [159, array([ 6.5401776 ,  0.81038815, -6.00067964, -3.27359745,  3.13631484,\n",
       "          5.04452022, -2.37741918,  0.91331757]), 0.5543006887435913],\n",
       " [160, array([ 6.45461428,  2.64530163, -1.39654664, -5.07142905,  7.19119428,\n",
       "          5.6986273 , -5.04365443,  3.60086775]), 0.5281991305351257],\n",
       " [161, array([ 7.67529805,  1.16924067, -3.63040356, -5.09536287,  4.53254312,\n",
       "          6.87307463, -4.09089761,  1.47745163]), 0.567327006816864],\n",
       " [162, array([ 4.85298258,  3.95362703, -0.62582727, -4.01055894,  1.80309999,\n",
       "          6.18550965, -4.39793854,  1.12524615]), 0.5278060567378998],\n",
       " [163, array([ 5.45957549,  0.27378493, -4.01512635, -5.12409822,  4.40574862,\n",
       "          2.07308826, -1.01770226,  4.37713008]), 0.5198831198215484],\n",
       " [164, array([ 7.69795358,  4.40793199,  1.017756  , -3.71198663,  5.42015402,\n",
       "          8.03197099, -3.50610676, -0.14985255]), 0.6296106278896332],\n",
       " [165, array([ 7.43772005,  2.91414885, -4.81098615, -3.95942302,  6.23143508,\n",
       "          4.65409898, -3.39479922,  3.5834368 ]), 0.5287870609760285],\n",
       " [166, array([ 5.70304827,  0.90250407, -6.33163785, -4.47332536,  4.06477968,\n",
       "          7.02878138, -3.28518574,  3.28160134]), 0.5284457480907441],\n",
       " [167, array([ 6.48952042,  2.76506989, -5.60806277, -5.11120112,  3.43547454,\n",
       "          7.25312271, -3.13632575,  2.29581188]), 0.539597464799881],\n",
       " [168, array([ 4.32272796,  1.20985683, -4.29957281, -3.79019819,  6.5631043 ,\n",
       "          5.67540427, -1.3348874 ,  1.38430546]), 0.5522155499458313],\n",
       " [169, array([ 8.17666578,  1.61498417, -3.39652913, -3.56894649,  4.06474953,\n",
       "          5.49219068, -2.72870468,  3.69407624]), 0.5377210083007813],\n",
       " [170, array([ 2.49317551,  1.44325126, -2.38437855, -3.0678647 ,  4.69413879,\n",
       "          1.8410261 , -1.90746   ,  3.3600687 ]), 0.5411638760566712],\n",
       " [171, array([ 6.90915168,  0.14897811, -6.31831793, -5.62393067,  3.7525984 ,\n",
       "          7.21967613, -2.77759656,  6.21905756]), 0.5246601691246032],\n",
       " [172, array([ 3.56234719,  0.39931298, -3.43506867, -5.44808901,  2.03949342,\n",
       "          4.22197791, -1.29592159,  2.56684111]), 0.5486837885379792],\n",
       " [173, array([ 9.85287609,  3.17761753, -4.61012488, -9.48011468,  5.370713  ,\n",
       "          6.64945484, -3.46548206,  4.84346146]), 0.5455070803165436],\n",
       " [174, array([ 5.07456036,  0.64084752, -1.91914974, -5.43384245,  1.72107825,\n",
       "          1.88366639, -0.82955412,  3.46519729]), 0.4945677134990692],\n",
       " [175, array([ 4.97691655,  1.53000135, -1.24577249, -3.63231047,  2.83400255,\n",
       "          3.724955  , -2.80109671,  3.94765095]), 0.5068891344070434],\n",
       " [176, array([ 7.7030199 ,  2.0959042 , -3.93509818, -4.22315797,  4.39004033,\n",
       "          5.9116896 , -4.93777768,  4.98069672]), 0.5484406862258911],\n",
       " [177, array([ 4.60339123,  0.34196919, -3.54416171, -5.21205546,  4.86737796,\n",
       "          3.5810645 , -3.47078721,  4.18750172]), 0.5541190948486329],\n",
       " [178, array([ 4.91186916,  1.9440854 , -2.65116201, -5.88521326,  3.56519816,\n",
       "          2.5846813 , -3.05499773,  2.8246849 ]), 0.5180218966007233],\n",
       " [179, array([ 5.5528071 ,  1.89631837, -2.02401336, -4.18691985,  5.69920928,\n",
       "          4.97618145, -2.4645254 ,  5.40494646]), 0.5657119402885437],\n",
       " [180, array([ 5.60166172,  0.08429249, -3.42933431, -7.06068004,  1.61819042,\n",
       "          1.96071149, -1.59996611,  5.85681436]), 0.5566981158256531],\n",
       " [181, array([ 5.95497022,  2.00816398, -3.06068659, -3.06075121,  2.42037431,\n",
       "          3.16910866, -2.92387292,  3.5179394 ]), 0.5174951248168945],\n",
       " [182, array([ 5.34139252,  0.86024142, -2.63324391, -5.2661629 ,  2.79230759,\n",
       "          2.98429693, -4.054296  ,  3.12108984]), 0.5396591267585754],\n",
       " [183, array([ 4.92713249,  0.82752109, -2.7405451 , -5.24479749,  0.40718579,\n",
       "          2.91270027, -0.41694733,  4.11867846]), 0.5342920665740967],\n",
       " [184, array([ 6.39867883,  2.58125064, -1.69872641, -3.65415153,  3.69097439,\n",
       "          1.93256032, -2.98366818,  3.64913762]), 0.5417504119873047],\n",
       " [185, array([ 6.67462477,  2.80616601, -0.47143705, -6.48975936,  4.33843062,\n",
       "          3.49649115, -4.0434844 ,  3.48222061]), 0.5019793183803558],\n",
       " [186, array([ 5.95529548,  1.02620746, -3.18647693, -2.75293442,  3.73597475,\n",
       "          3.45095617, -3.87165798,  5.60506185]), 0.538487253189087],\n",
       " [187, array([ 4.4846134 ,  1.65229991,  1.18280373, -7.27461065,  1.81745058,\n",
       "          0.42377543, -1.7084701 ,  1.37390638]), 0.6176333439350128],\n",
       " [188, array([ 4.68713575,  0.46090763, -2.15551815, -4.60812183,  4.27484765,\n",
       "          3.61624463, -2.32598541,  5.38591852]), 0.5309481663703919],\n",
       " [189, array([ 3.52524308, -1.49734195,  0.11134451, -2.51256112,  0.9752618 ,\n",
       "         -0.9096499 ,  1.28478682,  2.49431832]), 1.0385424294471741],\n",
       " [190, array([ 5.06240774,  3.72857723,  0.97665849, -5.92371261,  4.01421968,\n",
       "          3.59496782, -1.64649076,  3.3175075 ]), 0.6309955279827117],\n",
       " [191, array([ 4.33465549,  1.3376271 , -0.77379629, -4.95931266,  2.72854096,\n",
       "          2.35622727, -1.44338332,  2.57216143]), 0.5342959895133972],\n",
       " [192, array([ 4.9921811 ,  2.14192848, -0.36432995, -6.80693803,  2.32201948,\n",
       "          1.13911899, -2.28769332,  0.86187094]), 0.5242195010185242],\n",
       " [193, array([ 7.34034274,  3.21086759, -2.05974529, -4.85736641,  2.61024909,\n",
       "          4.18018935, -4.39876875,  3.83813794]), 0.5373608841896057],\n",
       " [194, array([ 5.78950683,  2.34809876, -1.19238558, -6.70007616,  3.81864163,\n",
       "          7.33676508, -2.82522192,  4.61072833]), 0.5511936802864075],\n",
       " [195, array([ 5.80706103,  0.0808022 , -1.29529207, -2.7238266 ,  1.98591818,\n",
       "          2.87513409, -2.5218143 ,  3.34025033]), 0.5629953060150147],\n",
       " [196, array([ 5.75516801,  2.30072522, -1.83406942, -6.65889794,  1.56011127,\n",
       "          2.50924351, -1.70827243,  2.50604581]), 0.5478662481307983],\n",
       " [197, array([ 6.36845616,  1.18061895, -1.61783541, -4.99897075,  3.09946975,\n",
       "          4.71796088, -4.41732148,  1.91221443]), 0.5300519208908081],\n",
       " [198, array([ 7.52298321,  1.99819583, -1.3754492 , -4.78126476,  1.33268642,\n",
       "          1.19746685, -4.13178533,  4.76090936]), 0.5487470483779907],\n",
       " [199, array([ 7.56139581,  3.10932663, -0.35028048, -4.90760605,  1.96176792,\n",
       "          4.16172629, -2.9927492 ,  2.61556096]), 0.5397809641361236]]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scatter = []\n",
    "y_scatter = []\n",
    "s_scatter = []\n",
    "ep_scatter = []\n",
    "for h in history:\n",
    "    l = []\n",
    "    for p in range(len(h[1])):\n",
    "        l.append(h[1][p])\n",
    "    x_scatter.append(l)\n",
    "    y_scatter.append(h[1][5])\n",
    "    s_scatter.append(0.3*(1/h[2]**10))\n",
    "    ep_scatter.append(0.4*h[0]/len(history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.09269267946073871,\n",
       "  0.5532662142692734,\n",
       "  -0.42939343536016694,\n",
       "  1.0516466577385795,\n",
       "  0.5176313725747432,\n",
       "  0.6449380740423029,\n",
       "  -0.9906440976031698,\n",
       "  1.58133417767476],\n",
       " [0.21298083746166782,\n",
       "  -0.6447339494582586,\n",
       "  -0.4138727386709791,\n",
       "  -2.0270610312407817,\n",
       "  -0.05518717979734703,\n",
       "  0.12452323180356045,\n",
       "  0.9224712933888561,\n",
       "  1.04696676848476],\n",
       " [0.289657493702662,\n",
       "  0.15380108552612265,\n",
       "  0.024143811145826026,\n",
       "  -0.2187181372204931,\n",
       "  0.5336927850386093,\n",
       "  0.06990533397699725,\n",
       "  -0.28987531851597537,\n",
       "  0.15773436503239138],\n",
       " [0.6238551409710681,\n",
       "  -1.0949552622164933,\n",
       "  -0.3533122285847599,\n",
       "  0.24412189773722917,\n",
       "  1.1761299741254005,\n",
       "  -0.3773879727337709,\n",
       "  -0.5728483414978918,\n",
       "  -0.8796861873731107],\n",
       " [0.5244142018443648,\n",
       "  0.9192817697552481,\n",
       "  1.1284466943837266,\n",
       "  0.21487503135238864,\n",
       "  -0.36481139184093436,\n",
       "  0.5584278544170115,\n",
       "  -1.1782111779247255,\n",
       "  -0.11893813354032053],\n",
       " [1.8105303132033974,\n",
       "  1.1343054679642883,\n",
       "  0.47886338978049714,\n",
       "  0.7352508578046947,\n",
       "  0.4721118589724134,\n",
       "  -0.4075595251053066,\n",
       "  0.5764633122383577,\n",
       "  -0.09720460635044534],\n",
       " [0.2749074723816072,\n",
       "  0.017452954402429194,\n",
       "  -0.24211932943673145,\n",
       "  -0.8499885971292487,\n",
       "  1.0769101405441548,\n",
       "  -1.1138982897616383,\n",
       "  0.9590523714606698,\n",
       "  -0.8873026478048552],\n",
       " [0.637275443648919,\n",
       "  1.0175647031240203,\n",
       "  -1.6024388070136284,\n",
       "  -0.40318969487616724,\n",
       "  0.9884443604061244,\n",
       "  -1.1665998693054753,\n",
       "  -0.5066126265112194,\n",
       "  -0.6652857185464015],\n",
       " [-1.080410206050755,\n",
       "  0.21922919251686224,\n",
       "  0.3030107727577299,\n",
       "  -0.252276923660186,\n",
       "  -0.6832679008467415,\n",
       "  0.08022235134288508,\n",
       "  2.8090213113107807,\n",
       "  0.3573718410574481],\n",
       " [0.8163964233570059,\n",
       "  -1.2056726919292993,\n",
       "  1.860478737349515,\n",
       "  0.2737840305169441,\n",
       "  -0.05203722750472792,\n",
       "  -0.32071350149854916,\n",
       "  -0.8730779708379492,\n",
       "  -1.3128109251930087],\n",
       " [1.1420278686764231,\n",
       "  -1.0872960520854784,\n",
       "  0.683454145579237,\n",
       "  -0.13565247493632804,\n",
       "  -0.29477201307409395,\n",
       "  1.9386895994934326,\n",
       "  0.8860991633147308,\n",
       "  -0.0546206920173089],\n",
       " [1.2553954303301202,\n",
       "  -0.4045231129236022,\n",
       "  -1.1803032284344288,\n",
       "  -0.3919393332506067,\n",
       "  -1.6795229209153697,\n",
       "  -0.9887345140022628,\n",
       "  1.9773025146015244,\n",
       "  1.0813466333705477],\n",
       " [1.1257246478537533,\n",
       "  0.019757090377142683,\n",
       "  -0.3236222072648387,\n",
       "  -1.7504645741146723,\n",
       "  -0.6778613108972159,\n",
       "  -0.8315508882082077,\n",
       "  -1.0194136551334705,\n",
       "  1.4317466977783062],\n",
       " [1.4500414121074185,\n",
       "  -0.8572608917967406,\n",
       "  -2.865748687242869,\n",
       "  -0.15317515999267117,\n",
       "  0.5710982968780172,\n",
       "  -0.04987152957914134,\n",
       "  0.6069571616135938,\n",
       "  1.1230574017144208],\n",
       " [0.41635032404960615,\n",
       "  -0.8562378610381112,\n",
       "  0.6710395302410188,\n",
       "  -2.6689262199042094,\n",
       "  -0.16111222381918955,\n",
       "  0.3292323653370186,\n",
       "  0.5627988145440378,\n",
       "  0.04423179624707313],\n",
       " [1.1660577669872296,\n",
       "  -0.5706412015797278,\n",
       "  -0.6476799935814583,\n",
       "  0.09336188030166603,\n",
       "  0.27771034374698045,\n",
       "  -0.41687396061155607,\n",
       "  0.8379907412220191,\n",
       "  0.753413129007451],\n",
       " [0.7786505793942091,\n",
       "  -0.42174239409067216,\n",
       "  -1.055091848184826,\n",
       "  -3.0935620706097016,\n",
       "  -0.36655368026652857,\n",
       "  -0.9491928931788478,\n",
       "  1.2221530541590726,\n",
       "  -0.09401579998655613],\n",
       " [-0.009595738873146353,\n",
       "  -1.1239919657818287,\n",
       "  -1.1699849452987479,\n",
       "  -2.3133094387115927,\n",
       "  0.32455165648898643,\n",
       "  0.48692677432271797,\n",
       "  -0.2144929653932442,\n",
       "  0.23390756624589376],\n",
       " [-0.9942252207366379,\n",
       "  0.9268108823463107,\n",
       "  -1.541467902169875,\n",
       "  -3.1033717907048644,\n",
       "  2.0019027087760195,\n",
       "  1.4939342457629414,\n",
       "  -0.45669605269954167,\n",
       "  -0.9654159305824137],\n",
       " [0.3533389323564079,\n",
       "  1.2123850260849303,\n",
       "  0.37160002314554363,\n",
       "  -3.2653876400783526,\n",
       "  0.4271357318738942,\n",
       "  -0.8399741274612407,\n",
       "  0.2200620301450643,\n",
       "  0.45719713116913174],\n",
       " [0.5174874649996801,\n",
       "  0.42089781813062216,\n",
       "  0.17119110205564422,\n",
       "  -3.206645004174124,\n",
       "  -1.0148313757980274,\n",
       "  1.1179985613888395,\n",
       "  -1.2670394244758363,\n",
       "  0.6594474045879153],\n",
       " [1.6055890064955012,\n",
       "  0.25329283325960433,\n",
       "  1.66673181968902,\n",
       "  -2.8552139733889432,\n",
       "  -0.8088608695279584,\n",
       "  0.6894826364388364,\n",
       "  -0.4815375670772651,\n",
       "  0.4874700571465458],\n",
       " [0.008083208124689989,\n",
       "  -0.7266290328612268,\n",
       "  0.4699484696768367,\n",
       "  -1.652398963217973,\n",
       "  2.887873510858294,\n",
       "  -0.42328406263561114,\n",
       "  -1.3475932886310524,\n",
       "  0.5427712448050255],\n",
       " [0.7565879975045913,\n",
       "  0.5454380763553968,\n",
       "  -0.7442775743290054,\n",
       "  -2.843319632465025,\n",
       "  0.3561457034784664,\n",
       "  -0.49942684432283346,\n",
       "  0.2695995490315282,\n",
       "  0.5021565204412948],\n",
       " [-0.5247425123829806,\n",
       "  -0.7897280639999531,\n",
       "  -0.5724037321057522,\n",
       "  -2.6306321508898556,\n",
       "  -0.5750553589373364,\n",
       "  0.42784323805781144,\n",
       "  -0.8915617581746256,\n",
       "  -0.5525975286648479],\n",
       " [-0.10328472004077682,\n",
       "  0.1660143596592374,\n",
       "  -2.0949998489265917,\n",
       "  -2.7525474528970895,\n",
       "  -1.6324297079267984,\n",
       "  0.3904910240303531,\n",
       "  0.35930636218851725,\n",
       "  -0.6865958177298923],\n",
       " [-0.29076792769483983,\n",
       "  -0.17088875229076508,\n",
       "  0.19143454306357885,\n",
       "  -1.8254595608819946,\n",
       "  0.6044056565321128,\n",
       "  -0.3640707035423839,\n",
       "  0.669000272234588,\n",
       "  -0.09965014338884054],\n",
       " [2.3150698957050375,\n",
       "  -0.3483185100656213,\n",
       "  -2.4433329680573017,\n",
       "  -3.0815536189527233,\n",
       "  -1.6295597887692335,\n",
       "  0.33947828734820895,\n",
       "  0.20968822388560623,\n",
       "  -0.8318294452069228],\n",
       " [0.315748487163506,\n",
       "  0.43835418956079947,\n",
       "  -0.32729899301361254,\n",
       "  -2.566520783391857,\n",
       "  -0.12241499637878528,\n",
       "  0.803728809185031,\n",
       "  0.7477287992166493,\n",
       "  -0.2974664745402224],\n",
       " [-0.424365714638108,\n",
       "  -1.3161739437645497,\n",
       "  -0.5738212166366071,\n",
       "  -0.813356544447942,\n",
       "  -0.7505335454585293,\n",
       "  1.9224292916426997,\n",
       "  -0.9931250248598213,\n",
       "  0.6694638746408892],\n",
       " [0.824625595736973,\n",
       "  0.02086909657677885,\n",
       "  0.19890615339776868,\n",
       "  -1.3277799893646776,\n",
       "  1.3058642258844069,\n",
       "  -0.07789131907142988,\n",
       "  -0.015812694609223216,\n",
       "  -2.0520443504367005],\n",
       " [0.4570800292501488,\n",
       "  -1.0755039911371762,\n",
       "  0.6620469646553653,\n",
       "  -3.660221828892563,\n",
       "  1.1111030665349104,\n",
       "  0.758897260086934,\n",
       "  0.37287867585622714,\n",
       "  1.0893793417212612],\n",
       " [0.3903045422418717,\n",
       "  -0.14143267730331166,\n",
       "  2.0993586993232483,\n",
       "  -3.275508663432136,\n",
       "  -1.8350701505011189,\n",
       "  0.47238355140419336,\n",
       "  -1.280496579823097,\n",
       "  0.18961969436533177],\n",
       " [1.5654150928407158,\n",
       "  0.5328453548440917,\n",
       "  -0.21536316791827947,\n",
       "  -3.5420842332442417,\n",
       "  0.538111536812107,\n",
       "  -0.04787193377796184,\n",
       "  -0.9235255000686895,\n",
       "  1.3315431624759586],\n",
       " [1.5323533982700401,\n",
       "  -0.6281568305300069,\n",
       "  1.8619849368280448,\n",
       "  -4.21405033875907,\n",
       "  -0.4280927542821648,\n",
       "  0.9998028265579955,\n",
       "  -0.10817759931779425,\n",
       "  -0.6258823421754118],\n",
       " [-0.04174474044143994,\n",
       "  0.8743337998365408,\n",
       "  1.1893448898814802,\n",
       "  -2.157030336476648,\n",
       "  0.6151142481264453,\n",
       "  0.37980877440573246,\n",
       "  -0.8475774358000605,\n",
       "  0.9798727394728434],\n",
       " [2.4603474254917974,\n",
       "  1.1477190031627664,\n",
       "  1.424412638076846,\n",
       "  -3.8642442528833145,\n",
       "  0.2733667102105435,\n",
       "  1.0529986298767968,\n",
       "  0.985330700751964,\n",
       "  0.5695461134556101],\n",
       " [1.5103045050411559,\n",
       "  0.3774422425016982,\n",
       "  -0.48149611023673744,\n",
       "  -2.1527890740034072,\n",
       "  0.19527301979717848,\n",
       "  -0.41102538285536594,\n",
       "  -0.7941862516094019,\n",
       "  0.2377093417558786],\n",
       " [1.0242650581060777,\n",
       "  1.5233564395000823,\n",
       "  1.7833395127368983,\n",
       "  -2.2317081775481675,\n",
       "  1.54234207073868,\n",
       "  0.8066450567010497,\n",
       "  0.4026685432571403,\n",
       "  0.775680574662031],\n",
       " [1.1180607569980663,\n",
       "  -0.36006367624571284,\n",
       "  1.3707375097058856,\n",
       "  -3.7702313369824143,\n",
       "  -0.8715675792353867,\n",
       "  1.19356661665217,\n",
       "  0.7464443279649561,\n",
       "  1.728505464005096],\n",
       " [-0.10759983807399909,\n",
       "  1.0162685000807055,\n",
       "  0.4259172530520964,\n",
       "  -2.0569394370025487,\n",
       "  -0.19694954560837397,\n",
       "  0.15723130776335414,\n",
       "  -0.47904392799779977,\n",
       "  0.7762403160625629],\n",
       " [0.4902996539136335,\n",
       "  0.2432523604052418,\n",
       "  0.12053956148681833,\n",
       "  -2.8232474263206626,\n",
       "  0.6236537368787086,\n",
       "  1.080381845534368,\n",
       "  -1.6268178308302392,\n",
       "  1.9172454839870339],\n",
       " [2.0499699543679624,\n",
       "  0.11465616149068686,\n",
       "  -0.02806642391027525,\n",
       "  -1.7678941826997603,\n",
       "  -1.094168800869752,\n",
       "  2.2277355373698464,\n",
       "  -0.9497101929114649,\n",
       "  -0.1270557156322859],\n",
       " [1.9794455732951555,\n",
       "  0.2878675864034571,\n",
       "  0.10647441860378032,\n",
       "  -2.7366672124167986,\n",
       "  0.7605584993187718,\n",
       "  -1.0431194355586153,\n",
       "  -1.625908624867676,\n",
       "  0.9210403988165086],\n",
       " [-0.026821569759496455,\n",
       "  0.28515549319620326,\n",
       "  0.6164435875385409,\n",
       "  -3.843335654173182,\n",
       "  -1.7502425894859446,\n",
       "  0.437611667570563,\n",
       "  -1.0682622234452133,\n",
       "  -0.459620816602862],\n",
       " [0.13609652087268653,\n",
       "  2.1140683661610185,\n",
       "  0.6692621962225156,\n",
       "  -3.7495014851428525,\n",
       "  -0.05961124708837487,\n",
       "  1.3706668720014585,\n",
       "  -1.8492883302067813,\n",
       "  0.3732565183747727],\n",
       " [0.994850245031928,\n",
       "  1.1659304288953058,\n",
       "  1.406863998597952,\n",
       "  -4.656601558318297,\n",
       "  -1.530047548842692,\n",
       "  1.688139639343346,\n",
       "  -1.483845986326008,\n",
       "  0.19046278086556595],\n",
       " [1.7622020883127079,\n",
       "  0.5603450358761174,\n",
       "  0.6977090361401048,\n",
       "  -5.420138263514015,\n",
       "  -0.8578463280573521,\n",
       "  -0.22186463761521222,\n",
       "  -0.9289030326985773,\n",
       "  1.1930017154913077],\n",
       " [2.3703746478407064,\n",
       "  1.1300716456782376,\n",
       "  1.0789768225100955,\n",
       "  -2.8735586026311255,\n",
       "  1.761270648941493,\n",
       "  0.40250536935762,\n",
       "  1.1137782889753003,\n",
       "  0.5282161190949302],\n",
       " [1.2094649475442145,\n",
       "  -1.2095487419617488,\n",
       "  -0.5687983535140877,\n",
       "  -3.553263393019878,\n",
       "  -0.10211046042375252,\n",
       "  -0.4087137589364076,\n",
       "  -1.2159860765211925,\n",
       "  0.9872026459212678],\n",
       " [3.187303862222408,\n",
       "  -1.092425732029689,\n",
       "  1.291952626904366,\n",
       "  -2.723956117413336,\n",
       "  -0.3110012129409231,\n",
       "  0.862821392255795,\n",
       "  -2.543926239374681,\n",
       "  0.7091706296865439],\n",
       " [2.079695078486993,\n",
       "  1.7360188486359969,\n",
       "  0.955139755468178,\n",
       "  -3.887441389047912,\n",
       "  -1.6344461173415632,\n",
       "  0.2501613010275925,\n",
       "  -1.5034233083380912,\n",
       "  -0.08258337037482827],\n",
       " [0.8426534497912057,\n",
       "  -0.4908902488856694,\n",
       "  0.05838559050278727,\n",
       "  -2.690601342991536,\n",
       "  0.4907599519130834,\n",
       "  3.0726314360910427,\n",
       "  -1.185143160954457,\n",
       "  -0.4656347460702082],\n",
       " [2.7842125350004174,\n",
       "  0.8642370743427465,\n",
       "  0.10673326849015047,\n",
       "  -1.5943598087978188,\n",
       "  -0.15959444802151213,\n",
       "  2.5818952724331092,\n",
       "  -1.3239211064042267,\n",
       "  0.5520105152092152],\n",
       " [0.30782287640805706,\n",
       "  -1.207831406959461,\n",
       "  1.8322362322518235,\n",
       "  -2.228256219175158,\n",
       "  -0.8986413668144085,\n",
       "  0.3143950528024242,\n",
       "  -1.1049731324919576,\n",
       "  -0.5983323193957499],\n",
       " [4.415635617506107,\n",
       "  1.4733214474922938,\n",
       "  0.0376588520641184,\n",
       "  -3.4032347225815958,\n",
       "  -1.9724630207711316,\n",
       "  2.096293751035325,\n",
       "  -2.3302347396343386,\n",
       "  -0.8862181620449585],\n",
       " [0.9269524174219175,\n",
       "  -0.5795152348987453,\n",
       "  0.10396251449976882,\n",
       "  -3.754162542970673,\n",
       "  -1.6488669804610687,\n",
       "  0.3723441150372524,\n",
       "  -0.5616298382441053,\n",
       "  -0.14372717420786918],\n",
       " [1.5555076020729637,\n",
       "  -0.3757612246925918,\n",
       "  1.3692428174630535,\n",
       "  -3.273399048829045,\n",
       "  -1.1077702378006309,\n",
       "  2.935184917144264,\n",
       "  -1.5767656006897717,\n",
       "  -0.16022213787436618],\n",
       " [0.17564267499877584,\n",
       "  -1.111123438099929,\n",
       "  0.8273552485565934,\n",
       "  -2.8967252186090793,\n",
       "  -0.7417146906410091,\n",
       "  0.7517463214139495,\n",
       "  -0.15699057144010786,\n",
       "  1.3972602547696051],\n",
       " [2.929275443213683,\n",
       "  1.3994634282521834,\n",
       "  1.2683107209448758,\n",
       "  -1.1650114005182388,\n",
       "  -1.0342713583494292,\n",
       "  0.46308559064931076,\n",
       "  -1.6130181301318187,\n",
       "  1.288554254241675],\n",
       " [3.5440187985399776,\n",
       "  0.8454373403102253,\n",
       "  0.8508537994744445,\n",
       "  -1.2918181478133959,\n",
       "  -0.8319563261868003,\n",
       "  -0.8749371600130991,\n",
       "  -0.8517526659899638,\n",
       "  0.4718377067717728],\n",
       " [3.8916114331491127,\n",
       "  0.2596662937550639,\n",
       "  0.8226655239275064,\n",
       "  -0.49297637063108257,\n",
       "  -1.805959118977225,\n",
       "  1.8121006215273103,\n",
       "  -1.2864091548142518,\n",
       "  0.08603521425494898],\n",
       " [2.843632198983604,\n",
       "  2.2460719907970828,\n",
       "  1.9306271244418434,\n",
       "  -3.3852875771017077,\n",
       "  -1.9947868097598296,\n",
       "  -0.5084913814852927,\n",
       "  -2.9902381465522683,\n",
       "  1.2030716395922325],\n",
       " [4.8041430678822294,\n",
       "  0.9978093700624088,\n",
       "  1.4642575984555561,\n",
       "  -1.869674604536829,\n",
       "  -2.739432485807086,\n",
       "  0.4027641085575834,\n",
       "  -1.5696673857692691,\n",
       "  -0.0881968010316383],\n",
       " [1.8120188972152678,\n",
       "  1.0012178374116916,\n",
       "  2.4391220701789655,\n",
       "  -3.3826447193070033,\n",
       "  -2.4336400482326033,\n",
       "  0.6023119059240738,\n",
       "  -1.4950172886206534,\n",
       "  -0.2645426924960176],\n",
       " [1.7268065169972573,\n",
       "  0.7639091340185767,\n",
       "  0.9400446854096929,\n",
       "  -1.611247430963444,\n",
       "  -1.2485265188752404,\n",
       "  2.6468428623593923,\n",
       "  -2.2164747206188884,\n",
       "  1.6926129454029755],\n",
       " [3.6707408310694154,\n",
       "  2.2521796189154486,\n",
       "  0.3859862668225036,\n",
       "  -2.259178547068264,\n",
       "  -2.6061227219805527,\n",
       "  1.0145627693420625,\n",
       "  -1.252915521428612,\n",
       "  -0.7201784392448635],\n",
       " [3.326916189081025,\n",
       "  1.5373398231090742,\n",
       "  -0.3080980710136322,\n",
       "  -2.7746414550294265,\n",
       "  -1.4795447794408807,\n",
       "  0.836212511953908,\n",
       "  -0.039128566970859646,\n",
       "  1.2934957150014907],\n",
       " [3.8222207318150905,\n",
       "  0.7211192964191361,\n",
       "  0.8752906162373109,\n",
       "  -1.1533280235052483,\n",
       "  -1.0548754456574645,\n",
       "  0.21822655402190594,\n",
       "  -3.017083197445824,\n",
       "  -0.05388049083816748],\n",
       " [2.265114921655356,\n",
       "  2.101135854428464,\n",
       "  0.47598134198732145,\n",
       "  -2.6814383932838703,\n",
       "  -2.989654598375994,\n",
       "  2.3459784705262896,\n",
       "  -3.3357611038445976,\n",
       "  1.3430619037943747],\n",
       " [1.4867884907292386,\n",
       "  2.9210585772213795,\n",
       "  0.9591006522676372,\n",
       "  -2.1174236937485427,\n",
       "  -3.5926089438915576,\n",
       "  1.4044432184447242,\n",
       "  -1.335033552462768,\n",
       "  2.1237520451382363],\n",
       " [2.3903757943891026,\n",
       "  2.281565742460263,\n",
       "  0.7527527425856088,\n",
       "  -2.8538996332874413,\n",
       "  -2.9620365151603267,\n",
       "  0.8686894243641206,\n",
       "  -3.250033335902817,\n",
       "  1.150567411837077],\n",
       " [4.277783082862225,\n",
       "  2.8070762378466894,\n",
       "  0.9472592330123173,\n",
       "  -2.8485476647678047,\n",
       "  -1.5827470414417837,\n",
       "  2.5556831468758245,\n",
       "  -1.3000822306149054,\n",
       "  -0.9609273365752456],\n",
       " [3.7787671110807777,\n",
       "  0.8778692693641913,\n",
       "  0.954712071098025,\n",
       "  -2.7214992680394334,\n",
       "  -1.7457149920014134,\n",
       "  2.5610389020637414,\n",
       "  -1.7786139997426282,\n",
       "  2.3072343645294797],\n",
       " [2.985074310410445,\n",
       "  2.948651912325976,\n",
       "  0.10535768839955573,\n",
       "  -2.2704295955858367,\n",
       "  -3.5278732601701748,\n",
       "  -0.07351971551870551,\n",
       "  -2.824670534626894,\n",
       "  1.2922590465597763],\n",
       " [0.24648744481341422,\n",
       "  2.0603185128212913,\n",
       "  1.5186554538742756,\n",
       "  -1.310606441661646,\n",
       "  -1.167342697005512,\n",
       "  0.4978796190054722,\n",
       "  -2.1584018560857783,\n",
       "  0.17951782786429138],\n",
       " [2.9744620238274395,\n",
       "  0.8560609521411546,\n",
       "  1.4837650248801686,\n",
       "  -3.12939815429952,\n",
       "  -2.3532621525861344,\n",
       "  0.9863420127094946,\n",
       "  -1.4560696552536982,\n",
       "  -0.42765786912322035],\n",
       " [2.3045641745658214,\n",
       "  2.724266147027201,\n",
       "  0.06057529128803607,\n",
       "  -1.2190679151348458,\n",
       "  -0.8843390436126368,\n",
       "  0.6055157987497424,\n",
       "  -2.254615939308523,\n",
       "  2.902853455733744],\n",
       " [3.786316674501041,\n",
       "  0.9542037824155508,\n",
       "  -2.0127618226297086,\n",
       "  -3.9006866944331104,\n",
       "  -0.5838024241386837,\n",
       "  2.635465940108194,\n",
       "  -2.086741664110068,\n",
       "  1.8720635426704288],\n",
       " [2.940211614003947,\n",
       "  1.1641612643226398,\n",
       "  0.5827015813360023,\n",
       "  -4.3216329388906125,\n",
       "  -2.6032402668647525,\n",
       "  -0.061810351102442596,\n",
       "  -2.083640021542931,\n",
       "  0.9444445575899337],\n",
       " [6.0997156800213475,\n",
       "  1.4738992000183126,\n",
       "  1.307429502451086,\n",
       "  -4.18720328244115,\n",
       "  -3.555338415278077,\n",
       "  1.9561526516706713,\n",
       "  -2.545644211600468,\n",
       "  2.2442170025579418],\n",
       " [2.0620663282218583,\n",
       "  0.19636901370568283,\n",
       "  -3.0898403718729437,\n",
       "  -2.9647273059705905,\n",
       "  -0.6783079112714928,\n",
       "  3.639252135680966,\n",
       "  -1.9722342090399128,\n",
       "  1.7188258170760227],\n",
       " [4.58581390436399,\n",
       "  0.7255686733613758,\n",
       "  -0.25643227798210266,\n",
       "  -3.3523342380150813,\n",
       "  -0.7119651438701047,\n",
       "  3.6691439006948663,\n",
       "  -3.3424143958272685,\n",
       "  1.4225785160627218],\n",
       " [5.06636131179018,\n",
       "  1.7703675804549297,\n",
       "  -0.8244439250985032,\n",
       "  -3.3815377036050744,\n",
       "  -0.08502776197250861,\n",
       "  0.5343252440024968,\n",
       "  -3.4061895441271783,\n",
       "  3.1352338641736166],\n",
       " [4.106836763458559,\n",
       "  0.6003123910319679,\n",
       "  -2.0689415547330445,\n",
       "  -5.015999248025794,\n",
       "  -1.1358686406962433,\n",
       "  3.935569892131343,\n",
       "  -1.5333478697108913,\n",
       "  2.0575157309112067],\n",
       " [2.3571611014787246,\n",
       "  0.7111476693217209,\n",
       "  0.12503534963918927,\n",
       "  -3.2053422263955835,\n",
       "  -2.4465065896496134,\n",
       "  1.3841851919160315,\n",
       "  -1.9447764385208044,\n",
       "  2.500019929797657],\n",
       " [3.2014277044242694,\n",
       "  2.0622687785677014,\n",
       "  -0.2825890051227195,\n",
       "  -4.493733719141838,\n",
       "  -3.1532992819344603,\n",
       "  3.8187887500215827,\n",
       "  -4.043137021733353,\n",
       "  -0.7029474539802794],\n",
       " [3.7257872827944554,\n",
       "  0.9089213875323656,\n",
       "  -2.382588003214628,\n",
       "  -5.586441630607729,\n",
       "  -0.7757745374771954,\n",
       "  4.532341021754032,\n",
       "  -0.0623438142935846,\n",
       "  1.8423115305255129],\n",
       " [3.1920055256251354,\n",
       "  -1.5149527651696864,\n",
       "  0.5395640565775819,\n",
       "  -3.8711368063240266,\n",
       "  -0.0893337321449359,\n",
       "  1.9047426430148735,\n",
       "  -1.860775257836809,\n",
       "  1.2019083636808283],\n",
       " [4.1733621975931365,\n",
       "  3.09852371724609,\n",
       "  -1.3261511380463844,\n",
       "  -3.8663767196383967,\n",
       "  -1.845287372987877,\n",
       "  4.780840566920221,\n",
       "  -1.3209895329297425,\n",
       "  -0.6308939797168691],\n",
       " [4.717350182540169,\n",
       "  0.7857795450547614,\n",
       "  -3.0137697469866085,\n",
       "  -5.827771855943347,\n",
       "  -1.4801786111708237,\n",
       "  4.257780906800626,\n",
       "  -1.1539898979609462,\n",
       "  3.597238014818932],\n",
       " [2.0921541214757404,\n",
       "  -0.036970298766441756,\n",
       "  -1.07877317619713,\n",
       "  -1.6078878755353436,\n",
       "  2.1632195123012865,\n",
       "  4.61861998256771,\n",
       "  -2.728644898304906,\n",
       "  2.7529333232184987],\n",
       " [4.79326063839647,\n",
       "  1.4518886338519899,\n",
       "  -1.7243064361281875,\n",
       "  -3.060551596428129,\n",
       "  -1.1518224351622555,\n",
       "  1.4079055935009643,\n",
       "  -2.0317232138129837,\n",
       "  1.9558506697972273],\n",
       " [1.6152210681501817,\n",
       "  -0.7003062732445068,\n",
       "  -0.06818432652188111,\n",
       "  -0.8430986194390209,\n",
       "  -0.6530556326354668,\n",
       "  5.046147483892509,\n",
       "  -3.8621726544424058,\n",
       "  1.1355418890065585],\n",
       " [3.27723913379519,\n",
       "  0.5054194599002544,\n",
       "  -3.6058226236828497,\n",
       "  -0.507143442040495,\n",
       "  1.5780347084221134,\n",
       "  3.7125652119463615,\n",
       "  -1.936337266907321,\n",
       "  2.6772885805733426],\n",
       " [5.157446935565785,\n",
       "  1.3303907445180667,\n",
       "  -2.3411182973731735,\n",
       "  -2.343180498930217,\n",
       "  0.16843842759866734,\n",
       "  6.037134082114553,\n",
       "  -2.3264738936171616,\n",
       "  1.1598799886123863],\n",
       " [1.0227797471082627,\n",
       "  -1.5523530800277778,\n",
       "  -2.4454970351568464,\n",
       "  -6.21025639517414,\n",
       "  -1.9609708181292387,\n",
       "  3.628308512563254,\n",
       "  -0.04657044470706406,\n",
       "  2.2517805092899623],\n",
       " [2.2204946964997747,\n",
       "  -0.26125352058154516,\n",
       "  -2.5126743829721554,\n",
       "  -6.250669704869572,\n",
       "  -0.1365853484520605,\n",
       "  6.739801413791779,\n",
       "  -4.642859444585552,\n",
       "  2.123196800242285],\n",
       " [3.834074812710119,\n",
       "  -0.7024005399370864,\n",
       "  -2.4119520559603247,\n",
       "  -3.327290974898463,\n",
       "  -1.640679651027643,\n",
       "  3.626949321536721,\n",
       "  -1.487436215643375,\n",
       "  1.6336486614769368],\n",
       " [3.404256450015102,\n",
       "  -1.4135006027236985,\n",
       "  -3.4202165644851963,\n",
       "  -6.259355974743733,\n",
       "  0.30047694311973117,\n",
       "  3.287207763630735,\n",
       "  -0.8614438013670909,\n",
       "  2.211219879817296],\n",
       " [5.699847759439276,\n",
       "  1.1390809314838604,\n",
       "  -2.6189175363138264,\n",
       "  -1.8111546285108553,\n",
       "  2.697110486431878,\n",
       "  4.9349290018866405,\n",
       "  -2.975986648254346,\n",
       "  3.7451673037392843],\n",
       " [5.8847419173352815,\n",
       "  0.7554701832122195,\n",
       "  -2.648600645992793,\n",
       "  -1.3008255088583733,\n",
       "  2.1431036625257023,\n",
       "  3.8190106689940047,\n",
       "  -2.956031636130491,\n",
       "  4.14407191613535],\n",
       " [4.5961427933235575,\n",
       "  3.3235162257398736,\n",
       "  -1.6477453627589778,\n",
       "  -3.559564291005705,\n",
       "  -0.8220750033465445,\n",
       "  6.260981410140436,\n",
       "  -2.5124904805588346,\n",
       "  -0.18356209416096236],\n",
       " [6.219132484507005,\n",
       "  2.133897133067803,\n",
       "  -0.7808544453351658,\n",
       "  -1.7575479138826897,\n",
       "  2.81593603439436,\n",
       "  3.620218278624962,\n",
       "  -2.015431844716301,\n",
       "  1.8682563226976616],\n",
       " [6.4648560695483335,\n",
       "  1.7735522621576147,\n",
       "  -3.1063114337366184,\n",
       "  2.9903549521792128,\n",
       "  0.3720833669391154,\n",
       "  6.809369303554817,\n",
       "  -3.327673854026404,\n",
       "  2.9241248533547406],\n",
       " [4.973569615878658,\n",
       "  3.0678468261830134,\n",
       "  -0.08471379178817307,\n",
       "  1.2025829851320862,\n",
       "  0.9596736871093035,\n",
       "  3.9310572028968327,\n",
       "  -3.291503593783801,\n",
       "  2.8606331119331507],\n",
       " [2.215092397887238,\n",
       "  -0.3489560604256198,\n",
       "  -4.284824361903981,\n",
       "  -3.797232088574976,\n",
       "  0.9678718499176375,\n",
       "  5.478688971639739,\n",
       "  -3.2060858437770405,\n",
       "  3.414838701111305],\n",
       " [2.328070305431417,\n",
       "  0.7197792800936448,\n",
       "  -2.2703703474811014,\n",
       "  -3.3479345283644646,\n",
       "  1.7214687237365414,\n",
       "  5.245459123790624,\n",
       "  -2.7978263671984904,\n",
       "  3.420534148532105],\n",
       " [3.4244344411934238,\n",
       "  0.7654149048683384,\n",
       "  -1.5579047617258412,\n",
       "  -0.7862943608322541,\n",
       "  -2.2948883743826944,\n",
       "  2.6641450116341474,\n",
       "  -1.1036080333070206,\n",
       "  1.3388292864100289],\n",
       " [6.237195271080795,\n",
       "  0.8613236149605947,\n",
       "  -1.9092449126357554,\n",
       "  -4.4544800781548695,\n",
       "  -1.6400122880590962,\n",
       "  1.9289052927713612,\n",
       "  -0.5868159857560222,\n",
       "  2.5569298290196545],\n",
       " [6.530904599307355,\n",
       "  1.5515505049819287,\n",
       "  -0.8821676487529233,\n",
       "  0.534315049885858,\n",
       "  -1.02453569906696,\n",
       "  8.011139321720893,\n",
       "  -3.4618040867612643,\n",
       "  0.09076312525827568],\n",
       " [5.78265874268223,\n",
       "  2.1112100776360903,\n",
       "  -1.0547521527105848,\n",
       "  1.9407405849607642,\n",
       "  0.33163139106815565,\n",
       "  3.389178319187324,\n",
       "  -1.9877923515955098,\n",
       "  -1.1927732236816548],\n",
       " [6.234123628707105,\n",
       "  0.2952361038002398,\n",
       "  -6.604992072852693,\n",
       "  -4.179687609630035,\n",
       "  3.2629989183104744,\n",
       "  3.4314260208207257,\n",
       "  -2.940579360486683,\n",
       "  5.9553773152158],\n",
       " [5.615981968242921,\n",
       "  1.814233992636905,\n",
       "  -1.8806525092365567,\n",
       "  -4.925766263987148,\n",
       "  0.3882725813132799,\n",
       "  3.522511946879119,\n",
       "  -2.5439843695358864,\n",
       "  0.6464326701997722],\n",
       " [5.81002683793136,\n",
       "  0.21248402320505066,\n",
       "  -2.251516271309256,\n",
       "  -3.061820052615225,\n",
       "  -0.3323049200300483,\n",
       "  4.210621592303081,\n",
       "  -3.4803330342684995,\n",
       "  2.8486444748247224],\n",
       " [4.2521073239576195,\n",
       "  1.270612900354783,\n",
       "  -2.0520939268301595,\n",
       "  -0.7531019936226446,\n",
       "  1.7078560369836957,\n",
       "  3.7628015995080615,\n",
       "  -3.4098674473583435,\n",
       "  2.012306215359928],\n",
       " [3.056669623851874,\n",
       "  2.7868317190863703,\n",
       "  -1.9876255007991328,\n",
       "  -0.5310049047176935,\n",
       "  4.603027611578002,\n",
       "  3.969152169422922,\n",
       "  -2.853753220381129,\n",
       "  -0.30859594737241736],\n",
       " [5.332803254013243,\n",
       "  1.2231011968549925,\n",
       "  -0.9213901566276266,\n",
       "  -4.74035385925443,\n",
       "  3.9968359644893265,\n",
       "  5.604207460766999,\n",
       "  -2.9017428541667076,\n",
       "  4.387882554649952],\n",
       " [2.950801939028752,\n",
       "  -0.8276531716359772,\n",
       "  -5.85895763175166,\n",
       "  -4.820334379993054,\n",
       "  1.6098044954780677,\n",
       "  6.567511263995097,\n",
       "  -1.257418160057486,\n",
       "  3.9519115181850246],\n",
       " [10.072114799269565,\n",
       "  1.2489296084709502,\n",
       "  -2.4778456256748265,\n",
       "  -2.8264136777221407,\n",
       "  0.8497176796908397,\n",
       "  9.947745071482899,\n",
       "  -5.055886227230575,\n",
       "  4.132263663794106],\n",
       " [6.816475026685931,\n",
       "  -0.0761541758067964,\n",
       "  -4.592709265306247,\n",
       "  -4.416923016197088,\n",
       "  2.1670822929750213,\n",
       "  4.436203993560543,\n",
       "  -0.2501175611823063,\n",
       "  3.600659320314267],\n",
       " [6.404715301158039,\n",
       "  0.09517341561512382,\n",
       "  0.5879475682609314,\n",
       "  -4.6071996731319595,\n",
       "  -2.3691422506891016,\n",
       "  5.501977190454879,\n",
       "  -4.346675456634088,\n",
       "  2.8341275265563315],\n",
       " [2.2557464055093757,\n",
       "  -0.7699617038416857,\n",
       "  -5.373584816106243,\n",
       "  -4.8481570770459,\n",
       "  2.194575769339125,\n",
       "  4.9769224084084085,\n",
       "  -3.11257128752556,\n",
       "  2.632813948932048],\n",
       " [7.213533574231543,\n",
       "  1.2026668870812514,\n",
       "  -4.041253197019408,\n",
       "  -2.9548199394269035,\n",
       "  0.5965713295273416,\n",
       "  6.143190242498553,\n",
       "  -1.965031155646653,\n",
       "  0.7581795086989431],\n",
       " [4.747761193944445,\n",
       "  -1.0640526864328423,\n",
       "  -6.915595958205545,\n",
       "  -3.743381704428286,\n",
       "  3.0736636376542688,\n",
       "  3.606761804692674,\n",
       "  -6.21655549397616,\n",
       "  3.722197120570735],\n",
       " [8.636081883520799,\n",
       "  -1.2971794099276888,\n",
       "  -3.955569731151204,\n",
       "  -4.720192541885712,\n",
       "  2.727611732247013,\n",
       "  4.524196277748243,\n",
       "  -6.816673437165844,\n",
       "  4.201989865492014],\n",
       " [4.429608785981346,\n",
       "  -1.0220323804779932,\n",
       "  -6.497025997871011,\n",
       "  -1.2704944546417276,\n",
       "  5.432512154765783,\n",
       "  1.1379974120927634,\n",
       "  -2.712320437680072,\n",
       "  2.7882607669617445],\n",
       " [6.3658216212001575,\n",
       "  0.39449516189113504,\n",
       "  -3.248684786662624,\n",
       "  -5.319072888464132,\n",
       "  -1.182181838143707,\n",
       "  6.309393115708254,\n",
       "  -4.200207240146921,\n",
       "  1.7062631349892965],\n",
       " [3.1343731103464085,\n",
       "  -0.24181023816210878,\n",
       "  -4.375934473688107,\n",
       "  -1.0499756347980056,\n",
       "  3.602481650004651,\n",
       "  5.76909075913074,\n",
       "  -2.507329724640046,\n",
       "  2.155522681660839],\n",
       " [5.586270825048201,\n",
       "  0.5481831531071589,\n",
       "  -2.0913412676662197,\n",
       "  -4.895610725165264,\n",
       "  3.932447135612331,\n",
       "  6.606961380853052,\n",
       "  -3.169766086173579,\n",
       "  8.258701096652487],\n",
       " [4.057774555671543,\n",
       "  0.9903939258590719,\n",
       "  -5.844591389803802,\n",
       "  -4.815558950834728,\n",
       "  1.3850354541383862,\n",
       "  4.449234657177752,\n",
       "  -2.0069236866339693,\n",
       "  2.5164312022675768],\n",
       " [6.853429046509368,\n",
       "  -0.37044126296498425,\n",
       "  -3.6931782133254085,\n",
       "  -7.454060888225529,\n",
       "  2.257928535997258,\n",
       "  7.067678922667865,\n",
       "  -2.5460117439199226,\n",
       "  2.5450651752933395],\n",
       " [9.610869070110716,\n",
       "  1.8853035566986054,\n",
       "  -1.7563236435960212,\n",
       "  -6.822019978349856,\n",
       "  -0.7750013927413875,\n",
       "  6.406504586618736,\n",
       "  -5.348751787777253,\n",
       "  2.66649626704659],\n",
       " [3.963500833102007,\n",
       "  -1.2550273508773122,\n",
       "  0.4169426918906547,\n",
       "  -6.111105871005908,\n",
       "  -1.9345879613990948,\n",
       "  6.123792972029399,\n",
       "  -1.883705281931725,\n",
       "  1.7740894115946313],\n",
       " [6.087102425872064,\n",
       "  1.106497100145005,\n",
       "  -4.947985191411841,\n",
       "  -3.0555764066755513,\n",
       "  -0.11009849086746182,\n",
       "  9.243471203312188,\n",
       "  -3.3538256445364723,\n",
       "  2.915663003831552],\n",
       " [8.529738696011965,\n",
       "  -0.10813560158002411,\n",
       "  -1.394381739096945,\n",
       "  -6.44220053415601,\n",
       "  -2.2504945491096837,\n",
       "  6.206813402178475,\n",
       "  -4.721703834788108,\n",
       "  3.37953875459424],\n",
       " [7.171706810142348,\n",
       "  0.6592904346941643,\n",
       "  -1.742691249686724,\n",
       "  -4.847623453064457,\n",
       "  -2.7696149525926606,\n",
       "  4.642310647897153,\n",
       "  -5.252438733669914,\n",
       "  2.72000317246352],\n",
       " [8.028686946156219,\n",
       "  2.3884665866983683,\n",
       "  -4.9517251213053495,\n",
       "  -8.427156825072966,\n",
       "  0.9687107343755157,\n",
       "  7.741871565539114,\n",
       "  -3.7929727149164756,\n",
       "  1.2791494986542062],\n",
       " [8.676806304248817,\n",
       "  1.3683797378191807,\n",
       "  -2.8317654335093225,\n",
       "  -3.5302155589928352,\n",
       "  2.581318789779597,\n",
       "  7.934739242757651,\n",
       "  -4.270053088827323,\n",
       "  2.9165777414620533],\n",
       " [7.935904145573393,\n",
       "  0.8651179799023557,\n",
       "  -2.1776706585227794,\n",
       "  -1.7175998981499125,\n",
       "  -0.1223503521014046,\n",
       "  6.4845417687109945,\n",
       "  -3.1836872568763877,\n",
       "  1.7359639658793662],\n",
       " [7.044773774151533,\n",
       "  2.2618982292773557,\n",
       "  -2.2502633197449353,\n",
       "  -4.730054598997553,\n",
       "  -0.8146697973073462,\n",
       "  7.808860163873378,\n",
       "  -3.3728968508673036,\n",
       "  3.9254207009556925],\n",
       " [4.147172993860089,\n",
       "  1.6649555501888988,\n",
       "  -3.693132208948879,\n",
       "  -4.712630514486141,\n",
       "  3.4529069606514358,\n",
       "  4.021306991622302,\n",
       "  -3.5458226209836807,\n",
       "  2.2864791096150627],\n",
       " [5.846922835539597,\n",
       "  -0.13921402044892517,\n",
       "  -1.8476169036589214,\n",
       "  -4.56300840211377,\n",
       "  -2.021976264317927,\n",
       "  4.580591575505858,\n",
       "  -4.36933340247899,\n",
       "  4.696546721959869],\n",
       " [5.7308601630235785,\n",
       "  3.7970227780992687,\n",
       "  -4.488701676693646,\n",
       "  -7.32107286387321,\n",
       "  1.1187974875650366,\n",
       "  8.291781582056473,\n",
       "  -3.7339445336145602,\n",
       "  1.1276009279718073],\n",
       " [5.189676490059234,\n",
       "  1.5599818345207317,\n",
       "  -3.016429017673148,\n",
       "  -4.150120853358528,\n",
       "  0.4253740279187427,\n",
       "  4.9534355679761735,\n",
       "  -3.181360912849178,\n",
       "  1.7041695721561125],\n",
       " [6.628706832599725,\n",
       "  0.29870013824848995,\n",
       "  -9.21257241425555,\n",
       "  -1.3835345024188723,\n",
       "  3.2692247768916713,\n",
       "  2.892252444495332,\n",
       "  -3.4293584870083142,\n",
       "  4.746095471989912],\n",
       " [5.746142216936453,\n",
       "  2.6458560279642365,\n",
       "  -4.575850624255872,\n",
       "  -0.5300616552445216,\n",
       "  2.0444652232123355,\n",
       "  4.968117824645307,\n",
       "  -3.0792730695323645,\n",
       "  1.4127504340506476],\n",
       " [5.372522260538366,\n",
       "  0.789788823674184,\n",
       "  -4.807790505475281,\n",
       "  -3.480604743690266,\n",
       "  0.9847260960213251,\n",
       "  4.906111816526594,\n",
       "  -2.3962696548333016,\n",
       "  1.378743004882884],\n",
       " [7.811274354269537,\n",
       "  1.2746062107834195,\n",
       "  -4.821189393456693,\n",
       "  -6.166118491572041,\n",
       "  2.6811855130966977,\n",
       "  8.110308155380903,\n",
       "  -4.403155524761014,\n",
       "  1.3562975927719811],\n",
       " [6.914935548636753,\n",
       "  1.244877827153494,\n",
       "  -5.027061028557566,\n",
       "  -3.7254051304375584,\n",
       "  2.638968503128643,\n",
       "  7.289952406005064,\n",
       "  -3.63593524769349,\n",
       "  2.8325177840531803],\n",
       " [4.90238656830944,\n",
       "  0.831367023818826,\n",
       "  -5.917832433286733,\n",
       "  -4.301315287553505,\n",
       "  3.8029763326437047,\n",
       "  6.118434734048206,\n",
       "  -4.258519246384191,\n",
       "  4.022145680725474],\n",
       " [4.808179081859735,\n",
       "  1.5364766471403486,\n",
       "  -3.115402733461762,\n",
       "  -2.129897854271647,\n",
       "  3.349788886852553,\n",
       "  6.516243662946337,\n",
       "  -1.3062579709054605,\n",
       "  2.6675467107324486],\n",
       " [7.742437432905165,\n",
       "  1.3005088621345804,\n",
       "  -7.112149216557107,\n",
       "  -1.8610990296762941,\n",
       "  1.418713698046247,\n",
       "  8.385196290208956,\n",
       "  -3.716530425725378,\n",
       "  2.6523582033343116],\n",
       " [5.297606397944531,\n",
       "  1.0431976861968104,\n",
       "  -4.125188288067098,\n",
       "  -4.366081006222791,\n",
       "  5.335923974429761,\n",
       "  3.57824873643874,\n",
       "  -3.4308919382314724,\n",
       "  3.296916778592208],\n",
       " [9.791811654504869,\n",
       "  1.3567889753196267,\n",
       "  -4.7309121226893724,\n",
       "  -5.384264113073643,\n",
       "  4.797317803194743,\n",
       "  7.638614370454153,\n",
       "  -4.4474596594478495,\n",
       "  2.878926011740364],\n",
       " [4.430220844813188,\n",
       "  1.8620535624303345,\n",
       "  -3.201060832457732,\n",
       "  -3.2888483109307707,\n",
       "  3.982075701182935,\n",
       "  8.45073632582432,\n",
       "  -5.178899942988739,\n",
       "  -0.4363466021078124],\n",
       " [2.8732935409806784,\n",
       "  2.6444441365725075,\n",
       "  -3.449774135614355,\n",
       "  -2.872188030184076,\n",
       "  2.390015689966008,\n",
       "  3.6953135251894444,\n",
       "  -0.8562632680733571,\n",
       "  2.57116658277627],\n",
       " [7.302006941340354,\n",
       "  3.9068928684728013,\n",
       "  -2.739126658731945,\n",
       "  -2.9052428244241124,\n",
       "  2.09711481609825,\n",
       "  5.171076132246389,\n",
       "  -3.521874652243034,\n",
       "  3.6421820177270767],\n",
       " [7.635319570616028,\n",
       "  3.063786245911994,\n",
       "  -3.321301934918207,\n",
       "  -5.301440640212762,\n",
       "  2.188586044640207,\n",
       "  7.555466718678646,\n",
       "  -4.832039588711227,\n",
       "  1.1312842466163173],\n",
       " [6.540177602475286,\n",
       "  0.8103881469545672,\n",
       "  -6.000679641686812,\n",
       "  -3.273597452135676,\n",
       "  3.1363148404913264,\n",
       "  5.044520215024409,\n",
       "  -2.37741918498494,\n",
       "  0.9133175669316054],\n",
       " [6.454614284646971,\n",
       "  2.6453016279367203,\n",
       "  -1.396546638267497,\n",
       "  -5.071429046899469,\n",
       "  7.191194278109676,\n",
       "  5.698627297032376,\n",
       "  -5.043654431362026,\n",
       "  3.6008677520433423],\n",
       " [7.675298045793713,\n",
       "  1.1692406693235966,\n",
       "  -3.6304035559241514,\n",
       "  -5.0953628702546965,\n",
       "  4.532543119453407,\n",
       "  6.873074628024456,\n",
       "  -4.090897614138978,\n",
       "  1.477451632484546],\n",
       " [4.852982582583814,\n",
       "  3.95362702925932,\n",
       "  -0.6258272716403095,\n",
       "  -4.010558938920137,\n",
       "  1.8030999900222893,\n",
       "  6.185509649343133,\n",
       "  -4.39793853805849,\n",
       "  1.1252461457818752],\n",
       " [5.459575490564335,\n",
       "  0.27378492961372425,\n",
       "  -4.01512634721323,\n",
       "  -5.124098224210639,\n",
       "  4.4057486219293445,\n",
       "  2.0730882594078732,\n",
       "  -1.0177022580047508,\n",
       "  4.377130078839471],\n",
       " [7.6979535831533195,\n",
       "  4.40793198759109,\n",
       "  1.0177560027842114,\n",
       "  -3.7119866298572664,\n",
       "  5.420154022924637,\n",
       "  8.031970985679743,\n",
       "  -3.5061067614971138,\n",
       "  -0.14985255279270016],\n",
       " [7.437720047166687,\n",
       "  2.914148852680711,\n",
       "  -4.810986146812368,\n",
       "  -3.95942302452577,\n",
       "  6.231435079993796,\n",
       "  4.654098979751813,\n",
       "  -3.394799217240685,\n",
       "  3.5834368035923942],\n",
       " [5.703048274994256,\n",
       "  0.9025040738881278,\n",
       "  -6.331637854107727,\n",
       "  -4.473325358567025,\n",
       "  4.064779679476932,\n",
       "  7.028781382729868,\n",
       "  -3.2851857431617866,\n",
       "  3.2816013427734383],\n",
       " [6.489520420415818,\n",
       "  2.765069885150472,\n",
       "  -5.608062774179594,\n",
       "  -5.111201120636695,\n",
       "  3.435474541840874,\n",
       "  7.253122712872473,\n",
       "  -3.1363257530795705,\n",
       "  2.2958118817866935],\n",
       " [4.322727960420424,\n",
       "  1.2098568339442601,\n",
       "  -4.299572809519345,\n",
       "  -3.79019819051338,\n",
       "  6.563104296803114,\n",
       "  5.675404268128189,\n",
       "  -1.3348873991950683,\n",
       "  1.3843054630369733],\n",
       " [8.176665783608955,\n",
       "  1.6149841663036846,\n",
       "  -3.3965291343205997,\n",
       "  -3.5689464861068476,\n",
       "  4.064749534294064,\n",
       "  5.492190682637187,\n",
       "  -2.7287046777110833,\n",
       "  3.6940762361262474],\n",
       " [2.493175506820209,\n",
       "  1.4432512635124795,\n",
       "  -2.384378550956067,\n",
       "  -3.0678646952834976,\n",
       "  4.6941387882163434,\n",
       "  1.8410260980634412,\n",
       "  -1.9074600030541746,\n",
       "  3.360068701151612],\n",
       " [6.909151679139912,\n",
       "  0.14897811042550346,\n",
       "  -6.318317934137254,\n",
       "  -5.623930670740823,\n",
       "  3.7525984021384478,\n",
       "  7.219676126690544,\n",
       "  -2.7775965601900543,\n",
       "  6.219057555500001],\n",
       " [3.562347193060492,\n",
       "  0.3993129813989811,\n",
       "  -3.4350686748774995,\n",
       "  -5.448089005286198,\n",
       "  2.039493419017429,\n",
       "  4.221977912144111,\n",
       "  -1.2959215922006675,\n",
       "  2.566841105410319],\n",
       " [9.852876091683434,\n",
       "  3.1776175325326785,\n",
       "  -4.6101248846610625,\n",
       "  -9.48011467786975,\n",
       "  5.370712997299389,\n",
       "  6.649454841761471,\n",
       "  -3.4654820604241774,\n",
       "  4.843461458574241],\n",
       " [5.0745603581766865,\n",
       "  0.6408475230502724,\n",
       "  -1.919149735907273,\n",
       "  -5.43384245111747,\n",
       "  1.72107824974686,\n",
       "  1.883666386473878,\n",
       "  -0.8295541247517004,\n",
       "  3.4651972882635977],\n",
       " [4.976916552781533,\n",
       "  1.5300013461049298,\n",
       "  -1.2457724891639221,\n",
       "  -3.632310471461333,\n",
       "  2.834002554837836,\n",
       "  3.7249550043618003,\n",
       "  -2.801096709038786,\n",
       "  3.9476509529257946],\n",
       " [7.703019900492663,\n",
       "  2.0959041956097786,\n",
       "  -3.935098177956962,\n",
       "  -4.223157970269535,\n",
       "  4.390040330788133,\n",
       "  5.9116896014802265,\n",
       "  -4.937777682016591,\n",
       "  4.980696716940253],\n",
       " [4.6033912335481455,\n",
       "  0.341969188090276,\n",
       "  -3.5441617135581054,\n",
       "  -5.212055459275401,\n",
       "  4.867377955859444,\n",
       "  3.5810644976717443,\n",
       "  -3.4707872064754066,\n",
       "  4.1875017198713484],\n",
       " [4.91186915845837,\n",
       "  1.9440853975172399,\n",
       "  -2.6511620121687303,\n",
       "  -5.8852132602430585,\n",
       "  3.5651981567172246,\n",
       "  2.584681302447681,\n",
       "  -3.0549977323301127,\n",
       "  2.8246849033262853],\n",
       " [5.552807096094715,\n",
       "  1.8963183745477208,\n",
       "  -2.024013361914532,\n",
       "  -4.186919845415054,\n",
       "  5.69920927908009,\n",
       "  4.976181454525028,\n",
       "  -2.4645254003777537,\n",
       "  5.404946458541345],\n",
       " [5.601661720348234,\n",
       "  0.08429249003229289,\n",
       "  -3.429334310038972,\n",
       "  -7.060680041813253,\n",
       "  1.618190417123769,\n",
       "  1.960711488801706,\n",
       "  -1.5999661129378966,\n",
       "  5.8568143630070635],\n",
       " [5.954970215159863,\n",
       "  2.0081639840437373,\n",
       "  -3.0606865913160237,\n",
       "  -3.0607512111712825,\n",
       "  2.4203743135910676,\n",
       "  3.1691086592815427,\n",
       "  -2.9238729216589707,\n",
       "  3.5179394003537006],\n",
       " [5.341392516579396,\n",
       "  0.8602414196130255,\n",
       "  -2.6332439095472897,\n",
       "  -5.266162897165524,\n",
       "  2.7923075936175703,\n",
       "  2.984296932760298,\n",
       "  -4.05429599739151,\n",
       "  3.12108983775801],\n",
       " [4.927132493860965,\n",
       "  0.8275210897328928,\n",
       "  -2.7405451037922943,\n",
       "  -5.244797489328107,\n",
       "  0.40718578717820986,\n",
       "  2.912700268442971,\n",
       "  -0.4169473333413276,\n",
       "  4.118678464256032],\n",
       " [6.398678833672981,\n",
       "  2.581250635839449,\n",
       "  -1.6987264127003108,\n",
       "  -3.6541515325968055,\n",
       "  3.6909743933143657,\n",
       "  1.9325603163046408,\n",
       "  -2.983668177095849,\n",
       "  3.6491376237661894],\n",
       " [6.674624771232823,\n",
       "  2.806166009950305,\n",
       "  -0.4714370485011319,\n",
       "  -6.489759355941979,\n",
       "  4.338430621722669,\n",
       "  3.496491153053314,\n",
       "  -4.0434844019192875,\n",
       "  3.4822206143418644],\n",
       " [5.955295480275972,\n",
       "  1.0262074633019358,\n",
       "  -3.1864769260726997,\n",
       "  -2.7529344225520944,\n",
       "  3.735974749992632,\n",
       "  3.45095616686598,\n",
       "  -3.8716579799448274,\n",
       "  5.605061848869747],\n",
       " [4.484613400423036,\n",
       "  1.6522999139699743,\n",
       "  1.182803731136414,\n",
       "  -7.274610649715671,\n",
       "  1.8174505751944867,\n",
       "  0.4237754305146266,\n",
       "  -1.7084701018149813,\n",
       "  1.373906379384303],\n",
       " [4.687135745614982,\n",
       "  0.46090762872427504,\n",
       "  -2.155518147035743,\n",
       "  -4.608121830154153,\n",
       "  4.274847650475459,\n",
       "  3.61624463416194,\n",
       "  -2.325985413795137,\n",
       "  5.3859185235397815],\n",
       " [3.525243082371604,\n",
       "  -1.4973419543534445,\n",
       "  0.11134451031333192,\n",
       "  -2.512561120118899,\n",
       "  0.9752617960543475,\n",
       "  -0.9096499000637661,\n",
       "  1.2847868161843345,\n",
       "  2.494318319639812],\n",
       " [5.062407735717501,\n",
       "  3.728577232131885,\n",
       "  0.9766584874019264,\n",
       "  -5.923712612292366,\n",
       "  4.014219677151675,\n",
       "  3.594967820140523,\n",
       "  -1.6464907643572144,\n",
       "  3.317507495260846],\n",
       " [4.334655487031788,\n",
       "  1.3376270970954234,\n",
       "  -0.7737962895946723,\n",
       "  -4.959312656972695,\n",
       "  2.72854096326542,\n",
       "  2.3562272671653393,\n",
       "  -1.443383317537124,\n",
       "  2.5721614313030745],\n",
       " [4.992181104114136,\n",
       "  2.141928476523215,\n",
       "  -0.36432994655296946,\n",
       "  -6.8069380251315135,\n",
       "  2.3220194830847127,\n",
       "  1.1391189942272812,\n",
       "  -2.287693320109147,\n",
       "  0.8618709389373924],\n",
       " [7.340342740183391,\n",
       "  3.210867590958857,\n",
       "  -2.059745293489598,\n",
       "  -4.8573664071020355,\n",
       "  2.6102490872184765,\n",
       "  4.180189350838301,\n",
       "  -4.3987687476551685,\n",
       "  3.838137944322333],\n",
       " [5.789506825134206,\n",
       "  2.348098759954285,\n",
       "  -1.1923855751132855,\n",
       "  -6.700076156288925,\n",
       "  3.8186416300869173,\n",
       "  7.336765078933485,\n",
       "  -2.8252219169993875,\n",
       "  4.610728334404153],\n",
       " [5.80706103111611,\n",
       "  0.08080220228207136,\n",
       "  -1.295292069174171,\n",
       "  -2.7238265997560585,\n",
       "  1.9859181753355206,\n",
       "  2.875134090060043,\n",
       "  -2.521814298506973,\n",
       "  3.3402503307524527],\n",
       " [5.755168013177,\n",
       "  2.300725222169024,\n",
       "  -1.834069419214888,\n",
       "  -6.6588979356812645,\n",
       "  1.560111269934553,\n",
       "  2.509243512606099,\n",
       "  -1.7082724302919339,\n",
       "  2.5060458061485718],\n",
       " [6.36845616273471,\n",
       "  1.1806189536143687,\n",
       "  -1.617835409472044,\n",
       "  -4.998970752665938,\n",
       "  3.099469754027544,\n",
       "  4.717960884228126,\n",
       "  -4.417321476342165,\n",
       "  1.9122144313759546],\n",
       " [7.522983205723817,\n",
       "  1.9981958314831108,\n",
       "  -1.375449195954969,\n",
       "  -4.781264755699662,\n",
       "  1.3326864235721732,\n",
       "  1.1974668462349602,\n",
       "  -4.131785332753154,\n",
       "  4.760909357596598],\n",
       " [7.561395811381603,\n",
       "  3.109326634931441,\n",
       "  -0.35028048247196386,\n",
       "  -4.907606049975314,\n",
       "  1.9617679240640804,\n",
       "  4.161726294538497,\n",
       "  -2.9927491975077385,\n",
       "  2.6155609560438]]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "for i in range(len(ep_scatter)):\n",
    "    plt.scatter(x_scatter[i][0], x_scatter[i][1], color='r', s=s_scatter[i], alpha = ep_scatter[i])\n",
    "    if i < len(ep_scatter)-1:\n",
    "        plt.plot([x_scatter[i-1][0]] + [x_scatter[i][1]], [x_scatter[i-1][0]] + [x_scatter[i][1]], color='black', alpha=0.4*i/len(ep_scatter))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nrows = 3\n",
    "ncols = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x268766198>]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(line_x, line_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid RGBA argument: array([[1.        , 0.        , 0.        , 0.1       ],\n       [1.        , 0.        , 0.        , 0.10452261],\n       [1.        , 0.        , 0.        , 0.10904523],\n       [1.        , 0.        , 0.        , 0.11356784],\n       [1.        , 0.        , 0.        , 0.11809045],\n       [1.        , 0.        , 0.        , 0.12261307],\n       [1.        , 0.        , 0.        , 0.12713568],\n       [1.        , 0.        , 0.        , 0.13165829],\n       [1.        , 0.        , 0.        , 0.1361809 ],\n       [1.        , 0.        , 0.        , 0.14070352],\n       [1.        , 0.        , 0.        , 0.14522613],\n       [1.        , 0.        , 0.        , 0.14974874],\n       [1.        , 0.        , 0.        , 0.15427136],\n       [1.        , 0.        , 0.        , 0.15879397],\n       [1.        , 0.        , 0.        , 0.16331658],\n       [1.        , 0.        , 0.        , 0.1678392 ],\n       [1.        , 0.        , 0.        , 0.17236181],\n       [1.        , 0.        , 0.        , 0.17688442],\n       [1.        , 0.        , 0.        , 0.18140704],\n       [1.        , 0.        , 0.        , 0.18592965],\n       [1.        , 0.        , 0.        , 0.19045226],\n       [1.        , 0.        , 0.        , 0.19497487],\n       [1.        , 0.        , 0.        , 0.19949749],\n       [1.        , 0.        , 0.        , 0.2040201 ],\n       [1.        , 0.        , 0.        , 0.20854271],\n       [1.        , 0.        , 0.        , 0.21306533],\n       [1.        , 0.        , 0.        , 0.21758794],\n       [1.        , 0.        , 0.        , 0.22211055],\n       [1.        , 0.        , 0.        , 0.22663317],\n       [1.        , 0.        , 0.        , 0.23115578],\n       [1.        , 0.        , 0.        , 0.23567839],\n       [1.        , 0.        , 0.        , 0.24020101],\n       [1.        , 0.        , 0.        , 0.24472362],\n       [1.        , 0.        , 0.        , 0.24924623],\n       [1.        , 0.        , 0.        , 0.25376884],\n       [1.        , 0.        , 0.        , 0.25829146],\n       [1.        , 0.        , 0.        , 0.26281407],\n       [1.        , 0.        , 0.        , 0.26733668],\n       [1.        , 0.        , 0.        , 0.2718593 ],\n       [1.        , 0.        , 0.        , 0.27638191],\n       [1.        , 0.        , 0.        , 0.28090452],\n       [1.        , 0.        , 0.        , 0.28542714],\n       [1.        , 0.        , 0.        , 0.28994975],\n       [1.        , 0.        , 0.        , 0.29447236],\n       [1.        , 0.        , 0.        , 0.29899497],\n       [1.        , 0.        , 0.        , 0.30351759],\n       [1.        , 0.        , 0.        , 0.3080402 ],\n       [1.        , 0.        , 0.        , 0.31256281],\n       [1.        , 0.        , 0.        , 0.31708543],\n       [1.        , 0.        , 0.        , 0.32160804],\n       [1.        , 0.        , 0.        , 0.32613065],\n       [1.        , 0.        , 0.        , 0.33065327],\n       [1.        , 0.        , 0.        , 0.33517588],\n       [1.        , 0.        , 0.        , 0.33969849],\n       [1.        , 0.        , 0.        , 0.34422111],\n       [1.        , 0.        , 0.        , 0.34874372],\n       [1.        , 0.        , 0.        , 0.35326633],\n       [1.        , 0.        , 0.        , 0.35778894],\n       [1.        , 0.        , 0.        , 0.36231156],\n       [1.        , 0.        , 0.        , 0.36683417],\n       [1.        , 0.        , 0.        , 0.37135678],\n       [1.        , 0.        , 0.        , 0.3758794 ],\n       [1.        , 0.        , 0.        , 0.38040201],\n       [1.        , 0.        , 0.        , 0.38492462],\n       [1.        , 0.        , 0.        , 0.38944724],\n       [1.        , 0.        , 0.        , 0.39396985],\n       [1.        , 0.        , 0.        , 0.39849246],\n       [1.        , 0.        , 0.        , 0.40301508],\n       [1.        , 0.        , 0.        , 0.40753769],\n       [1.        , 0.        , 0.        , 0.4120603 ],\n       [1.        , 0.        , 0.        , 0.41658291],\n       [1.        , 0.        , 0.        , 0.42110553],\n       [1.        , 0.        , 0.        , 0.42562814],\n       [1.        , 0.        , 0.        , 0.43015075],\n       [1.        , 0.        , 0.        , 0.43467337],\n       [1.        , 0.        , 0.        , 0.43919598],\n       [1.        , 0.        , 0.        , 0.44371859],\n       [1.        , 0.        , 0.        , 0.44824121],\n       [1.        , 0.        , 0.        , 0.45276382],\n       [1.        , 0.        , 0.        , 0.45728643],\n       [1.        , 0.        , 0.        , 0.46180905],\n       [1.        , 0.        , 0.        , 0.46633166],\n       [1.        , 0.        , 0.        , 0.47085427],\n       [1.        , 0.        , 0.        , 0.47537688],\n       [1.        , 0.        , 0.        , 0.4798995 ],\n       [1.        , 0.        , 0.        , 0.48442211],\n       [1.        , 0.        , 0.        , 0.48894472],\n       [1.        , 0.        , 0.        , 0.49346734],\n       [1.        , 0.        , 0.        , 0.49798995],\n       [1.        , 0.        , 0.        , 0.50251256],\n       [1.        , 0.        , 0.        , 0.50703518],\n       [1.        , 0.        , 0.        , 0.51155779],\n       [1.        , 0.        , 0.        , 0.5160804 ],\n       [1.        , 0.        , 0.        , 0.52060302],\n       [1.        , 0.        , 0.        , 0.52512563],\n       [1.        , 0.        , 0.        , 0.52964824],\n       [1.        , 0.        , 0.        , 0.53417085],\n       [1.        , 0.        , 0.        , 0.53869347],\n       [1.        , 0.        , 0.        , 0.54321608],\n       [1.        , 0.        , 0.        , 0.54773869],\n       [1.        , 0.        , 0.        , 0.55226131],\n       [1.        , 0.        , 0.        , 0.55678392],\n       [1.        , 0.        , 0.        , 0.56130653],\n       [1.        , 0.        , 0.        , 0.56582915],\n       [1.        , 0.        , 0.        , 0.57035176],\n       [1.        , 0.        , 0.        , 0.57487437],\n       [1.        , 0.        , 0.        , 0.57939698],\n       [1.        , 0.        , 0.        , 0.5839196 ],\n       [1.        , 0.        , 0.        , 0.58844221],\n       [1.        , 0.        , 0.        , 0.59296482],\n       [1.        , 0.        , 0.        , 0.59748744],\n       [1.        , 0.        , 0.        , 0.60201005],\n       [1.        , 0.        , 0.        , 0.60653266],\n       [1.        , 0.        , 0.        , 0.61105528],\n       [1.        , 0.        , 0.        , 0.61557789],\n       [1.        , 0.        , 0.        , 0.6201005 ],\n       [1.        , 0.        , 0.        , 0.62462312],\n       [1.        , 0.        , 0.        , 0.62914573],\n       [1.        , 0.        , 0.        , 0.63366834],\n       [1.        , 0.        , 0.        , 0.63819095],\n       [1.        , 0.        , 0.        , 0.64271357],\n       [1.        , 0.        , 0.        , 0.64723618],\n       [1.        , 0.        , 0.        , 0.65175879],\n       [1.        , 0.        , 0.        , 0.65628141],\n       [1.        , 0.        , 0.        , 0.66080402],\n       [1.        , 0.        , 0.        , 0.66532663],\n       [1.        , 0.        , 0.        , 0.66984925],\n       [1.        , 0.        , 0.        , 0.67437186],\n       [1.        , 0.        , 0.        , 0.67889447],\n       [1.        , 0.        , 0.        , 0.68341709],\n       [1.        , 0.        , 0.        , 0.6879397 ],\n       [1.        , 0.        , 0.        , 0.69246231],\n       [1.        , 0.        , 0.        , 0.69698492],\n       [1.        , 0.        , 0.        , 0.70150754],\n       [1.        , 0.        , 0.        , 0.70603015],\n       [1.        , 0.        , 0.        , 0.71055276],\n       [1.        , 0.        , 0.        , 0.71507538],\n       [1.        , 0.        , 0.        , 0.71959799],\n       [1.        , 0.        , 0.        , 0.7241206 ],\n       [1.        , 0.        , 0.        , 0.72864322],\n       [1.        , 0.        , 0.        , 0.73316583],\n       [1.        , 0.        , 0.        , 0.73768844],\n       [1.        , 0.        , 0.        , 0.74221106],\n       [1.        , 0.        , 0.        , 0.74673367],\n       [1.        , 0.        , 0.        , 0.75125628],\n       [1.        , 0.        , 0.        , 0.75577889],\n       [1.        , 0.        , 0.        , 0.76030151],\n       [1.        , 0.        , 0.        , 0.76482412],\n       [1.        , 0.        , 0.        , 0.76934673],\n       [1.        , 0.        , 0.        , 0.77386935],\n       [1.        , 0.        , 0.        , 0.77839196],\n       [1.        , 0.        , 0.        , 0.78291457],\n       [1.        , 0.        , 0.        , 0.78743719],\n       [1.        , 0.        , 0.        , 0.7919598 ],\n       [1.        , 0.        , 0.        , 0.79648241],\n       [1.        , 0.        , 0.        , 0.80100503],\n       [1.        , 0.        , 0.        , 0.80552764],\n       [1.        , 0.        , 0.        , 0.81005025],\n       [1.        , 0.        , 0.        , 0.81457286],\n       [1.        , 0.        , 0.        , 0.81909548],\n       [1.        , 0.        , 0.        , 0.82361809],\n       [1.        , 0.        , 0.        , 0.8281407 ],\n       [1.        , 0.        , 0.        , 0.83266332],\n       [1.        , 0.        , 0.        , 0.83718593],\n       [1.        , 0.        , 0.        , 0.84170854],\n       [1.        , 0.        , 0.        , 0.84623116],\n       [1.        , 0.        , 0.        , 0.85075377],\n       [1.        , 0.        , 0.        , 0.85527638],\n       [1.        , 0.        , 0.        , 0.85979899],\n       [1.        , 0.        , 0.        , 0.86432161],\n       [1.        , 0.        , 0.        , 0.86884422],\n       [1.        , 0.        , 0.        , 0.87336683],\n       [1.        , 0.        , 0.        , 0.87788945],\n       [1.        , 0.        , 0.        , 0.88241206],\n       [1.        , 0.        , 0.        , 0.88693467],\n       [1.        , 0.        , 0.        , 0.89145729],\n       [1.        , 0.        , 0.        , 0.8959799 ],\n       [1.        , 0.        , 0.        , 0.90050251],\n       [1.        , 0.        , 0.        , 0.90502513],\n       [1.        , 0.        , 0.        , 0.90954774],\n       [1.        , 0.        , 0.        , 0.91407035],\n       [1.        , 0.        , 0.        , 0.91859296],\n       [1.        , 0.        , 0.        , 0.92311558],\n       [1.        , 0.        , 0.        , 0.92763819],\n       [1.        , 0.        , 0.        , 0.9321608 ],\n       [1.        , 0.        , 0.        , 0.93668342],\n       [1.        , 0.        , 0.        , 0.94120603],\n       [1.        , 0.        , 0.        , 0.94572864],\n       [1.        , 0.        , 0.        , 0.95025126],\n       [1.        , 0.        , 0.        , 0.95477387],\n       [1.        , 0.        , 0.        , 0.95929648],\n       [1.        , 0.        , 0.        , 0.9638191 ],\n       [1.        , 0.        , 0.        , 0.96834171],\n       [1.        , 0.        , 0.        , 0.97286432],\n       [1.        , 0.        , 0.        , 0.97738693],\n       [1.        , 0.        , 0.        , 0.98190955],\n       [1.        , 0.        , 0.        , 0.98643216],\n       [1.        , 0.        , 0.        , 0.99095477],\n       [1.        , 0.        , 0.        , 0.99547739],\n       [1.        , 0.        , 0.        , 1.        ]])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_colors_full_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Not in cache, or unhashable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2047\u001b[0m                         \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2048\u001b[0m                         \u001b[0mdryrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2049\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   2050\u001b[0m                     \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cachedRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2051\u001b[0m                     \u001b[0mbbox_artists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bbox_extra_artists\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \"\"\"\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mtoolbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoolbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1649\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2626\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2628\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_gc_clip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m                 \u001b[0mlc_rgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_foreground\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlc_rgba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misRGBA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_colors_full_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Not in cache, or unhashable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_rgba_no_colorcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0m_colors_full_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/musicai/lib/python3.6/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m_to_rgba_no_colorcycle\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# float)` and `np.array(...).astype(float)` all convert \"0.5\" to 0.5.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# Test dimensionality to reject single floats.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid RGBA argument: {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0;31m# Return a tuple to prevent the cached value from being modified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid RGBA argument: array([[1.        , 0.        , 0.        , 0.1       ],\n       [1.        , 0.        , 0.        , 0.10452261],\n       [1.        , 0.        , 0.        , 0.10904523],\n       [1.        , 0.        , 0.        , 0.11356784],\n       [1.        , 0.        , 0.        , 0.11809045],\n       [1.        , 0.        , 0.        , 0.12261307],\n       [1.        , 0.        , 0.        , 0.12713568],\n       [1.        , 0.        , 0.        , 0.13165829],\n       [1.        , 0.        , 0.        , 0.1361809 ],\n       [1.        , 0.        , 0.        , 0.14070352],\n       [1.        , 0.        , 0.        , 0.14522613],\n       [1.        , 0.        , 0.        , 0.14974874],\n       [1.        , 0.        , 0.        , 0.15427136],\n       [1.        , 0.        , 0.        , 0.15879397],\n       [1.        , 0.        , 0.        , 0.16331658],\n       [1.        , 0.        , 0.        , 0.1678392 ],\n       [1.        , 0.        , 0.        , 0.17236181],\n       [1.        , 0.        , 0.        , 0.17688442],\n       [1.        , 0.        , 0.        , 0.18140704],\n       [1.        , 0.        , 0.        , 0.18592965],\n       [1.        , 0.        , 0.        , 0.19045226],\n       [1.        , 0.        , 0.        , 0.19497487],\n       [1.        , 0.        , 0.        , 0.19949749],\n       [1.        , 0.        , 0.        , 0.2040201 ],\n       [1.        , 0.        , 0.        , 0.20854271],\n       [1.        , 0.        , 0.        , 0.21306533],\n       [1.        , 0.        , 0.        , 0.21758794],\n       [1.        , 0.        , 0.        , 0.22211055],\n       [1.        , 0.        , 0.        , 0.22663317],\n       [1.        , 0.        , 0.        , 0.23115578],\n       [1.        , 0.        , 0.        , 0.23567839],\n       [1.        , 0.        , 0.        , 0.24020101],\n       [1.        , 0.        , 0.        , 0.24472362],\n       [1.        , 0.        , 0.        , 0.24924623],\n       [1.        , 0.        , 0.        , 0.25376884],\n       [1.        , 0.        , 0.        , 0.25829146],\n       [1.        , 0.        , 0.        , 0.26281407],\n       [1.        , 0.        , 0.        , 0.26733668],\n       [1.        , 0.        , 0.        , 0.2718593 ],\n       [1.        , 0.        , 0.        , 0.27638191],\n       [1.        , 0.        , 0.        , 0.28090452],\n       [1.        , 0.        , 0.        , 0.28542714],\n       [1.        , 0.        , 0.        , 0.28994975],\n       [1.        , 0.        , 0.        , 0.29447236],\n       [1.        , 0.        , 0.        , 0.29899497],\n       [1.        , 0.        , 0.        , 0.30351759],\n       [1.        , 0.        , 0.        , 0.3080402 ],\n       [1.        , 0.        , 0.        , 0.31256281],\n       [1.        , 0.        , 0.        , 0.31708543],\n       [1.        , 0.        , 0.        , 0.32160804],\n       [1.        , 0.        , 0.        , 0.32613065],\n       [1.        , 0.        , 0.        , 0.33065327],\n       [1.        , 0.        , 0.        , 0.33517588],\n       [1.        , 0.        , 0.        , 0.33969849],\n       [1.        , 0.        , 0.        , 0.34422111],\n       [1.        , 0.        , 0.        , 0.34874372],\n       [1.        , 0.        , 0.        , 0.35326633],\n       [1.        , 0.        , 0.        , 0.35778894],\n       [1.        , 0.        , 0.        , 0.36231156],\n       [1.        , 0.        , 0.        , 0.36683417],\n       [1.        , 0.        , 0.        , 0.37135678],\n       [1.        , 0.        , 0.        , 0.3758794 ],\n       [1.        , 0.        , 0.        , 0.38040201],\n       [1.        , 0.        , 0.        , 0.38492462],\n       [1.        , 0.        , 0.        , 0.38944724],\n       [1.        , 0.        , 0.        , 0.39396985],\n       [1.        , 0.        , 0.        , 0.39849246],\n       [1.        , 0.        , 0.        , 0.40301508],\n       [1.        , 0.        , 0.        , 0.40753769],\n       [1.        , 0.        , 0.        , 0.4120603 ],\n       [1.        , 0.        , 0.        , 0.41658291],\n       [1.        , 0.        , 0.        , 0.42110553],\n       [1.        , 0.        , 0.        , 0.42562814],\n       [1.        , 0.        , 0.        , 0.43015075],\n       [1.        , 0.        , 0.        , 0.43467337],\n       [1.        , 0.        , 0.        , 0.43919598],\n       [1.        , 0.        , 0.        , 0.44371859],\n       [1.        , 0.        , 0.        , 0.44824121],\n       [1.        , 0.        , 0.        , 0.45276382],\n       [1.        , 0.        , 0.        , 0.45728643],\n       [1.        , 0.        , 0.        , 0.46180905],\n       [1.        , 0.        , 0.        , 0.46633166],\n       [1.        , 0.        , 0.        , 0.47085427],\n       [1.        , 0.        , 0.        , 0.47537688],\n       [1.        , 0.        , 0.        , 0.4798995 ],\n       [1.        , 0.        , 0.        , 0.48442211],\n       [1.        , 0.        , 0.        , 0.48894472],\n       [1.        , 0.        , 0.        , 0.49346734],\n       [1.        , 0.        , 0.        , 0.49798995],\n       [1.        , 0.        , 0.        , 0.50251256],\n       [1.        , 0.        , 0.        , 0.50703518],\n       [1.        , 0.        , 0.        , 0.51155779],\n       [1.        , 0.        , 0.        , 0.5160804 ],\n       [1.        , 0.        , 0.        , 0.52060302],\n       [1.        , 0.        , 0.        , 0.52512563],\n       [1.        , 0.        , 0.        , 0.52964824],\n       [1.        , 0.        , 0.        , 0.53417085],\n       [1.        , 0.        , 0.        , 0.53869347],\n       [1.        , 0.        , 0.        , 0.54321608],\n       [1.        , 0.        , 0.        , 0.54773869],\n       [1.        , 0.        , 0.        , 0.55226131],\n       [1.        , 0.        , 0.        , 0.55678392],\n       [1.        , 0.        , 0.        , 0.56130653],\n       [1.        , 0.        , 0.        , 0.56582915],\n       [1.        , 0.        , 0.        , 0.57035176],\n       [1.        , 0.        , 0.        , 0.57487437],\n       [1.        , 0.        , 0.        , 0.57939698],\n       [1.        , 0.        , 0.        , 0.5839196 ],\n       [1.        , 0.        , 0.        , 0.58844221],\n       [1.        , 0.        , 0.        , 0.59296482],\n       [1.        , 0.        , 0.        , 0.59748744],\n       [1.        , 0.        , 0.        , 0.60201005],\n       [1.        , 0.        , 0.        , 0.60653266],\n       [1.        , 0.        , 0.        , 0.61105528],\n       [1.        , 0.        , 0.        , 0.61557789],\n       [1.        , 0.        , 0.        , 0.6201005 ],\n       [1.        , 0.        , 0.        , 0.62462312],\n       [1.        , 0.        , 0.        , 0.62914573],\n       [1.        , 0.        , 0.        , 0.63366834],\n       [1.        , 0.        , 0.        , 0.63819095],\n       [1.        , 0.        , 0.        , 0.64271357],\n       [1.        , 0.        , 0.        , 0.64723618],\n       [1.        , 0.        , 0.        , 0.65175879],\n       [1.        , 0.        , 0.        , 0.65628141],\n       [1.        , 0.        , 0.        , 0.66080402],\n       [1.        , 0.        , 0.        , 0.66532663],\n       [1.        , 0.        , 0.        , 0.66984925],\n       [1.        , 0.        , 0.        , 0.67437186],\n       [1.        , 0.        , 0.        , 0.67889447],\n       [1.        , 0.        , 0.        , 0.68341709],\n       [1.        , 0.        , 0.        , 0.6879397 ],\n       [1.        , 0.        , 0.        , 0.69246231],\n       [1.        , 0.        , 0.        , 0.69698492],\n       [1.        , 0.        , 0.        , 0.70150754],\n       [1.        , 0.        , 0.        , 0.70603015],\n       [1.        , 0.        , 0.        , 0.71055276],\n       [1.        , 0.        , 0.        , 0.71507538],\n       [1.        , 0.        , 0.        , 0.71959799],\n       [1.        , 0.        , 0.        , 0.7241206 ],\n       [1.        , 0.        , 0.        , 0.72864322],\n       [1.        , 0.        , 0.        , 0.73316583],\n       [1.        , 0.        , 0.        , 0.73768844],\n       [1.        , 0.        , 0.        , 0.74221106],\n       [1.        , 0.        , 0.        , 0.74673367],\n       [1.        , 0.        , 0.        , 0.75125628],\n       [1.        , 0.        , 0.        , 0.75577889],\n       [1.        , 0.        , 0.        , 0.76030151],\n       [1.        , 0.        , 0.        , 0.76482412],\n       [1.        , 0.        , 0.        , 0.76934673],\n       [1.        , 0.        , 0.        , 0.77386935],\n       [1.        , 0.        , 0.        , 0.77839196],\n       [1.        , 0.        , 0.        , 0.78291457],\n       [1.        , 0.        , 0.        , 0.78743719],\n       [1.        , 0.        , 0.        , 0.7919598 ],\n       [1.        , 0.        , 0.        , 0.79648241],\n       [1.        , 0.        , 0.        , 0.80100503],\n       [1.        , 0.        , 0.        , 0.80552764],\n       [1.        , 0.        , 0.        , 0.81005025],\n       [1.        , 0.        , 0.        , 0.81457286],\n       [1.        , 0.        , 0.        , 0.81909548],\n       [1.        , 0.        , 0.        , 0.82361809],\n       [1.        , 0.        , 0.        , 0.8281407 ],\n       [1.        , 0.        , 0.        , 0.83266332],\n       [1.        , 0.        , 0.        , 0.83718593],\n       [1.        , 0.        , 0.        , 0.84170854],\n       [1.        , 0.        , 0.        , 0.84623116],\n       [1.        , 0.        , 0.        , 0.85075377],\n       [1.        , 0.        , 0.        , 0.85527638],\n       [1.        , 0.        , 0.        , 0.85979899],\n       [1.        , 0.        , 0.        , 0.86432161],\n       [1.        , 0.        , 0.        , 0.86884422],\n       [1.        , 0.        , 0.        , 0.87336683],\n       [1.        , 0.        , 0.        , 0.87788945],\n       [1.        , 0.        , 0.        , 0.88241206],\n       [1.        , 0.        , 0.        , 0.88693467],\n       [1.        , 0.        , 0.        , 0.89145729],\n       [1.        , 0.        , 0.        , 0.8959799 ],\n       [1.        , 0.        , 0.        , 0.90050251],\n       [1.        , 0.        , 0.        , 0.90502513],\n       [1.        , 0.        , 0.        , 0.90954774],\n       [1.        , 0.        , 0.        , 0.91407035],\n       [1.        , 0.        , 0.        , 0.91859296],\n       [1.        , 0.        , 0.        , 0.92311558],\n       [1.        , 0.        , 0.        , 0.92763819],\n       [1.        , 0.        , 0.        , 0.9321608 ],\n       [1.        , 0.        , 0.        , 0.93668342],\n       [1.        , 0.        , 0.        , 0.94120603],\n       [1.        , 0.        , 0.        , 0.94572864],\n       [1.        , 0.        , 0.        , 0.95025126],\n       [1.        , 0.        , 0.        , 0.95477387],\n       [1.        , 0.        , 0.        , 0.95929648],\n       [1.        , 0.        , 0.        , 0.9638191 ],\n       [1.        , 0.        , 0.        , 0.96834171],\n       [1.        , 0.        , 0.        , 0.97286432],\n       [1.        , 0.        , 0.        , 0.97738693],\n       [1.        , 0.        , 0.        , 0.98190955],\n       [1.        , 0.        , 0.        , 0.98643216],\n       [1.        , 0.        , 0.        , 0.99095477],\n       [1.        , 0.        , 0.        , 0.99547739],\n       [1.        , 0.        , 0.        , 1.        ]])"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure()    \n",
    "\n",
    "f, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=True, sharey = True, figsize=(10,10))\n",
    "\n",
    "\n",
    "alphas = np.linspace(0.1, 1, len(ep_scatter))\n",
    "rgba_colors = np.zeros((len(ep_scatter),4))\n",
    "# for red the first column needs to be one\n",
    "rgba_colors[:,0] = 1.0\n",
    "# the fourth column needs to be your alphas\n",
    "rgba_colors[:, 3] = alphas\n",
    "\n",
    "for col in range(ncols):\n",
    "    for row in range(nrows):\n",
    "        line_x = []\n",
    "        line_y = []\n",
    "        for i in range(len(ep_scatter)):\n",
    "            line_x.append(x_scatter[i-1][row])\n",
    "            line_y.append(x_scatter[i-1][col])\n",
    "            \n",
    "        axes[row][col].scatter(line_x, line_y, s=s_scatter[i], color=rgba_colors)\n",
    "            \n",
    "        axes[row][col].plot(line_x, line_y, color = rgba_colors)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "1 0\n",
      "1 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure()    \n",
    "\n",
    "nrows = 2\n",
    "ncols = 2\n",
    "f, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=True, sharey = True, figsize=(10,10))\n",
    "\n",
    "\n",
    "\n",
    "for col in range(ncols):\n",
    "    for row in range(nrows):\n",
    "        for i in range(len(ep_scatter)):\n",
    "            line_x = []\n",
    "            line_y = []\n",
    "            line_x.append(x_scatter[i-1][0])\n",
    "            line_y.append(x_scatter[i-1][1])\n",
    "        for i in range(len(ep_scatter)):\n",
    "            axes[row][col].scatter(x_scatter[i][col], x_scatter[i][row],\n",
    "                                   color='r', s=s_scatter[i], alpha = ep_scatter[i])\n",
    "            if i < len(ep_scatter)-1:\n",
    "                axes[row][col].plot([x_scatter[i-1][col]] + [x_scatter[i][col]],\n",
    "                                    [x_scatter[i-1][row]] + [x_scatter[i][row]],\n",
    "                                    color='black', alpha=0.4*i/len(ep_scatter))\n",
    "            axes[row][col].plot(line_x, line_y, color='black', alpha=0.4*i/len(ep_scatter))\n",
    "            \n",
    "        print(col, row)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7408146354361135, 0.5331237899686446)\n"
     ]
    }
   ],
   "source": [
    "print(recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 34s 17ms/step - loss: 1.7717 - acc: 0.4660 - val_loss: 1.4153 - val_acc: 0.6330\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 158us/step - loss: 1.1038 - acc: 0.7135 - val_loss: 1.0279 - val_acc: 0.7070\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 157us/step - loss: 0.8090 - acc: 0.7800 - val_loss: 0.8314 - val_acc: 0.7730\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 162us/step - loss: 0.6462 - acc: 0.8100 - val_loss: 0.7143 - val_acc: 0.8090\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 156us/step - loss: 0.5515 - acc: 0.8450 - val_loss: 0.6540 - val_acc: 0.8180\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 154us/step - loss: 0.4789 - acc: 0.8700 - val_loss: 0.6028 - val_acc: 0.8340\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 155us/step - loss: 0.4295 - acc: 0.8775 - val_loss: 0.5693 - val_acc: 0.8430\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 173us/step - loss: 0.4070 - acc: 0.8755 - val_loss: 0.5565 - val_acc: 0.8490\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 243us/step - loss: 0.3816 - acc: 0.8935 - val_loss: 0.5455 - val_acc: 0.8470\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 181us/step - loss: 0.3634 - acc: 0.8910 - val_loss: 0.5390 - val_acc: 0.8500\n",
      "Test loss: 0.5390435092449188\n",
      "Test accuracy: 0.85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5390435092449188"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ifunc(recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 30, 0.2, True, 80, 'elu')\n",
      "OrderedDict()\n",
      " - arg #5: Value 20, from data: 5.0745603581766865\n",
      " - arg #6: Value 30, from data: 0.6408475230502724\n",
      " - arg #7: Value 0.2, from data: -1.919149735907273\n",
      " - arg #8: Value True, from data: [-5.43384245  1.72107825] yielding probas: \"False\": 0.0%, \"True\": 100.0%\n",
      " - arg #9: Value 80, from data: 1.883666386473878\n",
      " - arg #10: Value elu, from data: [-0.82955412  3.46519729] yielding probas: \"relu\": 1.0%, \"elu\": 99.0%\n"
     ]
    }
   ],
   "source": [
    "args, kwargs = ifunc.convert_to_arguments(recommendation)\n",
    "print(args[4:])     # skip the data we feed\n",
    "print(kwargs)  \n",
    "\n",
    "print(ifunc.get_summary(recommendation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on that we should make our network bigger, but that will be in the next chapter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
